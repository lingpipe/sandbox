\chapter{Latent Dirichlet Allocation}\label{chapter:lda}

Latent Dirichlet allocation (LDA) provides a statistical approach to
document clustering based on the words (or tokens) that appear in a
document.  Given a set of documents, a tokenization scheme to convert
them to bags of words, and a number of topics, LDA is able to infer
the set of topics making up the collection of documents and the
mixture of topics found in each document.  These assignments are soft
in that they are expressed as probability distributions, not absolute
decisions.

At the basis of the LDA model is the notion that each document is
generated from its own mixture of topics.  For instance, a blog post
about a dinner party might 70\% about cooking, 20\% about sustainable
farming, and 10\% about entertaining.

Applications of LDA include document similarity measurement, document
clustering, topic discovery, word association for search, feature
extraction for discriminative models such as logistic regression or
conditional random fields, classification of new documents, language
modeling, and many more.

\section{Corpora and Documents}

For LDA, a corpus is nothing more than a collection of documents.
Each document is modeled as a sequence of words (or tokens).  It turns
out that only the count of the words is significant, not their order.

A bag is a data structure like a set, only with (non-negative) counts
for each member.  Mathematically, we may model a bag as a mapping from
its members to their counts. Like other models that don't distinguish
word order, LDA is called a bag-of-words model.

We often start with a real document collection consisting of character
data.  LingPipe uses tokenizer factories to convert character
sequences to sequences of tokens (see \refchap{tokenization}).
Utilities in the LDA implementation class help convert these to
appropriate representations with the help of symbol tables (see
\refchap{symbol-tables}).


\section{Synthetic LDA Example}

We begin with a synthetic example due to Steyvers and Griffiths.%
%
\footnote{From Steyvers, Mark and Tom Griffiths. 2007. Probabilistic
  topic models. In Thomas K. Landauer, Danielle S. McNamara, Simon
  Dennis and Walter Kintsch (eds.), {\it Handbook of Latent Semantic
    Analysis}. Laurence Erlbaum.}
%
Synthetic or ``fake data'' models play an important role in
statistics, where they are used like unit tests for statistical
inference mechanisms.  

In this section, we will create an LDA model by fixing all of its
parameters, then use it to generate synthetic data according to the
model.  Then, we will provide the generated data to our LDA
inference algorithm and make sure that it can recover the known
structure of the data.

The implementation of our synthetic example, which illustrates
the basic inference mechanism for LDA, is in the demo class
\code{SyntheticLdaExample}.

\subsection{Two Topic, Five Word Model}

Steyvers and Griffiths' synthetic example involved only two topics and
five words.  The words are \stringmention{river},
\stringmention{stream}, \stringmention{bank}, \stringmention{money},
and \stringmention{loan}.  The first two words typically occur in
articles about rivers, and the last two words in articles about money.
The word \stringmention{bank}, the canonical example of an ambiguous
word in English, occurs in both types of articles.  

\subsubsection{What's a Topic?}

A topic in LDA is nothing more than a discrete probability
distribution over words.  That is, given a topic, each word
has a probability of occurring, and the sum of all word
probabilities in a topic must sum to 1.

Steyvers and Griffiths' example involves two topics, one about
banking and one about rivers, with the following word probabilities.
%
\begin{center}
\begin{tabular}{lrrrrr}
& \stringmention{river} 
& \stringmention{stream}
& \stringmention{bank}
& \stringmention{money}
& \stringmention{loan}
\\ \hline
\tblhead{Topic 1} & 1/3 & 1/3 & 1/3 & 0 & 0 
\\
\tblhead{Topic 2} & 0 & 0 & 1/3 & 1/3 & 1/3
\end{tabular}
\end{center}
%
Topic 1 is about water and topic 2 about money.  If a word is drawn
from topic 1, there is a 1/3 chance it is \charmention{river}, a 1/3
chance it is \charmention{stream} and a 1/3 chance it is
\charmention{bank}.  


\subsubsection{What's a Document?}

For the purposes of LDA, a document is modeled as a sequence of
tokens.  We use a tokenizer factory to convert the documents into
counts.  The identity of the tokens doesn't matter.  Neither does
the order of the tokens, though we preserve order so that we can
label tokens with topics in our final output.

LDA deals with token identifiers in the form of a matrix, not
with string tokens.  But there's a handy utility method to produce
such a matrix from a corpus of texts and a tokenizer factory.

The code in \code{SyntheticLdaExample} starts out defining an array of
sixteen character sequences making up the document collection that
will be clustered.  The texts are defined starting with
%
\codeblock{SyntheticLdaExample.1a}
%
and ending with
%
\codeblock{SyntheticLdaExample.1b}
%
The only reason we used concatenation is so that the lines would fit
on the pages of the book.  The complete set of documents is most
easily described in a table.
%
\begin{center}
\begin{tabular}{c|ccccc}
% & \multicolumn{5}{c}{\tblhead{Tokens}} \\
\tblhead{Doc ID}
& \tblhead{river}
& \tblhead{stream}
& \tblhead{bank}
& \tblhead{money}
& \tblhead{loan}
\\ \hline
\tblhead{0} & 0 & 0 & 4 & 6 & 6 
\\
\tblhead{1} & 0 & 0 & 5 & 7 & 4
\\
\tblhead{2} & 0 & 0 & 7 & 5 & 4 
\\
\tblhead{3} & 0 & 0 & 7 & 6 & 3 
\\
\tblhead{4} & 0 & 0 & 7 & 2 & 7
\\
\tblhead{5} & 0 & 0 & 9 & 3 & 4
\\
\tblhead{6} & 1 & 0 & 4 & 6 & 5 
\\
\tblhead{7} & 1 & 2 & 6 & 4 & 3
\\
\tblhead{8} & 1 & 3 & 6 & 4 & 2
\\
\tblhead{9} & 2 & 3 & 6 & 1 & 4
\\
\tblhead{10} & 2 & 3 & 7 & 3 & 1
\\
\tblhead{11} & 3 & 6 & 6 & 1 & 0
\\
\tblhead{12} & 6 & 3 & 6 & 0 & 1
\\
\tblhead{13} & 2 & 8 & 6 & 0 & 0
\\
\tblhead{14} & 4 & 7 & 5 & 0 & 0
\\
\tblhead{15} & 5 & 7 & 9 & 0 & 0
\end{tabular}
\end{center}
%
The first document, with identifier 0, has four instances of
\stringmention{bank}, six of \stringmention{money} and six of
\stringmention{loan}.  Each document has sixteen tokens, but this is
not a requirement, but rather an artifact of Steyvers and Griffiths'
example.



\section{The Generative Model}

LDA is a kind of probabilistic model known as a generative model.
Generative models provide step-by-step characterizations of how to
generate a corpus of documents.%
%
\footnote{LDA does not model the number of documents in a corpus
or the number of tokens in each document.  These values must be
provided as constants.  This is usually not a problem because
the corpus is given as data.}
%
This setup seems strange to many people at first because we are in
practice presenting LDA with a collection of documents and asking it
to infer the topic model, not the other way around.  Nevertheless, the
probabilistic model is generative.

LDA generates each document independently based on the model
parameters.  To generate a document, LDA first generates a topic
distribution for that document.%
%
\footnote{The topic distribution for a document is generated from
a Dirichlet distribution, which is where the model gets its name.}
%
This happens before any of the words in a document are generated.  At
this stage, we might only know that a document is 70\% about politics
and 30\% about the economy, not what words it contains.

After generating the topic distribution for a document, we generate
the words for the document.  

In the case of LDA, we suppose that the number of documents and the
number of tokens in each document is given.  This means they're
not part of 

That means we don't try
to predict how long each document is or use document-length
information for relating documents.  Furthermore, LDA does not We also do not attempt to model
the size of the corpus.


contain are known, but 


LDA is a probabilistic topic (or factor) model.  Historically, LDA
evolved as a Bayesian generalization of the probabilistic latent
semantic indexing (pLSI) model.  pLSI was itself motivated as a
properly discretized probabilistic form of latent semantic indexing
(LSI).  LingPipe's implementation of the LSI factor model is covered
in \refchap{svd}.

The topic model is latent \ie{unknown}.

LingPipe's implementation of LDA is able to start from a corpus of
documents and infer the set of topics in the corpus, the topic



\section{Generalized ``Documents''}

Although LDA is framed in terms of documents and words, it turns out
it only uses the identities of the tokens.  As such, it may be applied
to collections of ``documents'' consisting of any kind of count data,
not just bags of words.  For instance, LDA may be applied to RNA
expression data.

