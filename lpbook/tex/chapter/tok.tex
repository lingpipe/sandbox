\chapter{Tokenization}\label{chap:tokenization}

Many natural-language processing algorithms operate at the word level,
such as most part-of-speech taggers and named-entity chunkers, some
classifiers, some parameterizations of spelling correction, etc.

A token is a generalized kind of word that is derived from segmenting
an input character sequence and potentially transforming the segments.
For instance, a search engine query like
\searchquery{London restaurants} might be converted into a boolean
search for the (case normalized) token \stringmention{london} and the
(plurality normalized) token \stringmention{restaurant}.

\section{Tokenizers and Tokenizer Factories}

LingPipe provides a package \code{com.aliasi.tokenizer} for handling
tokenization.  

\subsection{The \code{TokenizerFactory} Interface}

The \code{TokenizerFactory} factory interface defines a single method,
\code{tokenizer(char[],int,int)}, which takes a slice of a character
array as an argument and returns an instance of \code{Tokenizer}.

LingPipe's tokenizer factory implementations come in two flavors Basic
tokenizer factories are constructed from simple parameters.  For the
basic tokenizers with no parameters, a singleton instance is supplied
as a static constant in the class.  

Tokenizer filters are constructed from other tokenizer factories and
modify their outputs in some way, such as by case normalization,
stemming, or stop-word filtering.

In order to bundle a tokenizer factory with a model, it must be
serializable.  All of LingPipe's tokenizer factories are serializable,
including ones made up by composing a sequence of filters.


\subsection{The \code{Tokenizer} Base Class}

All tokenizers extend the abstract base class \code{Tokenizer}.
Tokenizers provide a stream of tokens.  An instance of
\code{Tokenizer} represents the state of tokenizing a particular
string.  

\subsubsection{Constructing a Tokenizer}

There is no state represented in the \code{Tokenizer} abstract class,
so there is a single no-argument constructor \code{Tokenizer()}.

Because tokenizers are usually created through the 
\code{TokenizerFactory} interface, most.
classes extending \code{Tokenizer} are not delcared to be public.
Instead, only the factory is visible, and the documentation for a
tokenizer's behavior will be in the factory's class documentation.

\subsubsection{Streaming Tokens}

The only method that is required to be implemented is
\code{nextToken()}, which returns the next token in the token stream
as a string, or \code{null} if there are no more tokens.  There is no
reference in a tokenizer itself to the underlying sequence of
characters.  

\subsubsection{Token Positions}

We often wish to maintain the position of a token in the underlying
text.  Given that tokens may be modified or even dropped altogether,
the position of a token is not necessarily going to be recoverable
from the sequence of tokens and whitespaces.  So the \code{Tokenizer}
class supplies methods \code{lastTokenStartPosition()} and
\code{lastTokenEndPosition()}, which return the index of the first
character and of one past the last character.  If no tokens have yet
been returned, these methods both return -1.  These positions are
relative to the slice being tokenized, not to the underlying character
array.  

The token position methods are implemented in the \code{Tokenizer}
base class to throw an \code{UnsupportedOperationException}.
Subclasses that want token positions should override these methods.
Tokenizer filters should almost always just pass the positions of the
tokens being modified.

\subsubsection{Iteration}

The method \code{iterator()} returns an iterator over strings
representing tokens.  In the \code{Tokenzer} base class, the iterator
is defined by delegation to the \code{nextToken()} method.  Thus
subclasses do not usually need to redefine this method.

This \code{iterator()} method allows the \code{Tokenizer} class to
implement the \code{Iterable<String>} interface.  Thus the tokens
can be read from a tokenizer with a for loop.  Given a tokenizer
factory \code{tokFac} and the character slice for input, the
usual idiom is
%
\begin{verbatim}
Tokenizer tokenizer = tokFac.tokenizer(cs,start,length);
for (String token : tokenizer) { ... }
\end{verbatim}

\subsubsection{Bulk Tokenization}

The method \code{tokenize()} returns an array of the remaining tokens
and \code{tokenize(List,List)} writes the remaining tokens and
whitespaces to the specified list.

\subsubsection{Serializability and Thread Safety}

Because they involve dynamic state, tokenizers are almost never
serializable and almost never thread safe.  

 
\subsubsection{Streaming Whitespaces}

Over time, LingPipe has moved from the use of whitespace returned from
tokenizers to token start and end positions.  Unless otherwise noted,
tokenizers need not concern themselves with whitespace.  LingPipe's
built-in tokenizers almost all define the whitespace to be the 
string between the last token and the next token, or the empty string
if that is not well defined.

The method \code{nextWhitespace()} returns the next whitespace from
the tokenizer.  ``White space'' is the general term for the material
between tokens, because in most cases, all non-whitespace is part of
some token.  LingPipe's \code{Tokenizer} class generalizes the notion
of whitespace to arbitrary strings.

Each token is preceded by a whitespace and the sequence ends with a
whitespace.  That is, the sequence goes whitespace, token, whitespace,
token, \ldots, whitespace, token, whitespace.  So the number of
whitespaces is one greater than the number of tokens, and the minimum
output of a tokenizer is a single whitespace.

If the \code{nextWhitespace()} method is not implemented by a
subclass, the implementation inherited from the \code{Tokenizer} base
class simply returns a string consisting of a single space character,
\unicode{0020}, \unicodedesc{space}.

If the \code{nextWhitespace()} method is implemented to return the
text between tokens, tokens do not overlap, and the string for a token
is not modified in any way, then concatenating the sequence of
whitespaces and tokens will produce the underlying characters that
were tokenized.


\subsection{Token Display Demo}

We provide a demo program \code{DisplayTokens}, which runs a tokenizer
over the command-line argument.  The \code{main()} method of the command
calls a utility method on a string variable \code{text} supplied
on the command line, using a built-in tokenizer
%
\codeblock{DisplayTokens.1}
%
The \code{IndoEuropeanTokenizerFactory} class is in
\code{com.aliasi.tokenizer}, and provides a reusable instance through the static constant
\code{INSTANCE}.

The \code{displayTextPositions()} method just prints out the string on
a single line followed by lines providing indexes into the string.
This method won't display properly if there are newlines in the string
itself.

The work is actually done in the subroutine, the body of which is
%
\codeblock{DisplayTokens.2}
%
We first convert the character sequence \code{in} to a 
character array using the utility method \code{toCharArray(CharSequence)}
from the class \code{Strings} in the package \code{com.aliasi.util}.
then, we create the tokenizer from the tokenizer factory.
Next, we just iterate over the tokens, extract their start and end
positions, and print the results.

We provide a corresponding Ant target \code{display-tokens}, which
is given a single command-line argument consisting of the value of
the property \code{text}.  
%
\commandlinefollow{ant -Dtext="The note says, 'Mr.\ Sutton-Smith owes \$15.20.'" display-tokens}
\begin{verbatim}
The note says, 'Mr. Sutton-Smith owes $15.20.'
0123456789012345678901234567890123456789012345
0         1         2         3         4

START   END  TOKEN           START   END TOKEN
    0     3  |The|              26    27  |-|
    4     8  |note|             27    32  |Smith|
    9    13  |says|             33    37  |owes|
   13    14  |,|                38    39  |$|
   15    16  |'|                39    44  |15.20|
   16    18  |Mr|               44    45  |.|
   18    19  |.|                45    46  |'|
   20    26  |Sutton|
\end{verbatim}
%
In writing the string, we put the ones-place indexes below it, then
the tens-place indexes, and so on.  Because we count from zero, the
very first \charmention{T} has index 0, whereas the \charmention{M} in
\stringmention{Mr} has index 16 and the final apostrophe index 45.
The string is 46 characters long; the last index is always one less
than the length.

We then show the tokens along with their start and end positions.  As
always, the start is the index of the first character and the end is
one past the index of the last character.  Thus the name
\stringmention{Smith} starts at character 27 and ends on character 31.
This also means that the end position of one token may be the start
position of the next when there is no space between them.  For
example, \charmention{says} has an end position of 13 (which is
exclusive) and the following comma a start position of 13 (which is
inclusive).  


\section{LingPipe's Base Tokenizer Factories}

LingPipe provides several base tokenizer factories.  These may be combined
with filters, which we describe in the next section, to create compound
tokenizers.

\subsection{The \code{IndoEuropeanTokenizerFactory} Class}

LingPipe's \code{IndoEuropeanTokenizer} is a fairly fine-grained
tokenizer in the sense that it splits most things apart.  The notable
exception in this instance is the number \stringmention{15.20}, which
is kept as a whole token.  Basically, it consumes as large a token
as possible according to the following classes.

\subsubsection{Kinds of Tokens}

An alphanumeric token is a sequence of one or more digits or letters
as defined by the utility methods \code{isDigit()} and
\code{isCharacter()} methods in Java's \code{Character} class.  Thus
\stringmention{aaa}, \stringmention{A1}, and \stringmention{1234} are all
considered single tokens.

A token may consist of a combination of digits with any number of
token-internal periods or commas.  Thus \stringmention{1.4.2} is
considered a token, as is \stringmention{12,493.27}, but
\stringmention{a2.1} is three tokens, \stringmention{a2},
\stringmention{.} (a period), and \stringmention{1}.

A token may be an arbitrarily long sequence of hyphens (\code{-}) or
an arbitrarily long sequence of equal signs (\code{=}).  The former
are often used as en-dashes and em-dashes in ASCII-formatted text,
and longer sequences are often used for document structure.

Double grave accents (\code{``}) and double apostrophes (\code{''}) are
treated as single tokens, as they are often used to indicate quotations.

All other non-space characters, such as question marks or ampersands,
are considered tokens themselves.

\subsubsection{Construction}

The static constant \code{INSTANCE} refers to an instance of
\code{IndoEuropeanTokenizerFactory} that may be reused.   The
no-argument constructor may also be used.


\subsubsection{Thread Safety and Serializability}

The \code{IndoEuropeanTokenizerFactory} class is both thread safe and
serializable.  The deserialized object will be reference identical
to the factory picked out by the \code{INSTANCE} constant.



\subsection{The \code{CharacterTokenizerFactory} Class}

The \code{CharacterTokenizerFactory} treats each non-whitespace
\code{char} value in the input as a token.  The definition of
whitespace is from Java's \code{Character.isWhitespace()} method.
The whitespaces returned will consist of
all of the whitespace found between the non-whitespace characters.

For instance, for the string \stringmention{a dog}, there are four
tokens, \stringmention{a}, \stringmention{d}, \stringmention{o}, and
\stringmention{g}.  The whitespaces are all length zero other than 
for the single space between the \stringmention{a} and \stringmention{d}
characters.

This tokenizer factory is particularly useful for Chinese, where there
is no whitespace separating words, but words are typically only one or
two characters (with a long tail of longer words).

This class produces a token for each \code{char} value.  This means
that a surrogate pair consisting of a high surrogate UTF-16 value and
low surrogate UTF-16 value, which represent a single Unicode code
point, will produce two tokens.  Specifically, unicode code points
outside of the basic multilingual plane (BMP), that is with values at
or above \unicode{10000}, will produce two tokens (see \refsec{utf-16} for more
information).

The class is thread safe and serializable.  There is no constructor,
only a static constant \code{INSTANCE}.  The instance is also the
result of deserialization.


\subsection{The \code{RegExTokenizerFactory} Class}\label{section:tok-regex-tokenizer-factory}

For parsing programming languages, the lexical analysis stage
typically breaks a program down into a sequence of tokens.
Traditional C packages like Lex and Java packages like JavaCC specify
regular expressions for tokens (and context-free grammars with side
effects for parse trees).  Regular expression-based tokenization is
also popular for natural language processing.

LingPipe's \code{RegExTokenizerFactory} implements tokenizers where
the tokens are defined by running regular expression matchers in find
mode (see \refsec{regex-find}).  An instance may be constructed from a
\code{Pattern}, or from a \code{String} representing a regular
expression with optional integer flags (see \refsec{pattern-modes} for
information on how to use the flags and their numerical values).

The demo class \code{RegexTokens} implements a simple regular expression
tokenizer factory based on three command line arguments, one for the
regular expression, one for the flags, and one for the text being tokenized.
%
\codeblock{RegexTokens.1}
%

The Ant target \code{regex-tokens} runs the command, setting three
command line arguments for three properties, \code{regex} for the
string representation of the regular expression, \code{flags} for the
integer flags, and \code{text} for the text to be tokenized.
%
\commandlinefollow{ant -Dregex="{\bk}p\{L\}+" -Dflags=32 -Dtext="John likes 123-Jello." regex-tokens}
\begin{verbatim}
regex=|\p{L}+|
flags=32 (binary 100000)

John likes 123-Jello.
012345678901234567890
0         1         2

START   END TOKEN
    0     4  |John|
    5    10  |likes|
   15    20  |Jello|
\end{verbatim}
%
First we print the regex and the flags (in both decimal and binary
forms), then the string with indexes, and finally the tokens.  Because
we used the pattern \code{{\bk}p\{L\}+}, our tokens consist of
maximally long sequences of letter characters.  Other characters such
as spaces, numbers, punctuation, etc., are simply skipped so that they
become part of the whitespace.

Instances of \code{RegExTokenizerFactory} are serializable.  They are
also thread safe.


\subsection{The \code{NGramTokenizerFactory} Class}

An \code{NGramTokenizerFactory} produces tokens from an input string
consisting of all of the substrings of the input within specified size
bounds.  Substrings of an input string of length $n$ are typically
called $n$-grams (and sometimes $q$-grams).

Thus such a factory is constructed using a minimum and
maximum sequence size, with \code{NGramTokenizerFactory(int,int)}.
The sequences produced include the whitespaces between what we
typically think of as tokens.  Tokenizers such as these are especially
useful for extracting features for classifiers.

We provide an example implementation in the demo class
\code{NGramTokens}.  The only thing novel is the construction of
the tokenizer factory itself,
%
\codeblock{NGramTokens.1}
%

It takes three command-line arguments, the
minimum and maximum sequence lengths and the text to analyze.
These are supplied to Ant target \code{ngram-tokens} as 
properties \code{minNGram}, \code{maxNGram}, and \code{text}.
%
\commandlinefollow{ant -DminNGram=1 -DmaxNGram=3 -Dtext="I ran." ngram-tokens}
\begin{verbatim}
minNGram=1
maxNGram=3

I ran.
012345

START   END TOKEN          START   END TOKEN
    0     1  |I|               2     4  |ra|
    1     2  | |               3     5  |an|
    2     3  |r|               4     6  |n.|
    3     4  |a|               0     3  |I r|
    4     5  |n|               1     4  | ra|
    5     6  |.|               2     5  |ran|
    0     2  |I |              3     6  |an.|
    1     3  | r|
\end{verbatim}
%
You can see from the output that the tokenizer first outputs the
1-grams , then the 2-grams, then the 3-grams.  These are typically
referred to as ``unigrams,'' ``bigrams,'' and ``trigrams.''%
%
\footnote{Curiously, we get a transition in common usage from Latin to Greek at
4-grams (tetragrams), and 5-grams (pentagrams), 6-grams (hexagrams)
etc.  Some authors stick to all Greek prefixes, preferring the terms
``monogram'' and ``digram'' to ``unigram'' and ``bigram.''}
%
Note that the spaces and punctuation are treated like any other
characters.  For instance, the unigram token spanning positions 1 to 2
is the string consisting of a single space character and the one
spanning from 5 to 6 is the string consisting of a single period
character.  The first bigram token, spanning from positions 0 to 2, is
the two-character string consisting of an uppercase I followed by a
space.  By the time we get to trigrams, we begin to get cross-word
tokens, like the trigram token spanning from position 0 to 3,
consistinf of an uppercase I, space, and lowercase r.

In terms of the information conveyed, the spaces before and after
words help define word boundaries.  Without the bigram token spanning
1 to 3 or the trigram spanning 1 to 4, we wouldn't know that the
lowercase r started a word.  The cross-word ngram tokens help define
the transitions between words.  With long enough ngrams, we can even
get three-word effects, 


\subsection{The \code{LineTokenizerFactory} Class}

A \code{LineTokenizerFactory} treats each line of input as a single
token.  It is just a convenience class extending
\code{RegExTokenizerFactory} supplying the regex \code{.+}.  

Line termination is thus defined as for regular expression patterns (see
\refsec{regex-lines} for the full set of line-end sequences
recognized).  Final empty lines are not included in the sequence of
tokens.

There is a static constant \code{INSTANCE} which may be used.  This is
also the value of deserialization.  Line tokenizer factories are thread
safe.


\section{LingPipe's Filtered Tokenizers}

Following the same filter-based pattern as we saw for Java's I/O,
LingPipe provides a number of classes whose job is to filter the
tokens from an embedded tokenizer.  In all cases, there will be a base
tokenizer producing a tokenizer which is then modified by a filter to
produce a different tokenizer.  

The reason the tokenizer is modified rather than, say, the characters
being tokenized, is that we usually want to be able to link the tokens
back to the original text.  If we allow an arbitrary transformation of
the text, that's no longer possible.  Thus if the text itself needs to
be modified, that should be done externally to tokenization.

\subsection{The \code{ModifiedTokenizerFactory} Abstract Class}

The abstract class \code{ModifiedTokenizerFactory} in the package
\code{com.aliasi.tokenizer} provides a basis for filtered tokenizers
implemented in the usual way.  Like many LingPipe filter classes, a
modified tokenizer factory is immutable.  Thus the base tokenizer
factory it contains must be provided at construction time.  This is
done through the single constructor
\code{ModifiedTokenizerFactory(TokenizerFactory)}.  The specified
tokenizer factory will be stored, and is made available through the
method \code{baseTokenizerFactory()} (it is common to see protected
member variables here, but we prefer methods).

It then implements the \code{tokenizer(char[],int,int)} method in the
tokenizer factory interface using the base tokenizer factory to 
produce a tokenizer which is then passed through the method
\code{modify(Tokenizer)}, which returns a \code{Tokenizer} that may
then be returned.  This is perhaps easier to see in code,
%
\begin{verbatim}
protected abstract Tokenizer modify(Tokenizer tokenizer);

public Tokenizer tokenizer(char[] cs, int start, int length) {
    TokenizerFactory tokFact = baseTokenizerFactory();
    Tokenizer tok = tokFact.tokenizer(cs,start,length);
    return modify(tok);
}
\end{verbatim}

\subsubsection{Serialization}

A modified tokenizer factory will be serializable if its base
tokenizer factory is serializable.  But unless the subclass defines a
public no-argument constructor, trying to deserialize it will throw an
exception.  If a subclass fixes a base tokenizer factory, there may be
a public no-argument constructor.  A subclass intended to be used
as a general filter will not have a public no-argument constructor
because it would lead to instances without base tokenizers defined.

As elsewhere, subclasses should take control of their serialization.
We recommend using the serialization proxy, and show an example
in the next section.

\subsubsection{Thread Safety}

A modified tokenizer factory is thread safe if its \code{modify()}
method is thread safe.

\subsection{The \code{ModifyTokenTokenizerFactory} Abstract Class}

In the previous section, we saw how the \code{ModifyTokenizerFactory}
base class could be used to define tokenizer filters.  That class
had a modify method that consumed a tokenizer and returned a tokenizer.
The class \code{ModifyTokenTokenizerFactory} is a subclass of
\code{ModifiedTokenizerFactory} that works at the token level.  Almost
all of LingPipe's tokenizer filters are instances of this subclass.

The method \code{modifyToken(String)} operates a token at a time,
returning either a modified token or \code{null} to remove the token
from the stream.  The implementation in the base class just returns
its input.

There is a similar method \code{modifyWhitespace(String)}, though
whitespaces may not be removed.  If tokens are removed, whitespaces
around them are accumulated.  For instance, if we had $w_1, t_1, w_2,
t_2, w_3$ as the stream of whitespaces and tokens, then removed token
$t_2$, the stream would be $w_1, t_1, w_2\cdot w_3$, with the last two
whitespaces being concatenated and the token being removed.

The start and end points for modified tokens will be the same as for
the underlying tokenizer.  This allows classes that operate on
modified tokens to link their results back to the string from which
they arose.

This subclass behaves exactly the same way as its superclass with
respect to serialization and thread safety.  For thread safety, both
the modify methods must both be thread safe.


\subsubsection{Example: Reversing Tokens}

As an example, we provide a demo class
\code{ReverseTokenTokenizerFactory}.  This filter simply reverses the
text of each token. Basically, the implementation involves a declaration,
constructor and definition of the modify operation,
%
\codeblock{ReverseTokenTokenizerFactory.1}
%
In addition, the class implements a serialization proxy (see
\refsec{io-serialization-proxy}).

The \code{main()} method for the demo reads a command line
argument into the variable \code{text}, then constructs a token-reversing
tokenizer factory based on an Indo-European tokenizer factory,
then displays the output,
%
\codeblock{ReverseTokenTokenizerFactory.2}
%
It also serializes and deserializes the factory and displays
the output for that, but we've not shown the code here.

The Ant target \code{reverse-tokens} calls the class's \code{main()}
method, supplying the value of property \code{text} as the command-line
argument.
%
\commandlinefollow{ant reverse-tokens}
\begin{verbatim}
The note says, 'Mr. Sutton-Smith owes $15.20.'
0123456789012345678901234567890123456789012345
0         1         2         3         4

START   END TOKEN
    0     3  |ehT|
    4     8  |eton|
    9    13  |syas|
...
   39    44  |02.51|
   44    45  |.|
   45    46  |'|
\end{verbatim}
%
The results are the same positions as for the Indo-European tokenizer,
only with the content of each token reversed.  We have not shown the
serialized and then deserialized output; it's the same.

\subsection{The \code{LowerCaseTokenizerFactory} Class}

The \code{LowerCaseTokenizerFactory} class modifies tokens by
converting them to lower case.  This could've just as easily been
upper case -- it just needs to be normalized.

Case normalization is often applied in contexts where the case of
words doesn't matter, or provides more noise than information.  For
instance, search engines typically store case-normalized tokens in
their reverse index.  And classifiers often normalize words for
case (and other properties).

Instances are constructed by wrapping a base tokenizer factory.
Optionally, an instance of \code{Locale} to define the specific
lowercasing operations.  If no locale is specified,
\code{Locale.ENGLISH} is used.  (This ensures consistent behavior
across platforms, unlike using the platform's default locale.)

Whitespace and start and end positions of tokens are defined
as by the base classifier.

The class is used like other filters.  For instance, to create a
tokenizer factory that returns lowercase tokens produced the
Indo-European tokenizer using German lowercasing, just use
%
\begin{verbatim}
TokenizerFactory f = IndoEuropeanTokenizerFactory.INSTANCE;
Locale loc = Locale.GERMAN;
TokenizerFactory fact = new LowerCaseTokenizerFactory(f,loc);
\end{verbatim}

Instances will be serializable if the base tokenizer factory is
serializable.  Serialization stores the locale using its built-in
serialization, as well as the base tokenizer factory.  Instances will
be thread safe if the base tokenizer factory is thread safe.


\subsection{The \code{StopTokenizerFactory} Class}

The \code{StopTokenizerFactory} provides a tokenizer factory filter
that removes tokens that appear in a specified set of tokens.  The set
of tokens removed is typically called a \stringmention{stop list}.
The stop list is case sensitive for this class, but stoplisting is
often applied after case normalization.

It is common in token-based classifiers and search engines to further
normalize input by removing very common words.  For instance, the
words \stringmention{the} or \stringmention{be} in English documents
typically convey very little information themselves (they can convey
more information in context).  Sometimes this is done for efficiency
reasons, in order to store fewer tokens in an index or in statistical
model parameter vectors.  It may also help with accuracy, especially
in situations with limited amounts of training data.  In small data
sets, the danger is that these stop words will become associated with
some categories more than others at random and thus randomly bias
the resulting system.

Stop lists themselves are represented as sets of tokens.  Stoplisting
tokenizers are constructed by wrapping a base tokenizer and providing
a stop list.  For instance, to remove the words \stringmention{the} and
\stringmention{be}, we could use
%
\begin{verbatim}
Set<String> stopSet = CollectionUtils.asSet("the","be");
TokenizerFactory f1 = IndoEuropeanTokenizerFactory.INSTANCE;
TokenizerFactory f2 = new LowerCaseTokenizerFactory(f1);
TokenizerFactory f3 = new StopTokenizerFactory(f2,stopSet);
\end{verbatim}
%
The first line uses the static utility method \code{asSet()}
in the LingPipe utility class \code{CollectionUtils}.  This
method is defined with signature 
%
\begin{verbatim}
static <E> HashSet<E> asSet(E... es);
\end{verbatim}
%
so that it takes a variable-length number of \code{E} objects and
returns a hash set of that type.  Because Java performs type inference
on method types (like the \code{<E>} generic in \code{asSet()}),
we don't need to specify a type on the method call.

We then wrap an Indo-European tokenizer factory in a lowercasing and
then a stoplisting filter.  Operationally, what will happen is that
the Indo-European tokenizer will tokenize, the resulting tokens will
be converted to lower case, and then stop words will be removed.  The
calls start out in reverse, though, with \code{f3} called to tokenize,
which then calls \code{f2} and filters the resulting tokens, where
\code{f2} itself calls \code{f1} and then filters its results.

\subsubsection{What's a Good Stop List?}

The utility of any given stop list will depend heavily on the
application.  It often makes sense in an application to run several
different stop lists to see which one works best.  

Usually words on the stop list will be of relatively high
frequency.  One strategy is to print out the top few hundred
or thousand words in a corpus sorted by frequency and inspect
them by hand to see which ones are ``uninformative'' or potentially
misleading.

\subsubsection{Built-in For English}

LingPipe provides a built-in stop list for English.  The
\code{EnglishStopTokenizerFactory} class extends the
\code{StopTokenizerFactory} class, plugging in the following
stoplist for English:
%
\begin{quote}
a, be, had, it, only, she, was, about, because, has, its, of, some,
we, after, been, have, last, on, such, were, all, but, he, more, one,
than, when, also, by, her, most, or, that, which, an, can, his, mr,
other, the, who, any, co, if, mrs, out, their, will, and, corp, in,
ms, over, there, with, are, could, inc, mz, s, they, would, as, for,
into, no, so, this, up, at, from, is, not, says, to
\end{quote}
%
Note that the entries are all lowercase.  It thus makes sense
to run this filter to a tokenizer factory that produces only
lowercase tokens.  

This is a fairly large stop list.  It's mostly function words, but
also includes some content words like \stringmention{mr},
\stringmention{corp}, and \stringmention{last}.  Also note that
some words are ambiguous, which is problematic for stoplisting.  For
instance, the word \stringmention{can} acts as both a modal auxiliary
verb (presently possible) and a noun (meaning among other things, a
kind of container).


\subsection{The \code{RegExFilteredTokenizerFactory} Class}

The \code{RegExFilteredTokenizerFactory} class provides a general
filter that removes all tokens that do not match a specified regular
expression.  For a token to be retained, it must match the specified
regular expression (in the sense of \code{match()} from
\code{Matcher}).

An instance of \code{RegExFilteredTokenizerFactory} may be constructed
from a base tokenizer factory and a \code{Pattern} (from
\code{java.util.regex}).  For instance, we could retain only tokens
that started with capital letters (as defined by Unicode) using
%
\begin{verbatim}
TokenizerFactory f = ...;
Pattern p = Pattern.compile("\p{Lu}.*");
TokenizerFactory f2 = new RegExFilteredTokenizerFactory(f,p);
\end{verbatim}
%
We could just as easily retain only tokens that didn't start with an
uppercase letter by using the regex \code{[\^{\bk}p\{Lu\}].*} instead
of the one supplied.

Although stop lists could be implemented this way, it would not be
efficient.  For one thing, regex matching is not very efficient for
large disjunctions; set lookup is much faster.  Also, at the present
time, the class creates a new \code{Matcher} for each token,
generating a large number of short-lived objects for garbage
collection.%
%
\footnote{A more efficient implementation would reuse a matcher for
each token in a given tokenization.  The only problem is that it can't
be done naturally and thread-safely by extending
\code{ModifyTokenTokenizerFactory}.  It would be possible to do this
by extending \code{ModifiedTokenizerFactory}.}
%

Like the other filter tokenizers, a regex filtered tokenizer is
thread safe and serializable if its base tokenizer factory is.


\subsection{The \code{TokenLengthTokenizerFactory} Interface}

The class \code{TokenLengthTokenizerFactory} implements a tokenizer
filter that removes tokens that do not fall in a prespecified length
range.  In real text, tokens can get very long.  Fror instance,
mistakes in document formatting can produce a document where all
the words are concatenated without space.  If you look at large
amounts of text (in the gigabytes range), especially if you're
looking at unedited sources like the web, you will find longer
and longer tokens used on purpose.  It's not unusual to see
\stringmention{no}, \code{noo}, and \code{nooooooo!}.  Look
at enough text, and someone will add 250 lowercase O characters
after the \charmention{n}.

The length range is supplied in the constructor, which takes a
tokenizer factory, minimum token length, and maximum token length.
Like the other filters, a length filtering tokenizer will be
serializable and thread safe if the base tokenizer factory is.  And
like some of the other filters, this could be implemented very easily,
but not as efficiently, with regular expressions (in this case the
regex \code{.\{\codeVar{m},\codeVar{n}\}} would do the
trick).

\subsection{The \code{WhitespaceNormTokenizerFactory} Class}

When interpreting HTML documents on the web, whitespace (outside of
\code{<pre>} environments) comes in two flavors, no space and some
space.  Any number of spaces beyond will be treated the same way for
rendering documents as a single space.

This is such a reasonable normalization scheme that we often normalize
our input texts in exactly the same way.  This is easy to do with a
regex, replacing a string \code{text} with
\code{text.replaceAll("{\bk}{\bk}s+"," ")}, which says to replace
any non-empty sequence of spaces with a single space. 

It's not always possible to normalize inputs.  You may need to link
results back to some original text that's not under your control.  In
these cases, it's possible to modify the whitespaces produced by the
tokenizer.  This will not have any effect on the underlying text or
the spans of the tokens themselves.  Unfortunately, very few of
LingPipe's classes pay attention to the whitespace, and for some that
do, it's the token bounds that matter, not the values returned by
\code{nextWhitespace()}.

A \code{WhitespaceNormTokenizerFactory} is constructed from a base
tokenizer factory.  Like the other filter tokenizer factories, it will
be serializable if the base tokenizer factory is serializable and will
be thread safe if the base tokenizer factory is thread safe.


\section{Morphology, Stemming, and Lemmatization}

In linguistics, morphology is the study of how words are formed.  A

Morphology crosses the study of several other phenomena,
including syntax (parts of speech and how words combine), phonology
(how words are pronounced), orthography (how a language is written,
including how words are spelled), and semantics (what a word means).

LingPipe does not contain a general morphological analyzer, but it
implements many parts of word syntax, semantics and orthography.

A morpheme is the minimal meaningful unit of language.  A morpheme
doesn't need to be a word itself.  As we will see shortly, it can be a
word, a suffix, a vowel-change rule, or a template instantiation.
Often the term is extended to include syntactic markers, though these
almost always have some semantic or semantic-like effect, such as
marking gender.

We consider these different aspects of word forms in different
sections.  LingPipe has part-of-speech taggers to determine the
syntactic form a word, syllabifiers to break a word down into phonemic
or orthographic syllables, word-sense disambiguators which figure out
which meaning of an ambiguous word is intended, and many forms of word
clustering and comparison that can be based on frequency, context
similarity and orthographic similarity.  LingPipe also has spelling
correctors to fix misspelled words, which is often handled in the
same part of a processing pipeline as morphological analysis.

In this section, we provide a brief overview of natural language
morphology, then consider the processes of stemming and
lemmatization and how they relate to tokenization.   

\subsection{Inflectional Morphology}

Inflectional morphology is concerned with how the basic paradigmatic
variants of a word are realized.  For instance, in English, the two
nouns \code{computer} and \code{computers} differ in number, the first
being singular, the second plural.  These show up in different
syntactic contexts --- we say \stringmention{one computer} and
\stringmention{two computers}, but not \stringmention{one computers}
and \stringmention{two computer}.%
%
\footnote{These ungrammatical forms like \stringmention{one computers}
will appear in text.  You'll find just about everything in free text
and even in fairly tightly edited text like newswire if you look at
enough of it.}
%
This is an instance of agreement --- the grammatical number of the
noun must agree with the number of the determiner.  In English, there
are only two numbers, singular and plural, but other languages, like
Icelandic and Sanskrit, have duals (for two things), and some
languages go even further, defining numbers for three objects, or
distinguishing many from a few.

In English, the verbs \code{code}, \code{codes}, \code{coded}, and
\code{coding} are all inflectional variants of the same underlying word,
differing in person, number, tense, and aspect.  We get agreement
between verbs and subject nouns in English for these features.  For
instance, a person might say \stringmention{I code} in referring to
themselves, but \stringmention{she codes} when referring to another
woman coding.  Other languages, such as French or Russian, also have
agreement for gender, which can be natural (male person versus female
person versus versus non-person) or grammatical (as in French).
Natural gender is based on what is being talked about.  That's the
distinction we get in English between the relative pronouns
\stringmention{who}, which is used for people and other animate
objects, and \stringmention{what}, which is used for inanimate
objects.  For instance, we say \stringmention{who did you see?} if
we expect the answer to be a person, and 
\stringmention{what did you see?} if we want a thing.

Most languages have fairly straightforward inflectional morphology,
defining a few variants of the basic nouns and verbs.  The actual
morphological operations may be as simple as adding an affix, but even
this is complicated by boundary effects.  For instance, in English
present participle verbs, we have
\stringmention{race}/\stringmention{racing},
\stringmention{eat}/\stringmention{eating}, and \stringmention{run}/\stringmention{running}.
In the first case, we delete the final \charmention{e} before adding
the suffix \stringmention{-ing}, in the second case, we just append
the suffix, and in the third case, we insert an extra \charmention{n}
before adding the suffix.  The same thing happens with number for
nouns, as with \stringmention{boy}/\stringmention{boys} versus
\stringmention{box}/\stringmention{boxes}, in which the first case
appends the suffix \charmention{s} whereas the second adds an
\charmention{e} before the suffix (the result of which, \stringmention{boxes},
is two syllables).

It is also common to see internal vowel changes, as in the English
alternation \stringmention{run}/\stringmention{ran}.  Languages
like Arabic and Hebrew take this to the extreme with their
templatic morphology systems in which a base form consists of
a sequence of consonants and an inflected form mixes in vowles.


\subsection{Derivational Morphology}

Derivational morphology, in contrast to inflectional morphology,
involves modifying a word to create a new word, typically with a
different function.  For instance, from the verb \stringmention{run}
we can derive the noun \stringmention{runner} and from the adjective
\stringmention{quick} we can derive the adverb
\stringmention{quickly}.  After we've derived a new form, we can
inflect it, with singular form \stringmention{runner} and plural form
\stringmention{runners}.

Inflectional morphology is almost always bounded to a finite set of
possible inflected forms for each base word and category.  Derivational
morphology, on the other hand, is unbounded.  We can take the noun
\stringmention{fortune}, derive the adjective \stringmention{fortunate},
the adjective \stringmention{unfortunate}, and then the adverb
\stringmention{unfortunately}.  

Sometimes there's zero derivational morphology, meaning not that
there's no derivational morphology, but that it has no surface effect.
For instance, we can turn most nouns in English into verbs, especially
in the business world, and most verbs into nouns (where they are
called gerunds).  For instance, I can say \stringmention{the running
of the race} where the verb \stringmention{running} is used as a noun,
or \stringmention{Bill will leverage that deal}, where the noun
\stringmention{leverage} (itself an inflected form of
\stringmention{lever}) may be used as a verb.  In American English,
we're also losing the adjective/adverb distinction, with many speakers
not even bothering to use an adverbial form of an adjective, saying
things like \stringmention{John runs fast}.  

\subsection{Compounding}

Some languages, like German, allow two nouns to be combined
into a single word, written with no spaces, as in combining
\stringmention{schnell} (fast) and \stringmention{Zug} (train)
into the compound \stringmention{schnellzug}.  This is a relatively
short example involving two nouns. 

Comnpounds may themselves be inflected.  For instance, in English, we
may combine \stringmention{foot} and \stringmention{ball} to produce
the compound noun \stringmention{football}, which may then be
inflected in the plural to get \stringmention{footballs}.  They may
also be modified derivationally, to get \stringmention{footballer},
and then inflected, to get \stringmention{footballers}.


\subsection{Languages Vary in Amount of Morphology}

Languages are not all the same.  Some code up lots of information in
their morphological systems \eg{Russian}, and some not much at all
\eg{Chinese}.  Some languages allow very long words consisting of
lots of independent morphemes \eg{Turkish, Finnish}, whereas most
only allow inflection and derivation.

\subsection{Stemming and Lemmatization}

In some applications, the differences between different morphological
variants of the same base word are not particularly informative.  For
instance, should the query \searchquery{new car} provide a different
search result than \searchquery{new cars}?  The difference is that one
query uses the singular form \stringmention{car} and the other the
plural \stringmention{cars}.

Lemmatization is the process of mapping a word to its root form(s).
It may also involve determining which affixes, vowel changes, wrappers
or infixes or templates are involved, but typically not for most
applications.

Stemming is the process of mapping a word to a canonical
representation.  The main difference from a linguistic point of view
is that the output of a stemmer isn't necessarily a word (or morpheme)
itself. 

\subsubsection{Example: Lemma \stringmention{author}}

It is very hard to know where to draw the line in stemming.  Some
systems take a strictly inflectional point of view, only stripping off
inflectional affixes or templatic fillers.  Others allow full
derivational morphology.  Even so, the line is unclear.  

Consider the stem \stringmention{author}.  In the English Gigaword
corpus,%
%
\footnote{Graff, David and Christopher Cieri. 2003. {\it English Gigaword}. Linguistic Data Consortium (LDC). University of Pennsylvania.  Catalog number LDC2003T05.}
%
which consists of a bit over a billion words of English newswire text,
which is relatively well behaved, the following variants are observed
%
\begin{quote}
antiauthoritarian, antiauthoritarianism, antiauthority, author,
authoratative, authoratatively, authordom, authored, authoress,
authoresses, authorhood, authorial, authoring, authorisation,
authorised, authorises, authoritarian, authoritarianism,
authoritarians, authoritative, authoritatively, authoritativeness,
authorities, authoritory, authority, authorization, authorizations,
authorize, authorized, authorizer, authorizers, authorizes,
authorizing, authorless, authorly, authors, authorship, authorships,
coauthor, coauthored, coauthoring, coauthors, cyberauthor,
deauthorized, multiauthored, nonauthor, nonauthoritarian,
nonauthorized, preauthorization, preauthorizations, preauthorized,
quasiauthoritarian, reauthorization, reauthorizations, reauthorize,
reauthorized, reauthorizes, reauthorizing, semiauthoritarian,
semiauthorized, superauthoritarian, unauthorised, unauthoritative,
unauthorized
\end{quote}
%
We're not just looking for the substring \stringmention{author}.
Because of the complexity of morphological affixing in English, this
is neither necessary or sufficient.  To see that it's not sufficient,
note that \stringmention{urn} and \stringmention{turn} are not
related, nor are \stringmention{knot} and \stringmention{not}.  To
see that it's not necessary, consider \stringmention{pace} and
\stringmention{pacing} or \stringmention{take} and \stringmention{took}.

Each of the words in the list above is etymologically derived from the
word \stringmention{author}.  The problem is that over time, meanings
drift.  This makes it very hard to draw a line for which words to
consider equivalent for a given task.  The shared root of
\stringmention{author} and \stringmention{authorize} is not only lost to most
native speakers, the words are only weakly semeantically related,
despite the latter form being derived from the former using the
regular suffix \stringmention{-ize}.  The meaning has drifted.  

In contrast, the relation between \stringmention{notary} and
\stringmention{notarize} is regular and the meaning of the latter
is fairly predictable from the meaning of the former.  Moving down a
derivational level, the relation between \stringmention{note} and
\stringmention{notary} feels more opaque.  

Suppose we search for \searchquery{authoritarian}, perhaps researching
organizational behavior.  We probably don't want to see documents
about coauthoring papers.  We probably don't want to see documents
containing \stringmention{unauthoritative}, because that's usually
used in a different context, but we might want to see documents
containing the word \stringmention{antiauthority}, and would probably
want to see documents containing
\stringmention{antiauthoritarianism}.
As you can see, it's rather difficult to draw a line here. 

David A.~Hull provides a range of interesting examples of the
performance of a range of stemmers from the very simple (see
\refsec{tok-simple-stemmer}) through the mainstream medium complex
systems (see \refsec{tok-porter-stemmer}) to the very complex (Xerox's
finite-state-transducer based stemmer).%
%
\footnote{Hull, David A. 1996. 
Stemming algorithms: a case study for detailed evaluation.
{\it Journal of the American Society for Information Science} {\bf
47}(1).}
%
For example, one of Hull's queries contained the word
\stringmention{superconductivity}, but matching documents
contained only the word \stringmention{superconductor}.  This clearly
requiring derivational morphology to find the right documents. A
similar problem occurs for the terms \stringmention{surrogate mother}
versus \stringmention{surrogate motherhood} and \stringmention{genetic
engineering} versus \stringmention{genetically engineered} in another.
Another example where stemming helped was \stringmention{failure}
versus \stringmention{fail} in the context of bank failures.

An example where stemming hurt was in the compound term
\stringmention{client server} matching documents containing
the words \stringmention{serve} and \stringmention{client}, leading to
numerous false positive matches.  This is really a frequency argument,
as servers in the computer sense do serve data, but so many other
people and organizations serve things to clients that it provides more
noise than utility. Another example that caused problems was
was reducing \stringmention{fishing} to \stringmention{fish};
even though this looks like a simple inflectional change, the
nominal use of \stringmention{fish} is so prevalent that 
documents about cooking and eating show up rather than documents
about catching fish.  Similar problems arise in lemmatizing
\stringmention{privatization} to \stringmention{private}; the latter
term just shows up in too many contexts not related to privatization.

These uncertainties in derivational stem equivalence are why many
people want to restrict stemming to inflectional morphology. The
problem with restricting to inflectional morphology is that it's not
the right cut at the problem. Sometimes derivational stemming helps
and sometimes inflectional stemming hurts.%

\subsubsection{Context Sensitivity of Morphology}

Words are ambiguous in many different ways, including morphologically.
Depending on the context, the same token may be interpreted
differently.  For instance, the plural noun \stringmention{runs} is
the same word as the present tense verb \stringmention{runs}.  For
nouns, only \stringmention{run} and \stringmention{runs} are
appropriate, both of which are also verbal forms.  Additional verbal
forms include \stringmention{running} and \stringmention{ran}.  A word
like \stringmention{hyper} might be used as the short-form adjective
meaning the same thing as \stringmention{hyperactive}, or it could be
a noun derived from the the verb \stringmention{hype}, meaning someone
who hypes something (in the publicicity sense).


\subsection{A Very Simple Stemmer}\label{section:tok-simple-stemmer}

One approach to stemming that is surprisingly effective for English
(and other mostly suffixing languages) given its simplicity is to just
map every word down to its first $k$ characters.  Typical values for
$k$ would be 5 or 6.  This clearly misses prefixes like
\stringmention{un-} and \stringmention{anti-}, it'll lump together all
the other forms of \stringmention{author} discussed in the previous
section.  It is also too agressive at lumping things together, merging
words like \stringmention{regent} and \stringmention{regeneration}.

We provide an implementation of this simple stemmer in the class
\code{PrefixStemmer}.  The class is defined to extend
\code{ModifyTokenTokenizerFactory}, the constructor passes the
base tokenizer factory to the superclass, and the work is all
done in the \code{modifyToken()} method,
%
\codeblock{PrefixStemmer.1}
%
There is also a \code{main()} method that wraps an Indo-European
tokenizer factory in a prefix stemmer and displays the tokenizer's
output.

The Ant target \code{prefix-stem-tokens} runs the program, 
with first argument given by property \code{prefixLen} and
the second by property \code{text}.
%
\commandlinefollow{ant -DprefixLen=4 -Dtext="Joe smiled" prefix-stem-tokens}
\begin{verbatim}
START   END TOKEN          START   END TOKEN
    0     3  |Joe|             4    10  |smil|
\end{verbatim}
%
As you can see, the token \stringmention{smiled} is reduced to
\stringmention{smil}, but \stringmention{Joe} is unaltered.  

In an application, the non-text tokens would probably also be
normalized in some way, for instance by mapping all non-decimal
numbers to \stringmention{0} and all decimal numbers to
\stringmention{0.0}, or simply by replacing all digits with
\stringmention{0}.  Punctuation might also get normalized into
bins such as end-of-sentence, quotes, etc.  The Unicode classes
are helpful for this.


\subsection{The \code{PorterStemmerTokenizerFactory} Class}\label{section:tok-porter-stemmer}

The most widely used approach to stemming in English is
that developed by Martin Porter in what has come to be known
generically as the Porter stemmer.%
%
\footnote{Porter, Martin. 1980. An algorithm for suffix
stripping. {\it Program}. {\bf 14}:3.}
%
One of the reasons for its popularity is that Porter and others
have made it freely available and ported it to a number of
programming languages such as Java.  

The stemmer has a very simple input/output behavior, taking a string
argument and returning a string.  LingPipe incorporates Porter's
implementation of his algorithm in the
\code{PorterStemmerTokenizerFactory}.  Porter's stemming operation
is encapsulated in the static method \code{stem(String)}, which returns
the stemmed form of a string.

The tokenizer factory implementation simply delegates
the \code{modifyToken(String)} method in the superclass
\code{ModifyTokenTokenizerFactory} to the \code{stem(String)} method.

\subsubsection{Demo Code}

We provide an example use of the stemmer in the demo class
\code{PorterStemTokens}.  The \code{main()} method does nothing
more than create a Porter stemmer tokenizer,
%
\codeblock{PorterStemmerTokens.1}
%
and then display the results.  

The Ant target \code{porter-stemmer-tokens} runs the program
with a command line argument given by the value of the property
\code{text},
%
\commandlinefollow{ant -Dtext="Smith was bravely charging the front lines" porter-stemmer-tokens}
\begin{verbatim}
Smith was Bravely charging the front lines.
0123456789012345678901234567890123456789012
0         1         2         3         4

START   END TOKEN          START   END TOKEN
    0     5  |Smith|          27    30  |the|
    6     9  |wa|             31    36  |front|
   10    17  |Brave|          37    42  |line|
   18    26  |charg|          42    43  |.|
\end{verbatim}
%
Note that the token spans are for the token before stemming,
so that the token \stringmention{breavely} starts at position
10 (inclusive) and ends at 17 (exclusive), spanning the substring
of the input text \stringmention{bravley}.  

You can see that the resulting tokens are not necessarily words,
with \stringmention{was} stemmed to \stringmention{wa} and
\stringmention{charging} stemmed to \stringmention{charge}.  The
token \stringmention{charge} also stems to \stringmention{charg}.  In
many cases, the Porter stemmer strips off final \charmention{e} and
\charmention{s} characters.  It performs other word-final normalizations,
such as stemming \stringmention{cleanly} and
\stringmention{cleanliness} to \stringmention{cleanli}.

You can also see from the reduction of \stringmention{Bravely}
to \stringmention{Brave} that the stemmer is not case sensitive,
but also does not case normalization.  Typically the input or output
would be case normalized.  You can also see that punctuation, like
the final period, is passed through unchanged.

The Porter stemmer does not build in knowledge of common words of
English.  It thus misses many non-regular relations.  For instance,
consider
%
\commandlinefollow{ant -Dtext="eat eats ate eaten eating" porter-stemmer-tokens}
\begin{verbatim}
START   END TOKEN          START   END TOKEN
    0     3  |eat|            13    18  |eaten|
    4     8  |eat|            19    25  |eat|
    9    12  |at|
\end{verbatim}
%
It fails to equate \stringmention{ate} and \stringmention{eaten} to
the other forms.  And it unfortunately reduces \stringmention{at} to
the same stem as would be found for the preposition of the same
spelling.

The Porter stemmer continues to apply until no more reductions are
possible.  This provides the nice property that for any string
\code{s}, the expression \code{stem(stem(s))} produces a string that's
equal to \code{stem(s)}. 
%
\commandlinefollow{ant -Dtext="fortune fortunate fortunately" porter-stemmer-tokens}
\begin{verbatim}
START   END TOKEN          START   END TOKEN
    0     7  |fortun|         18    29  |fortun|
    8    17  |fortun|         
\end{verbatim}
%

The Porter stemmer does not attempt to remove prefixes.
%
\commandlinefollow{ant -Dtext="antiwar unhappy noncompliant inflammable tripartite" porter-stemmer-tokens}
\begin{verbatim}
START   END TOKEN          START   END TOKEN
    0     7  |antiwar|        29    40  |inflamm|
    8    15  |unhappi|        41    51  |tripartit|
   16    28  |noncompli|
\end{verbatim}


\subsubsection{Thread Safety and Serialization}

A Porter stemmer tokenizer factory is thread safe if its base
tokenizer factory is thread safe.  It is serializable if the base
factory is serializable.


\section{Soundex: Pronunciation-Based Tokens}

For some applications involving unedited or transliterated text, it's
useful to reduce tokens to punctuation.  Although LingPipe does not
contain a general pronunciation module, it does provide a combination
of lightweight pronunciation and stemming through an implementation of
the Soundex System.  Soundex was defined in the early 20th century and
patented by Robert C.~Russell.
%
\footnote{United States Patents 1,261,167 and 1,435,663.}
%
The main application was for indexing card catalogs of names.
The LingPipe version is based on the a modified version of
Soundex known as ``American Soundex,'' which was used by the
United States Census Bureau in the 1930s.  You can still access
census records by Soundex today.

\subsection{The American Soundex Algorithm}

Soundex produces a four-character long representation of arbitrary
words that attempts to model something like a pronunciation class.


\subsubsection{The Algorithm}

The basic procedure, as implemented in LingPipe, is described
in the following pseudocode, which references the character code
table in the next section.  The algorithm itself, as well as the
examples, are based on pseudocode by Donald Knuth.%
%
\footnote{Knuth, Donald E. 1973. {\it The Art of Computer Programming, Volume 3:
 Sorting and Searching}. 2nd Edition.  Addison-Wesley. Pages 394-395.}
%
\begin{enumerate}
\item
Normalize input by removing all characters that are not Latin1
letters, and converting all other characters to uppercase ASCII after
first removing any diacritics.
\item 
If the input is empty, return \stringmention{0000}
\item 
Set the first letter of the output to the first letter of the input.
\item 
While there are less than four letters of output do:
%
\begin{enumerate}
  \item If the next letter is a vowel, unset the last letter's code.
  \item If the next letter is \charmention{A}, \charmention{E}, 
     \charmention{I}, \charmention{O}, \charmention{U}, \charmention{H}, 
     \charmention{W}, \charmention{Y}, continue.
  \item If the next letter's code is equal to the previous letter's code, continue.
  \item Set the next letter of output to the current letter's code. 
\end{enumerate}
\item 
If there are fewer than four characters of output, pad the output with zeros (\charmention{0})
\item 
Return the output string. 
\end{enumerate}

\subsubsection{Character Codes} 

Soundex's character coding attempts to group together letters with
similar sounds.  While this is a reasonable goal, it can't be perfect
(or even reasonably accurate) because pronunciation in English is
so context dependent.

The table of individual character codes is as follows.
%
\begin{center}
\begin{tabular}{ll}
\tblhead{Characters} & \tblhead{Code} \\ \hline
\charmention{B}, \charmention{F}, \charmention{P}, \charmention{V} & 1
\\
\charmention{C}, \charmention{G}, \charmention{J}, \charmention{K}, \charmention{Q}, \charmention{S}, \charmention{X}, \charmention{Z} & 2
\\
\charmention{D}, \charmention{T} & 3
\\
\charmention{L} & 4
\\
\charmention{M}, \charmention{N} & 5
\\
\charmention{R} & 6
\end{tabular}
\end{center}

\subsubsection{Examples}

Here are some examples of Soundex codes, drawn from Knuth's
presentation.
%
\begin{center}
\hfill
\begin{tabular}{ll}
\tblhead{Tokens} & \tblhead{Encoding}
\\ \hline
\stringmention{Gutierrez} & \code{G362}	
\\
\stringmention{Pfister} & \code{P236}	
\\
\stringmention{Jackson} & \code{J250}	
\\
\stringmention{Tymczak} & \code{T522}	
\\
\stringmention{Ashcraft} & \code{A261}	
\\
\stringmention{Robert}, \stringmention{Rupert} & \code{R163}	
\\
\stringmention{Euler}, \stringmention{Ellery} & \code{E460}	
\end{tabular}
\hfill
\begin{tabular}{ll}
\tblhead{Tokens} & \tblhead{Encoding}
\\ \hline
\stringmention{Gauss}, \stringmention{Ghosh} & \code{G200}	
\\
\stringmention{Hilbert}, \stringmention{Heilbronn} & \code{H416}	
\\
\stringmention{Knuth}, \stringmention{Kant} & \code{K530}	
\\
\stringmention{Lloyd}, \stringmention{Liddy} & \code{L300}	
\\
\stringmention{Lukasiewicz}, \stringmention{Lissajous} & \code{L222}	
\\
\stringmention{Wachs}, \stringmention{Waugh} & \code{W200}
\end{tabular}
\hfill
\end{center}

\subsection{The \code{SoundexTokenizerFactory} Class}

LingPipe provides a tokenizer factory filter based on the
American Soundex defined above in the class \code{SoundexTokenizerFactory}
in the package \code{com.aliasi.tokenizer}.

Like the Porter stemmer tokenizer filter, there is a static utility
method, here named \code{soundexEncoding(String)}, for carrying out
the basic encoding.  Provide a token, get its American Soundex encoding
back.  Also like the Porter stemmer, the rest of the implementation just
uses this static method to transform tokens as they stream by.

\subsubsection{Demo}

We provide a demo of the Soundex encodings in the class
\code{SoundexTokens}.  Like the Porter stemmer demo,%
%
\footnote{While we could've encapsulated the text and display
functionality, we're trying to keep our demos relatively
independent from one another so that they are easier to follow.}
%
it just wraps an Indo-European tokenizer factory in a Soundex
tokenizer factory.
%
\codeblock{SoundexTokens.1}
%

Also like the Porter stemmer demo, the \code{main()} method
consumes a single command-line argument and displays the
text with indexes and then the resulting tokens.  

We provide an Ant target \code{soundex-tokens}, which 
feeds the value of property \code{text} to the program.
%
\commandlinefollow{ant -Dtext="Mr.\ Roberts knows Don Knuth." soundex-tokens}
\begin{verbatim}
Mr. Roberts knows Don Knuth.
0123456789012345678901234567
0         1         2

START   END TOKEN          START   END TOKEN
    0     2  |M600|           18    21  |D500|
    2     3  |0000|           22    27  |K530|
    4    11  |R163|           27    28  |0000|
   12    17  |K520|
\end{verbatim}
%
For instance, the first token spans from 0 to 2,
covering the text \stringmention{Mr} and producing
the Soundex encoding \code{M600}.  The following
period, spanning from position 2 to 3, gets code
\code{0000}, because it is not all Latin1 characters.
The token spanning 12 to 17, covering text \stringmention{Roberts},
gets code \code{R163}, the same as \stringmention{Robert} in the table
of examples.  Similarly, we see that \stringmention{Knuth} gets its
proper code from positions 22 to 27.

From this example, we see that it might make sense to follow the
Soundex tokenizer with a stop list tokenizer that removes the token
\stringmention{0000}, which is unlikely to be very informative in most
contexts.


\subsection{Variants of Soundex}

Over the years, Soundex-like algorithms have been developed for many
languages.  Other pronunciation-based stemmers have also been
developed, perhaps the most popular being Metaphone and its variant,
Double Metaphone.


\section{Character Normalizing Tokenizer Filters}

We demonstrated how to use the International Components for Unicide
(ICU) package for normalizing sequences of Unicode characters in
\refsec{icu-unicode-normalization}.  In this section, we'll show how
to plug that together with tokenization to provide tokenizers that
produce output with normalized text.  

The alternative to normalizing with a token filter would be to
normalize the text before input.  As a general rule, we prefer not to
modify the original underlying text if it can be avoided.  A case
where it makes sense to parse or modify text before processing would
be an XML and HTML document.  

\subsection{Demo: ICU Transliteration}

We wrote a demo class, \code{UnicodeNormTokenizerFactory}, which
uses the the \code{Transliterate} class from ICU to do the work (see
\refsec{char-unicode-transliterate} for an introduction to the class and
more examples).  A transliterator is configured with a transliteration
scheme, so we configure our factory the same way,
%
\codeblock{UnicodeNormTokenizerFactory.1}
%
As with our other filters, we extend
\code{ModifyTokenTokenizerFactory}, and then do the actualy work in
the \code{modifyToken(String)} method.  Here, the constructor stores
the scheme and creates the transliterator using the static factory
method \code{getInstance(String)} from \code{Transliterator}.
Transliteration operations in ICU are thread safe as long as the
transliterator is not modified during the transliteration.

The class also includes a definition of a serialization proxy,
which only needs to store the scheme and base tokenizer in order
to reconstruct the transliterating tokenizer.

The Ant target \code{unicode-norm-tokens} runs the program,
using the property \code{translit.scheme} to define the
transliteration scheme (see \refsec{char-unicode-transliterate} for
a description of the available schemes and the language for
composing them) and property \code{text} for the text to modify.
%
\commandlinefollow{ant "-Dtext=Dj vu" "-DtranslitScheme=NFD; [:Nonspacing Mark:] Remove; NFC" unicode-norm-tokens}
\begin{verbatim}
translitScheme=NFD; [:Nonspacing Mark:] Remove; NFC

Dj vu
0123456

START   END TOKEN
    0     4  |Deja|
    5     7  |vu|
\end{verbatim}
%
To get the output encoded correctly, we had to reset \code{System.out}
to use UTF-8 (see \refsec{io-reset-system-out}).


\section{Penn Treebank Tokenization}

One of the more widely used resources for natural language
processing is the Penn Treebank.%
%
\footnote{Marcus, Mitchell P., Beatrice Santorini, Mary Ann
  Marcinkiewicz, and Ann Taylor. 1999. {\it Treebank-3}. Linguistic
  Data Consortium.  University of Pennsylvania.  Catalog number
  LDC99T42.}
%

\subsection{Pre-Tokenized Datasets}

Like many older data sets, the Penn Treebank is presented only in
terms of its tokens, each separated by a space.  So there's no way to
get back to the underlying text.  This is convenient because it allows
very simple scripts to be used to parse data.  Thus it is easy to
train and evaluate models.  You don't even have to write a tokenizer,
and there can't be any tokenization errors.  More modern datasets tend
to use either XML infix annotation or offset annotation, allowing the
boundaries of tokens in the original text to be recovered.

The major drawback is that there is no access to the underlying text.
As a consequence, there is no way to train whitespace-sensitive models
using the Penn Treebank as is (it might be possible to reconstitute
the token alignments given the original texts and the parser).

Although it is easy to train and evaluate in a pre-tokenized data
set, using it on new data from the wild requires an implementation
of the tokenization scheme used to create the corpus.

\subsection{The Treebank Tokenization Scheme}

In this section, we provide an overview of the Penn Treebank's
tokenization scheme.%
%
\footnote{We draw from the online description
  \url{http://www.cis.upenn.edu/~treebank/tokenization.html}
  (downloaded 30 July 2010) and Robert MacIntyre's implementation as
  a SED script, released under the Gnu Public License, in
  \url{http://www.cis.upenn.edu/~treebank/tokenizer.sed}.}
%
The major distinguishing feature of the Treebank's tokenizer is that
they analyze punctuation based on context.  For instance, a period at
the end of the sentence is treated as a standalone token, whereas an
abbreviation followed by a period, such as \stringmention{Mr.}, is
treated as a single token.  Thus the input needs to already be
segmented into sentences, a process we consider in
\refchap{sentence}.

The Treebank tokenizer was designed for the ASCII chaacters
(\unicode{0000}--\unicode{007F}), which means it is not defined for
characters such as Unicode punctuation.  Following the convention of
\LaTeX rather than Unicode, the Treebank tokenizer analyzes a quote
character (\charmention{"}) as two grave accents (\charmention{``}) if
it's an opening qoe and two apostrophes (\charmention{''}) if it's a
closing quote.  Like periods, determining the status of the quotes
requires context.

The Treebank tokenizer is also English specific.  In its
context-sensitive treatment of apostrophes (\charmention{'}), it
attempts to distinguish the underlying words making up contractions
together, so that a word like \stringmention{I'm} is split into two
tokens, \stringmention{I} and \stringmention{'m}, the second of which
is the contracted form of \stringmention{am}.  To handle negative
contractions, it keeps the negation together, splitting
\stringmention{don't} into \stringmention{do} and \stringmention{n't}.
The reason for this is that the Treebank labels syntactic categories,
and they wanted to assign \stringmention{I} to a pronoun category and
\stringmention{'m} to an auxiliary verb category just like
\stringmention{am}; similarly, \stringmention{do} is an auxiliary verb
and \stringmention{n't} a negative particle.

Ellipses, which appear in ASCII text as a sequence of periods
(\stringmention{...}), are handled as single tokens.  Pairs of hyphens
(\stringmention{{-}{-}}), which are found as an ASCII replacement for
an en-dash (\charmention{{}--{}}).

Other than the treatment of periods, quotation marks and apostrophes
in the aforementioned contexts, the Treebank tokenizer splits out
other punctuation characters into their own tokens.

The ASCII bracket characters, left and right, round and square and
curly, are replaced with acronyms and hyphens.  For instance,
\unicode{005B}, \unicodedesc{left square bracket} (\charmention{[})
  produces the token \stringmention{-LSB-}.  The acronym
  \charmention{LSB} stands for left square bracket; other brackets are
  coded similarly alternating \charmention{R} for right in the first
  position, and \charmention{R} for round and \charmention{C} for
  curly in the second position.

\subsection{Demo: Penn Treebank Tokenizer}

We provide a port and generalization of the Penn Treebank tokenizer
based on Robert MacIntyre's SED script in the class
\code{PennTreebankTokenizerFactory}.  We have also generalized it
to Unicode character codes where possible.  

The original SED script worked by transforming the input string by
adding or deleting spaces and transforming characters.  If we transformed
the input, the positions of the tokens would all be off unless we
somehow kept track of them.  Luckily, an easier solution presents itself
with Java regular expressions and LingPipe's built-in regex tokenizer
factories and token filter.

Our implementation works in three parts.  First, the original string
is transformed, but in a position-preserving way.  This mainly takes
care of issues in detecting opening versus closing quotes.  Second,
a regular expression tokenizer is used to break the input into tokens.
Third, the tokens are transformed on a token-by-token basis into
the Penn Treebank's representation. 

\subsubsection{Construction}

Before any of these steps, we declare the tokenizer factory to extend
the token-modifying token factory and to be serailizable,
%
\codeblock{PennTreebankTokenizerFactory.1}
%
We had to supply the base tokenizer factory as an argument
to \code{super()} in order to supply the superclass with its
base tokenizer.  This is a regex-based tokenizer factory which
we describe below.  There's also a serialization proxy implementation
we don't show.

We chose to implement the Penn Treebank tokenizer as a singleton, with
a private constructor and static constant instance.  The constructor
could've just as easily been made public, but we only need a single
instance.

\subsubsection{Input String Munging}

In the first processing step, we override the tokenization method
itself to trasnform the input text using a sequence of regex replace
operations (see \refsec{regex-replace}),
%
\codeblock{PennTreebankTokenizerFactory.2}
%
The \code{replaceAll()} methods chain, starting with a string produced
from the character slice input.  The first replacement replaces an
instance of \unicode{0022}, \unicodedesc{quotation mark}, (\charmention{"}),
the ordinary typewriter non-directional quotation mark symbol, with 
\unicode{201C}, \unicodedesc{left double quotation mark} (\charmention{``{}}).
The second replacement replaces a quotation mark after characters of
class \code{Z} (separators such as spaces, line or paragraph
separators) or class \code{Ps}, the start-punctuation class, which
includes all the open brackets specified in the original SED script.
The context characters are specified as a non-capturing lookbehind
(see \refsec{regex-lookbehind}), The third replacement converts
remaining quote characters
\unicode{201D}, \unicodedesc{right double quotation mark}
(\charmention({}'')).  The fourth replacement converts any character
typed as initial quote punctuation (\code{Pi}) and replaces it with a
left double quotation mark, and similarly for final quote punctuation
(\code{Pf}).  

We take the final string and convert it to a character array to send
the superclass's implementation of \code{tokenizer()}.  We have been
careful to only use replacements that do not modify the length of the
input, so that all of our offsets for tokens will remain correct.  


\subsubsection{Base Tokenizer Regex}

The implementation of \code{tokenize()} in the superclass
\code{ModifyTokenTokenizerFactory} invokes the base
tokenizer, which was supplied to the constructor, then feeds the
results through the \code{modifyToken()} method, which we describe
below.

The base tokenizer factory is constructed from a constant representing
the regular expression and the specification that matching is to
be case insensitive.  The regex used is
%
\codeblock{PennTreebankTokenizerFactory.4}
%
We have broken it out into pieces and used string concatenation to put
it back together so it's more readable.  

The first disjunct allows three period characters to be a token, the
second a pair of dashes.  

The next thirteen disjuncts are spread over several lines.  Each of these
simply breaks a compound apart.  For instance, \stringmention{cannot}
will be split into two tokens, \stringmention{can} and
\stringmention{not}.  This is achieved in each case by matching the
first half of a compound when followed by the second half.  The
following context is a positive lookahead, requiring the rest of the
compound and then a word boundary (regex \code{{\bk}b} is the word
boundary matcher).

The next two disjuncts pull out the contracted part of contractions,
such as \charmention{'s}, \charmention{'ve} and \charmention{n't}.  

Next, we match tokens consisting of sequences of letters and/or
numbers (Unicode types \code{L} and \code{N}).  The first disjunct
pulls off tokens that directly precede the final period or the
sequence \stringmention{n't}.  The disjunct uses positive lookahead to
match the following context.  Because regexes are greedy, the disjunct
just described will match if possible.  If not, periods are included
alongw ith letters and numbers (but no other punctuation).

The final disjunct treats any other non-separator character as
a token.

\subsubsection{Modified Token Output}

To conform to the Penn Treebank's output format for characters,
we do a final replace on the tokens, using
%
\codeblock{PennTreebankTokenizerFactory.3}
%
Each of the ASCII brackets is replaced with a five-letter sequence.
Any initial quote characters are replaced with two grave accents
(\stringmention{{`}{`}}) and then final quote characters are replaced
with two apostrophes (\stringmention{{'}{'}}).

\subsubsection{Running It}

The Ant target \code{treebank-tokens} runs the Penn Treebank tokenizer,
supplying the value of the Ant property \code{text} as the first
argument, which is the string to tokenize.
%
\commandlinefollow{ant -Dtext="Ms. Smith's papers' -- weren't (gonna) miss." treebank-tokens}
\begin{verbatim}
Ms. Smith's papers' -- weren't (gonna) miss.
01234567890123456789012345678901234567890123
0         1         2         3         4

START   END TOKEN          START   END TOKEN
    0     3  |Ms.|            27    30  |n't|
    4     9  |Smith|          31    32  |-LRB-|
    9    11  |'s|             32    35  |gon|
   12    18  |papers|         35    37  |na|
   18    19  |'|              37    38  |-RRB-|
   20    22  |--|             39    43  |miss|
   23    27  |were|           43    44  |.|
\end{verbatim}
%
This short example illustrates the different handling of periods
sentence-internally and finally, the treatment of contractions,
distinguished tokens like the double-dash, and compounds like
\charmention{gonna}.  It also shows how the brackets get
substituted, for instance the left with token \charmention{-LRB-};
note that these have their original spans, so that the
token \charmention{-RRB-} spans from 37 (inclusive) to 38 (exclusive),
the position of the right round bracket in the original text.

The first disjunct after all the special cases allows a sequence of
letters and numbers to form a token subject to the negative lookahead
constraint.  The negative lookahead makes sure that periods are not
picked up as part of tokens if they are followed by the end of input
or a non-empty sequence of punctuation characters including final
punctuation like right brackets (Unicode class \code{Pf}), the double
quote character or the apostrophe.  
%
\commandlinefollow{ant -Dtext="(John ran.)" treebank-tokens}
\begin{verbatim}
(john ran.)
01234567890

START   END TOKEN          START   END TOKEN
    0     1  |-LRB-|           9    10  |.|
    1     5  |John|           10    11  |-RRB-|
    6     9  |ran|
\end{verbatim}
%
The period is not tokenized along with \stringmention{ran}, because it
is followed by a right round bracket character (\charmention{)}), an
instance of final punctuation matching \code{Pf}.  Without the
negative lookahead disjunct appearing before the general disjunct, the
period would've been included.  

The final disjunct in the negative
lookahead constraint ensures the suffix \stringmention{n't} is kept
together as we saw in the first example; without lookahead, the
\stringmention{n} would be part of the previous token.

Given how difficult it is to get a standard quote character into a
property definition,%
%
\footnote{Although the double quote character itself can be escaped in
  any halfway decent shell, they all do it differently.  The real
  problem is that Ant's scripts can't handle \code{-D} property
  specifications with embedded quotes.}
% 
we've used a properties file in
\filepath{config/text2.properties}.  The content of the properties
file is 
%
\begin{verbatim}
text=I say, \\"John runs\\".
\end{verbatim}
%
Recall that in Java properties files, strings are parsed in the same
way as string literals in Java programs.  In particular, we can use
Unicode and Java backslash escapes,%
%
\footnote{Unicode escapes for the quote character will be interpreted
rather than literal, so you need to use the backslash.  And the
backslash needs to be escaped, just as in Java source.}
%
We run the Ant target by specifying the properties file on the
command line,
%
\commandlinefollow{ant -propertyfile=config/text2.properties treebank-tokens}
\begin{verbatim}
I say, "John runs".
0123456789012345678
0         1

START   END TOKEN          START   END TOKEN
    0     1  |I|               8    12  |John|
    2     5  |say|            13    17  |runs|
    5     6  |,|              17    18  |''|
    7     8  |``|             18    19  |.|
\end{verbatim}
%
The output demonstrates how open double quotes are treated differently
than close double quotes.  Like the bracket token substitutions,
the two characters substituted for quotes still span only a single token
of the input.


\subsubsection{Discussion}

There are several problems with the Penn Treebank tokenizer as
presented.  The problems are typical of systems developed on limited
sets of data with the goal of parsing the data at hand rather than
generalizing.  We fixed one such problem, only recognizing ASCII
quotes and only recognizing ASCII brackets, by generalizing to Unicode
classes in our regular expressions.

Another problem, which is fairly obvious from looking at the code, is
that we've only handled a small set of contractions.  We haven't
handled \stringmention{y'all} or \stringmention{c'mon}, which are in
everyday use in English.  I happen to be a fan of nautical fiction,
but the tokenizer will stumble on \stringmention{fo'c's'le}, the
conventional contraction of \stringmention{forecastle}.  Things get
even worse when we move into technical domains like chemistry or
mathematics, where apostrophes are used as primes or have other
meanings.

We've also only handled a small set of word-like punctuation.  What
about the e-mail smiley punctuation, \stringmention{:-)}?  It's a unit,
but will be broken into three tokens by the current tokenizer.


\section{Lucene's Analyzers}

Given a set of texts and a tokenization scheme to map texts to
sequences of tokens, a reverse index is a mapping from tokens to the
set of documents in which they occur.  Such reverse indexes form
the backbone of search APIs like Apache's Lucene.

\subsection{Documents, Fields and Analyzers}

In Lucene, a document consists (in part) of a mapping from string
field names to string values.%
%
\footnote{It may also contain values that are binary blobs.  Each
  field is also marked as to whether it is (a) reverse indexed, (b)
  tokenized for indexing, and (c) stored in the document store.}
%
Lucene manages reverse indexes for each indexed field of a document.
Each field may have its own tokenization scheme.  

Lucene's abstract base class \code{TokenStream}, in package
\code{org.apache.lucene.analysis}, provides a streaming interface to
tokens serving the same role as LingPipe's \code{Tokenizer} abstract
base class.

LingPipe's \code{TokenizerFactory} interface provides a representation
of a mapping from character sequences to tokenizers.  Lucene bypasses
this level of representation.  Instead, it uses the \code{Analyzer}
abstract base class in package \code{org.apache.lucene.analysis} to
map fields and sequences of characters into token streams.  The method
\code{tokenStream(String,Reader)} returns a \code{TokenStream} given
the name of the field being tokenized and the text to be tokenized as
a Java \code{Reader} (character input stream).


\subsection{Token Streams and Attributes}

Before version 3.0 of Lucene, token streams had a string-position
oriented tokenization API, much like LingPipe's tokenizers.  Version
3.0 generalized the interface for token streams and other basic
objects using a very general pattern based on what they call
attributes.

\subsubsection{Code Walkthrough}

We provide a sample class \code{LuceneAnalysis} that applies an
analyzer to a field name and text input and prints out the resulting
tokens.  The work is done in a simple \code{main()} with two
arguments, the field name and the text to be analyzed.  

The first step is to create the analyzer.
%
\codeblock{LuceneAnalysis.1}
%
Here we've used Lucene's \code{StandardAnalyzer}, which applies case
normalization and English stoplisting to their basic standard
tokenizer, which pays attentiont to issues like periods and e-mail
addresses.  Note that it's constructed with a constant for the Lucene
version, as the behavior has changed over time.

The standard analyzer, like almost all of Lucene's built-in analyzers,
ignores the name of the field that is passed in.  Such analyzers
essentially implement simple token stream factories, like LingPipe's
tokenizer factories.

The next step of the \code{main()} method constructs the token stream
given the string values of the command-line arguments \code{fieldName}
and \code{text}.  
%
\codeblock{LuceneAnalysis.2}
%
We first have to create a \code{Reader}, which we do by wrapping the
input text string in a \code{StringReader} (from \code{java.io}).
Then we use the analyzer to create a token stream from the field name
and text reader.  The next three statements attach attributes to the
token stream, specifically a term attribute, offset attribute and
position increment attribute.  These are used to retrieve the text of
a term, the span of the term in the original text, and the ordinal
position of the term in the sequence of terms in the document.  The
position is given by an increment from the previous position, and
Lucene uses these values for phrase-based search \ie{searching for a
fixed sequence of tokens in the given order without intervening
material}.

The last block of code in the \code{main()} method iterates through
the tokens tream, printing the attributes of each token it finds.
%
\codeblock{LuceneAnalysis.3}
%
The while loop continually calls \code{incrementToken()} on the token
stream, which advances to the next token, returning \code{true} if
there are more tokens.  The body of the loop just pulls out the
increment, start and end positions, and term for the token.  The
rest of the code, which isn't shown, just prints these values.

\subsubsection{Running the Demo}

It may be run from the Ant target \code{lucene-analysis}, with
the arguments provided by properties \code{field.name} and 
\code{text} respectively.

\commandlinefollow{ant -Dfield.name=foo -Dtext="Mr.\ Sutton-Smith will pay \$1.20 for the book." lucene-analysis}
\begin{verbatim}
Mr. Sutton-Smith will pay $1.20 for the book.
012345678901234567890123456789012345678901234
0         1         2         3         4

 INCR (START,   END) TERM         INCR (START,   END) TERM
    1 (    0,     2) mr              2 (   22,    25) pay
    1 (    4,    10) sutton          1 (   27,    31) 1.20
    1 (   11,    16) smith           3 (   40,    44) book
\end{verbatim}
%
The terms are all lowercased, and non-word-internal punctuation has
been removed.  The stop words \charmention{will}, \charmention{for}
and \charmention{the} are also removed from the output.  Unlike
punctuation, when a stop word is removed, it causes the increment
between terms to be larger.  For instance, the increment between
\charmention{smith} and \charmention{pay} is 2, because the stopword
\charmention{will} was removed between them.  

The start (inclusive) and end (exclusive) positions of the
extracted terms is also shown.


\subsection{Adapting Analyzers for Tokenizer Factories}

Because of the wide range of basic analyzers and filters employed by
Lucene, it's useful to be able to wrap a Lucene analyzer for use in
LingPipe.  For instance, we could analyze Arabic with stemming this
way, or even Chinese.

The demo class \code{AnalyzerTokenizerFactory} implements LingPipe's
\code{TokenizerFactory} interface based on a Lucene \code{Analyzer}
and field name.  In general design terms, the class adapts an analyzer
to do the work of a tokenizer factory.

The class is declared to implement the tokenizer factory interface to
be serializable,
%
\codeblock{AnalyzerTokenizerFactory.1}
%
The constructor simply stores an analyzer and field name in private
final member variables.

The only method we need to implement is \code{tokenizer()},
%
\codeblock{AnalyzerTokenizerFactory.2}
%
It creates a reader from the character array slice and then uses the
stored analyzer to convert it to a Lucene \code{TokenStream}.  A
new instance of \code{TokenStreamTokenizer} is constructed based
on the token stream and returned.

The \code{TokenStreamTokenizer} class is a nested static class.
It is defined to extend LingPipe's \code{Tokenizer} base class and
implement Java's \code{Closeable} interface.
%
\codeblock{AnalyzerTokenizerFactory.3}
%
The constructor stores the token stream in a member variable, along
with the term and offset attributes it adds to the token stream.
The token start positions are initialized to -1, which is the proper
return value before any tokens have been found.

The \code{nextToken()} method is implemented by delegating to the
contained token stream's \code{incrementToken()} method, which
advances the underlying token stream.  
%
\codeblock{AnalyzerTokenizerFactory.4}
After the increment, the start
and end positions are stored befre returning the term as the next
token.  If the token stream is finished or the increment method throws
an I/O exception, the stream is ended and closed quietly (see below).
%
\codeblock{AnalyzerTokenizerFactory.6}

The methods for start and end just return the stored
values.  For example,
%
\codeblock{AnalyzerTokenizerFactory.5}

The \code{main()} method for the demo takes a command-line argument
for the text.  It sets up a standard Lucene analyzer, then constructs
a tokenizer factory from it using the field \code{text} (the field
doesn't matter to the standard analyzer, but may matter for other
analyzers).  
%
\codeblock{AnalyzerTokenizerFactory.7}
%
We then just display the text and tokens as we have in
previous demos.

The Ant target \code{lucene-lp-tokens} runs the example, with
property \code{text} passed in as the command-line argument.
%
\commandlinefollow{ant -Dtext="Mr.\ Smith is happy!" lucene-lp-tokens}
\begin{verbatim}
Mr. Smith is happy!
0123456789012345678
0         1

START   END TOKEN          START   END TOKEN
    0     2  |mr|             13    18  |happy|
    4     9  |smith|
\end{verbatim}
%
As before, we see the stoplisting and case normalization of
the standard Lucene analyzer.

\subsubsection{An Arabic Analyzer with Stemming and Stoplisting}

The contributed Lucene class \code{ArabicAnalyzer} implements
Larkey, Ballesteros and Connell's tokenizer and stemmer.%
%
\footnote{Larkey, Leah, Lisa Ballesteros and Margaret
  Connell. 2007. Light Stemming for Arabic Information Retrieval. In
  A.~Soudi, A.~van~den~Bosch, and G.~Neumann, (eds.) {\it Arabic
    Computational Morphology}. 221--243.  Springer.}
%
This class performs basic word segmentation, character normalization,
``light'' stemming, and stop listing, with a configuarable stop list.
Because it's a contributed class, it's found in a separate jar,
\code{lucene-analyzers-3.0.2.jar}, which we've included with the
distribution and inserted into the classpath for this chapter's Ant
build file.

We provide a demo class \code{ArabicTokenizerFactory}, which provides
a static singleton implementation of a tokenizer factory wrapping
Lucene's Arabic analyzer.  
We don't need anything other than the
Lucene \code{Analyzer} and LingPipe \code{TokenizerFactory},
%
\codeblock{ArabicTokenizerFactory.1}
%
In the file \code{src/tok/config/arabic-sample.utf8.txt}, we
have placed some Arabic text%
%
\footnote{Downloaded from 
\url{http://ar.wikipedia.org/wiki/\%D9\%86\%D9\%87\%D8\%B1_\%D8\%A3\%D9\%88\%D9\%84\%D9\%88\%D9\%8A\%D9\%88\%D9\%83\%D9\%8A}
on 9 July 2010.}
%
encoded with UTF-8; the first few lines of the file are as follows.
%
\begin{quote}
\begin{RLtext}
 $\mbox{\rm (Oulujoki)}$     . 
        
     .    12 
      
 541 . ...
\end{RLtext}
\end{quote}
%

There is a \code{main()} class that reads a file name from the command
line, reads the file in and tokenizes it for display.  The Ant target
\code{arabic-tokens} runs the tokenizer over a file specified by name
as the first command specified file,
%
\commandlinefollow{ant "-Dfile=config{\bk}arabic-sample.utf8.txt" arabic-tokens}
\begin{Verbatim}[commandchars=\!\[\],commentchar=\^]
file=config\arabic-sample.utf8.txt
 -----text-----
!RL[] (Oulujoki) ...
...
from: http://ar.wikipedia.org/wiki/!RL[]_!RL[]
-----end text-----
START   END TOKEN
    0     8  |!RL[]|
   10    18  |oulujoki|      
   20    23  |!RL[]|
   27    33  |!RL[]|   
...
  292   295  |org|
  296   300  |wiki|
  301   309  |!RL[]|
  310   313  |!RL[]|
\end{Verbatim}
%
Note that the text mixes Roman characters and Arabic characters.  The
source URL is part of the text.  The Roman characters are separated
out as their own tokens and retained, but all punctuation is ignored.
The very first token, which is also part of the URL, is stemmed.

\subsection{Adapting Tokenizer Factories for Analyzers}

Recall that a Lucene analyzer maps fields and readers to token
streams.  We can implement an analyzer given a mapping from fields to
tokenizer factories.  We will also use a default tokenizer factory to
apply to fields that are not defined in the map.

The demo class \code{TokenizerFactoryAnalyzer} provides such
an implementation.  The class definition and constructor are
%
\codeblock{TokenizerFactoryAnalyzer.1}
%
The mapping and default factory are stored as final member variables.

Version 3.0 of Lucene has been defined to allow greater efficiency
through object pooling, such as reuse of token streams.  This makes
the implementation of \code{tokenStream()} more complex than may at
first seem necessary.  
%
\codeblock{TokenizerFactoryAnalyzer.2}
%
The method creates a new instance of \code{TokenizerTokenStream}, the
definition of which we show below.  It then sets the field name using
the method \code{setField()} and the reader using the method
\code{reset()}.  If the setting a field throws an exception, the
\code{tokenStream()} method returns an instance
\code{EmptyTokenStream}.%
%
\footnote{From the package
  \code{org.apache.lucene.analysis.miscellaneous}, in the contributed
  analyzers library, \code{lucene-analyzers-3.0.2.jar}.}
%

We also define the \code{reusableTokenStream()} method for efficiency,
which reuses the previous token stream if there is one or uses
\code{tokenStream()} to create one.  

The class \code{TokenizerTokenStream} is defined as a (non-static)
inner class.  
%
\codeblock{TokenizerFactoryAnalyzer.3}
%
It is defined to extend Lucene's \code{Tokenizer} interface, whose
full package, \code{org.apache.lucene.analysis}, is included to avoid
conflict with LingPipe's \code{Tokenizer} class.  Lucene's
\code{Tokenizer} class is a subclass of \code{TokenStream} defined to
support basic tokenizers (the other abstract base class supports token
stream filters).  As with our use of token streams in the previous
section to adapt analyzers to tokenizer factories, we again add and
store the term, offset and position increment attributes.

The field and reader are set and reset as follows.
%
\codeblock{TokenizerFactoryAnalyzer.4}
%
The \code{setField()} method simply stores the field name.  The
\code{reset(Reader)} method extracts and stores the characters from
the reader using LingPipe's utility method \code{toCharArray()}.
The tokenizer factory is retrieved from the map using the field
name as a key, returning the default tokenizer factory if the
field name is not in the map.  After setting the characters and
tokenizer, it calls the \code{reset()} method.  The \code{reset()}
method sets the tokenizer based on the current tokenizer factory
and stored strings, throwing an exception if \code{reset(Reader)}
has not yet been called or the \code{close()} method has been
called.

Once a token stream is set up, its \code{incrementToken()} method
sets up teh properties for the next token if there is one.
%
\codeblock{TokenizerFactoryAnalyzer.5}
%
It gathers the token itself from the LingPipe tokenizer stored in
member variable \code{mTokenizer}.  If the token's \code{null}, we've
reached the end of the token stream, and return \code{false} from the
\code{incrementToken()} method to signal the end of stream.
Otherwise, we grab the character buffer from the term attribute and
fill it with the characters from the token using the \code{String}
method \code{getChars()}.  We also set the Lucene tokenizr's offset
attribute based on the LingPipe tokenizer's start and end position.

Lucene's \code{Tokenizer} class specifies a method \code{end} that
advances the token position past the final token; we set the offset to
indicate there is no more of teh input string left.  The
\code{close()} method, which is intended to be called after a client
is finished with the token stream, frees up the character array
used to store the characters being tokenized.

\subsubsection{Tokenizer Factory Analzyer Demo}

There's a \code{main()} method in the \code{TokenizerFactoryAnalzyer}
class that we will use as a demo.  It mirrors the Lucene analysis
demo.  
%
\codeblock{TokenizerFactoryAnalyzer.7}
%
First, we grab the text from the first command-line argument.  Then
we set up the mapping from field names to tokenizer factories, using
an Indo-European tokenizer factory for field \code{foo} and
a trigram tokenizer factory for field \code{bar}.  The default
tokenizer factory is based on a regex matching sequences of
non-space characters.  Finally, the analyzer is constructed from
the map and default factory.  The rest just prints the tokens and
the results of the analysis.


The Ant target \code{lp-lucene-tokens} runs the command with the
value of property \code{text} passed in as an argument.
%
\commandlinefollow{ant -Dtext="Jim ran." lp-lucene-tokens}
\begin{verbatim}
Jim ran.
01234567

Field=foo                   Field=jib
  Pos Start   End            Pos Start   End              
    1     0     3 Jim          1     0     3 Jim
    1     4     7 ran          1     4     8 ran.
    1     7     8 .

Field=bar
  Pos Start   End
    1     0     3 Jim
    1     1     4 im
...
    1     5     8 an.
\end{verbatim}
%
We see the tokens produced in each field are derived in the case of
\code{foo} from the Indo-European tokenzier factory, in the case of
\code{bar} from the trigram tokenizer factory, and in the case of the
feature \code{jar}, the default tokenizer factory.  In all cases, the
position increment is 1; LingPipe's tokenization framework is not set up
to account for position offsets for phrasal search.


\section{Tokenizations as Objects}

In our examples so far, we have used the iterator-like streaming
interface to tokens. 

\subsection{Tokenizer Results}

The \code{Tokenizer} base class itself provides two ways to get at the
entire sequence of tokens (and whitespaces) produced by a tokenizer.  

The method \code{tokenize()} returns a string array containing the
sequence of tokens produced by the tokenizer.  
%
\codeblock{FragmentsTok.1}
%
The method \code{tokenize(List<String>,List<String>)} adds the lists
of tokens to the first list specified and the list of tokens to the
second list.  The usage pattern is
%
\codeblock{FragmentsTok.2}
%
In both cases, the results are only sequences of tokens and
whitespaces.  The underlying string being tokenized nor the
positions of the tokens are part of the result.

\subsection{The \code{Tokenization} Class}

The class \code{Tokenization} in \code{com.aliasi.tokenizer}
represents all of the information in a complete tokenization of an
input character sequence.  This includes the sequence of tokens, the
sequence of whitespaces, the character sequence that was tokenized,
and the start and end positions of each token.

\subsubsection{Constructing a \code{Tokenization}}

There are three constructors for the \code{Tokenization} class.  Two
of them, \code{Tokenization(char[],int,int,TokenizerFactory)} and
\code{Tokenization(CharSequence,TokenizerFactory)}, take a tokenizer
factory and a sequence of characters as input, and store the results
of the tokenizer produced by the tokenizer factory.  The third
constructor takes all of the components of a tokenization as
arguments, the text, tokens, whitespaces, and token positions,
\code{Tokenization(String,List<String>,List<String>,int[],int[])}.

The constructors all store copies of their arguments so that they are
not linked to the constructor arguments.


\subsubsection{Getters for \code{Tokenization}}

Once constructed, a tokenization is immutable.  There are getters for
all of the information in a tokenization.  The method \code{text()}
returns the underlying text that was tokenized.  The method
\code{numTokens()} returns the total number of tokens (there will be
one more whitespace than tokens).  The methods \code{token(int)} and
\code{whitespace(int)} return the token and whitespace at the
specified position.  The methods \code{tokenStart(int)} and
\code{tokenEnd(int)} return the starting offset (inclusive) of the
token and the ending position (exclusive0 for the token at the
specified index.

There are also methods to retrieve sequences of values.  The method
\code{tokenList()} returns an unmodifiable view of the underlying
tokens and \code{tokens()} returns a copy of an array of the tokens.
The methods \code{whitespaceList()} and \code{whitespaces()} are
similar.  


\subsubsection{Equality and Hash Codes}

Two tokenization objects are equal if their character sequences
poroduce the same strings, they have equivalent token and
whitespace lists, and the arrays of start and end positions for
tokens are the same.  Hash codes are defined consistently with
equality.


\subsubsection{Serialization}

A tokenization may be serialized.  The deserialized object is 
a \code{Tokenization} that is equal to the first.


\subsubsection{Thread Safety}

Tokenizations are immutable and completely thread safe once
constructed.

\subsubsection{Demo of \code{Tokenization}}

The demo class \code{DisplayTokenization} provides an example of how
the \code{Tokenization} class may be used.  The work is in the
\code{main()} method, which begins as follows.
%
\codeblock{DisplayTokenization.1}
%
It begins by creatig a tokenizer factory from a regex that matches
arbitrary sequences of letters as tokens.  Then it creates the
\code{Tokenization} object using the text from the command-line
argument and the tokenizer factory.  Then we collect up the
list of tokens, list of whitespaces and underlying text.
%
The command continues by looping over the tokens.
%
\codeblock{DisplayTokenization.2}
%
For each token, it extracts the start position (inclusive), end
position (exclusive), whitespace before the token, and the token
itself.  These are printed.  After the loop, it prints out
the final whitespace.
%
\codeblock{DisplayTokenization.3}

The Ant target \code{tokenization} runs the demo command with
the value of property \code{text} as the command-line argument.
%
\commandlinefollow{ant -Dtext="John didn't run.\ " tokenization}
\begin{verbatim}
tokenList=[John, didn, t, run]
whitespaceList=[,  , ',  , . ]
textTok=|John didn't run. |

John didn't run.
01234567890123456
0         1

   n start   end  whsp      token
   0     0     4    ||     |John|
   1     5     9   | |     |didn|
   2    10    11   |'|        |t|
   3    12    15   | |      |run|
lastWhitespace=|. |
\end{verbatim}
%
We have added vertical bars around the text, tokens and whitespaces to
highlight the final space.  The spans here are only for the tokens.
The whitespaces span the interstitial spaces between tokens, so their
span can be computed as the end of the previous token to the start of
the next token.  The last whitespace ends at the underlying text length.

Because tokens are only defined to be contiguous sequences of letters,
the apostrophe in \stringmention{didn't} and the final period appear
in the whitespace.  The period only appears in the final whitespace.

