\chapter{Tokenization}\label{chapter:tokenization}

Many natural-language processing algorithms operate at the word level,
such as most part-of-speech taggers and named-entity chunkers, some
classifiers, some parameterizations of spelling correction, etc.

A token is a generalized kind of word that is derived from segmenting
an input character sequence and potentially transforming the segments.
For instance, a search engine query like
\searchquery{London restaurants} might be converted into a boolean
search for the (case normalized) token \stringmention{london} and the
(plurality normalized) token \stringmention{restaurant}.

\section{Tokenizers and Tokenizer Factories}

LingPipe provides a package \code{com.aliasi.tokenizer} for handling
tokenization.  

\subsection{The \code{TokenizerFactory} Interface}

The \code{TokenizerFactory} factory interface defines a single method,
\code{tokenizer(char[],int,int)}, which takes a slice of a character
array as an argument and returns an instance of \code{Tokenizer}.

LingPipe's tokenizer factory implementations come in two flavors Basic
tokenizer factories are constructed from simple parameters.  For the
basic tokenizers with no parameters, a singleton instance is supplied
as a static constant in the class.  

Tokenizer filters are constructed from other tokenizer factories and
modify their outputs in some way, such as by case normalization,
stemming, or stop-word filtering.

In order to bundle a tokenizer factory with a model, it must be
serializable.  All of LingPipe's tokenizer factories are serializable,
including ones made up by composing a sequence of filters.


\subsection{The \code{Tokenizer} Base Class}

All tokenizers extend the abstract base class \code{Tokenizer}.
Tokenizers provide a stream of tokens.  An instance of
\code{Tokenizer} represents the state of tokenizing a particular
string.  

\subsubsection{Constructing a Tokenizer}

There is no state represented in the \code{Tokenizer} abstract class,
so there is a single no-argument constructor \code{Tokenizer()}.

Because tokenizers are usually created through the 
\code{TokenizerFactory} interface, most.
classes extending \code{Tokenizer} are not delcared to be public.
Instead, only the factory is visible, and the documentation for a
tokenizer's behavior will be in the factory's class documentation.

\subsubsection{Streaming Tokens}

The only method that is required to be implemented is
\code{nextToken()}, which returns the next token in the token stream
as a string, or \code{null} if there are no more tokens.  There is no
reference in a tokenizer itself to the underlying sequence of
characters.  

\subsubsection{Token Positions}

We often wish to maintain the position of a token in the underlying
text.  Given that tokens may be modified or even dropped altogether,
the position of a token is not necessarily going to be recoverable
from the sequence of tokens and whitespaces.  So the \code{Tokenizer}
class supplies methods \code{lastTokenStartPosition()} and
\code{lastTokenEndPosition()}, which return the index of the first
character and of one past the last character.  If no tokens have yet
been returned, these methods both return -1.  These positions are
relative to the slice being tokenized, not to the underlying character
array.  

The token position methods are implemented in the \code{Tokenizer}
base class to throw an \code{UnsupportedOperationException}.
Subclasses that want token positions should override these methods.
Tokenizer filters should almost always just pass the positions of the
tokens being modified.

\subsubsection{Iteration}

The method \code{iterator()} returns an iterator over strings
representing tokens.  In the \code{Tokenzer} base class, the iterator
is defined by delegation to the \code{nextToken()} method.  Thus
subclasses do not usually need to redefine this method.

This \code{iterator()} method allows the \code{Tokenizer} class to
implement the \code{Iterable<String>} interface.  Thus the tokens
can be read from a tokenizer with a for loop.  Given a tokenizer
factory \code{tokFac} and the character slice for input, the
usual idiom is
%
\begin{verbatim}
Tokenizer tokenizer = tokFac.tokenizer(cs,start,length);
for (String token : tokenizer) { ... }
\end{verbatim}

\subsubsection{Bulk Tokenization}

The method \code{tokenize()} returns an array of the remaining tokens
and \code{tokenize(List,List)} writes the remaining tokens and
whitespaces to the specified list.

\subsubsection{Serializability and Thread Safety}

Because they involve dynamic state, tokenizers are almost never
serializable and almost never thread safe.  

 
\subsubsection{Streaming Whitespaces}

Over time, LingPipe has moved from the use of whitespace returned from
tokenizers to token start and end positions.  Unless otherwise noted,
tokenizers need not concern themselves with whitespace.  LingPipe's
built-in tokenizers almost all define the whitespace to be the 
string between the last token and the next token, or the empty string
if that is not well defined.

The method \code{nextWhitespace()} returns the next whitespace from
the tokenizer.  ``White space'' is the general term for the material
between tokens, because in most cases, all non-whitespace is part of
some token.  LingPipe's \code{Tokenizer} class generalizes the notion
of whitespace to arbitrary strings.

Each token is preceded by a whitespace and the sequence ends with a
whitespace.  That is, the sequence goes whitespace, token, whitespace,
token, \ldots, whitespace, token, whitespace.  So the number of
whitespaces is one greater than the number of tokens, and the minimum
output of a tokenizer is a single whitespace.

If the \code{nextWhitespace()} method is not implemented by a
subclass, the implementation inherited from the \code{Tokenizer} base
class simply returns a string consisting of a single space character,
\unicode{0020}, \unicodedesc{space}.

If the \code{nextWhitespace()} method is implemented to return the
text between tokens, tokens do not overlap, and the string for a token
is not modified in any way, then concatenating the sequence of
whitespaces and tokens will produce the underlying characters that
were tokenized.


\subsection{Token Display Demo}

We provide a demo program \code{DisplayTokens}, which runs a tokenizer
over the command-line argument.  The \code{main()} method of the command
calls a utility method on a string variable \code{text} supplied
on the command line, using a built-in tokenizer
%
\codeblock{DisplayTokens.1}
%
The \code{IndoEuropeanTokenizerFactory} class is in
\code{com.aliasi.tokenizer}, and provides a reusable instance through the static constant
\code{INSTANCE}.

The \code{displayTextPositions()} method just prints out the string on
a single line followed by lines providing indexes into the string.
This method won't display properly if there are newlines in the string
itself.

The work is actually done in the subroutine, the body of which is
%
\codeblock{DisplayTokens.2}
%
We first convert the character sequence \code{in} to a 
character array using the utility method \code{toCharArray(CharSequence)}
from the class \code{Strings} in the package \code{com.aliasi.util}.
then, we create the tokenizer from the tokenizer factory.
Next, we just iterate over the tokens, extract their start and end
positions, and print the results.

We provide a corresponding Ant target \code{display-tokens}, which
is given a single command-line argument consisting of the value of
the property \code{text}.  
%
\commandlinefollow{ant -Dtext="The note says, 'Mr. Sutton-Smith owes \$15.20.'" display-tokens}
\begin{verbatim}
The note says, 'Mr. Sutton-Smith owes $15.20.'
0123456789012345678901234567890123456789012345
0         1         2         3         4

START   END  TOKEN           START   END TOKEN
    0     3  |The|              26    27  |-|
    4     8  |note|             27    32  |Smith|
    9    13  |says|             33    37  |owes|
   13    14  |,|                38    39  |$|
   15    16  |'|                39    44  |15.20|
   16    18  |Mr|               44    45  |.|
   18    19  |.|                45    46  |'|
   20    26  |Sutton|
\end{verbatim}
%
In writing the string, we put the ones-place indexes below it, then
the tens-place indexes, and so on.  Because we count from zero, the
very first \charmention{T} has index 0, whereas the \charmention{M} in
\stringmention{Mr} has index 16 and the final apostrophe index 45.
The string is 46 characters long; the last index is always one less
than the length.

We then show the tokens along with their start and end positions.  As
always, the start is the index of the first character and the end is
one past the index of the last character.  Thus the name
\stringmention{Smith} starts at character 27 and ends on character 31.
This also means that the end position of one token may be the start
position of the next when there is no space between them.  For
example, \charmention{says} has an end position of 13 (which is
exclusive) and the following comma a start position of 13 (which is
inclusive).  


\section{LingPipe's Base Tokenizer Factories}

LingPipe provides several base tokenizer factories.  These may be combined
with filters, which we describe in the next section, to create compound
tokenizers.

\subsection{The \code{IndoEuropeanTokenizerFactory} Class}

LingPipe's \code{IndoEuropeanTokenizer} is a fairly fine-grained
tokenizer in the sense that it splits most things apart.  The notable
exception in this instance is the number \stringmention{15.20}, which
is kept as a whole token.  Basically, it consumes as large a token
as possible according to the following classes.

\subsubsection{Kinds of Tokens}

An alphanumeric token is a sequence of one or more digits or letters
as defined by the utility methods \code{isDigit()} and
\code{isCharacter()} methods in Java's \code{Character} class.  Thus
\stringmention{aaa}, \stringmention{A1}, and \stringmention{1234} are all
considered single tokens.

A token may consist of a combination of digits with any number of
token-internal periods or commas.  Thus \stringmention{1.4.2} is
considered a token, as is \stringmention{12,493.27}, but
\stringmention{a2.1} is three tokens, \stringmention{a2},
\stringmention{.} (a period), and \stringmention{1}.

A token may be an arbitrarily long sequence of hyphens (\code{-}) or
an arbitrarily long sequence of equal signs (\code{=}).  The former
are often used as en-dashes and em-dashes in ASCII-formatted text,
and longer sequences are often used for document structure.

Double grave accents (\code{``}) and double apostrophes (\code{''}) are
treated as single tokens, as they are often used to indicate quotations.

All other non-space characters, such as question marks or ampersands,
are considered tokens themselves.

\subsubsection{Construction}

The static constant \code{INSTANCE} refers to an instance of
\code{IndoEuropeanTokenizerFactory} that may be reused.   The
no-argument constructor may also be used.


\subsubsection{Thread Safety and Serializability}

The \code{IndoEuropeanTokenizerFactory} class is both thread safe and
serializable.  The deserialized object will be reference identical
to the factory picked out by the \code{INSTANCE} constant.



\subsection{The \code{CharacterTokenizerFactory} Class}

The \code{CharacterTokenizerFactory} treats each non-whitespace
\code{char} value in the input as a token.  The definition of
whitespace is from Java's \code{Character.isWhitespace()} method.
The whitespaces returned will consist of
all of the whitespace found between the non-whitespace characters.

For instance, for the string \stringmention{a dog}, there are four
tokens, \stringmention{a}, \stringmention{d}, \stringmention{o}, and
\stringmention{g}.  The whitespaces are all length zero other than 
for the single space between the \stringmention{a} and \stringmention{d}
characters.

This tokenizer factory is particularly useful for Chinese, where there
is no whitespace separating words, but words are typically only one or
two characters (with a long tail of longer words).

This class produces a token for each \code{char} value.  This means
that a surrogate pair consisting of a high surrogate UTF-16 value and
low surrogate UTF-16 value, which represent a single Unicode code
point, will produce two tokens.  Specifically, unicode code points
outside of the basic multilingual plane (BMP), that is with values at
or above \unicode{10000}, will produce two tokens (see \refsec{utf-16} for more
information).

The class is thread safe and serializable.  There is no constructor,
only a static constant \code{INSTANCE}.  The instance is also the
result of deserialization.


\subsection{The \code{RegExTokenizerFactory} Class}

For parsing programming languages, the lexical analysis stage
typically breaks a program down into a sequence of tokens.
Traditional C packages like Lex and Java packages like JavaCC specify
regular expressions for tokens (and context-free grammars with side
effects for parse trees).  Regular expression-based tokenization is
also popular for natural language processing.

LingPipe's \code{RegExTokenizerFactory} implements tokenizers where
the tokens are defined by running regular expression matchers in find
mode (see \refsec{regex-find}).  An instance may be constructed from a
\code{Pattern}, or from a \code{String} representing a regular
expression with optional integer flags (see \refsec{pattern-modes} for
information on how to use the flags and their numerical values).

The demo class \code{RegexTokens} implements a simple regular expression
tokenizer factory based on three command line arguments, one for the
regular expression, one for the flags, and one for the text being tokenized.
%
\codeblock{RegexTokens.1}
%

The Ant target \code{regex-tokens} runs the command, setting three
command line arguments for three properties, \code{regex} for the
string representation of the regular expression, \code{flags} for the
integer flags, and \code{text} for the text to be tokenized.
%
\commandlinefollow{ant -Dregex="{\bk}p\{L\}+" -Dflags=32 -Dtext="John likes 123-Jello." regex-tokens}
\begin{verbatim}
regex=|\p{L}+|
flags=32 (binary 100000)

John likes 123-Jello.
012345678901234567890
0         1         2

START   END TOKEN
    0     4  |John|
    5    10  |likes|
   15    20  |Jello|
\end{verbatim}
%
First we print the regex and the flags (in both decimal and binary
forms), then the string with indexes, and finally the tokens.  Because
we used the pattern \code{{\bk}p\{L\}+}, our tokens consist of
maximally long sequences of letter characters.  Other characters such
as spaces, numbers, punctuation, etc., are simply skipped so that they
become part of the whitespace.

Instances of \code{RegExTokenizerFactory} are serializable.  They are
also thread safe.


\subsection{The \code{NGramTokenizerFactory} Class}

An \code{NGramTokenizerFactory} produces tokens from an input string
consisting of all of the substrings of the input within specified size
bounds.  Substrings of an input string of length $n$ are typically
called $n$-grams (and sometimes $q$-grams).

Thus such a factory is constructed using a minimum and
maximum sequence size, with \code{NGramTokenizerFactory(int,int)}.
The sequences produced include the whitespaces between what we
typically think of as tokens.  Tokenizers such as these are especially
useful for extracting features for classifiers.

We provide an example implementation in the demo class
\code{NGramTokens}.  The only thing novel is the construction of
the tokenizer factory itself,
%
\codeblock{NGramTokens.1}
%

It takes three command-line arguments, the
minimum and maximum sequence lengths and the text to analyze.
These are supplied to Ant target \code{ngram-tokens} as 
properties \code{minNGram}, \code{maxNGram}, and \code{text}.
%
\commandlinefollow{ant -DminNGram=1 -DmaxNGram=3 -Dtext="I ran." ngram-tokens}
\begin{verbatim}
minNGram=1
maxNGram=3

I ran.
012345

START   END TOKEN          START   END TOKEN
    0     1  |I|               2     4  |ra|
    1     2  | |               3     5  |an|
    2     3  |r|               4     6  |n.|
    3     4  |a|               0     3  |I r|
    4     5  |n|               1     4  | ra|
    5     6  |.|               2     5  |ran|
    0     2  |I |              3     6  |an.|
    1     3  | r|
\end{verbatim}
%
You can see from the output that the tokenizer first outputs the
1-grams , then the 2-grams, then the 3-grams.  These are typically
referred to as ``unigrams,'' ``bigrams,'' and ``trigrams.''%
%
\footnote{Curiously, we get a transition in common usage from Latin to Greek at
4-grams (tetragrams), and 5-grams (pentagrams), 6-grams (hexagrams)
etc.  Some authors stick to all Greek prefixes, preferring the terms
``monogram'' and ``digram'' to ``unigram'' and ``bigram.''}
%
Note that the spaces and punctuation are treated like any other
characters.  For instance, the unigram token spanning positions 1 to 2
is the string consisting of a single space character and the one
spanning from 5 to 6 is the string consisting of a single period
character.  The first bigram token, spanning from positions 0 to 2, is
the two-character string consisting of an uppercase I followed by a
space.  By the time we get to trigrams, we begin to get cross-word
tokens, like the trigram token spanning from position 0 to 3,
consistinf of an uppercase I, space, and lowercase r.

In terms of the information conveyed, the spaces before and after
words help define word boundaries.  Without the bigram token spanning
1 to 3 or the trigram spanning 1 to 4, we wouldn't know that the
lowercase r started a word.  The cross-word ngram tokens help define
the transitions between words.  With long enough ngrams, we can even
get three-word effects, 


\subsection{The \code{LineTokenizerFactory} Class}

A \code{LineTokenizerFactory} treats each line of input as a single
token.  It is just a convenience class extending
\code{RegExTokenizerFactory} supplying the regex \code{.+}.  

Line termination is thus defined as for regular expression patterns (see
\refsec{regex-lines} for the full set of line-end sequences
recognized).  Final empty lines are not included in the sequence of
tokens.

There is a static constant \code{INSTANCE} which may be used.  This is
also the value of deserialization.  Line tokenizer factories are thread
safe.


\section{LingPipe's Filtered Tokenizers}

Following the same filter-based pattern as we saw for Java's I/O,
LingPipe provides a number of classes whose job is to filter the
tokens from an embedded tokenizer.  In all cases, there will be a base
tokenizer producing a tokenizer which is then modified by a filter to
produce a different tokenizer.  

The reason the tokenizer is modified rather than, say, the characters
being tokenized, is that we usually want to be able to link the tokens
back to the original text.  If we allow an arbitrary transformation of
the text, that's no longer possible.  Thus if the text itself needs to
be modified, that should be done externally to tokenization.

\subsection{The \code{ModifiedTokenizerFactory} Abstract Class}

The abstract class \code{ModifiedTokenizerFactory} in the package
\code{com.aliasi.tokenizer} provides a basis for filtered tokenizers
implemented in the usual way.  Like many LingPipe filter classes, a
modified tokenizer factory is immutable.  Thus the base tokenizer
factory it contains must be provided at construction time.  This is
done through the single constructor
\code{ModifiedTokenizerFactory(TokenizerFactory)}.  The specified
tokenizer factory will be stored, and is made available through the
method \code{baseTokenizerFactory()} (it is common to see protected
member variables here, but we prefer methods).

It then implements the \code{tokenizer(char[],int,int)} method in the
tokenizer factory interface using the base tokenizer factory to 
produce a tokenizer which is then passed through the method
\code{modify(Tokenizer)}, which returns a \code{Tokenizer} that may
then be returned.  This is perhaps easier to see in code,
%
\begin{verbatim}
protected abstract Tokenizer modify(Tokenizer tokenizer);

public Tokenizer tokenizer(char[] cs, int start, int length) {
    TokenizerFactory tokFact = baseTokenizerFactory();
    Tokenizer tok = tokFact.tokenizer(cs,start,length);
    return modify(tok);
}
\end{verbatim}

\subsubsection{Serialization}

A modified tokenizer factory will be serializable if its base
tokenizer factory is serializable.  But unless the subclass defines a
public no-argument constructor, trying to deserialize it will throw an
exception.  If a subclass fixes a base tokenizer factory, there may be
a public no-argument constructor.  A subclass intended to be used
as a general filter will not have a public no-argument constructor
because it would lead to instances without base tokenizers defined.

As elsewhere, subclasses should take control of their serialization.
We recommend using the serialization proxy, and show an example
in the next section.

\subsubsection{Thread Safety}

A modified tokenizer factory is thread safe if its \code{modify()}
method is thread safe.

\subsection{The \code{ModifyTokenTokenizerFactory} Abstract Class}

In the previous section, we saw how the \code{ModifyTokenizerFactory}
base class could be used to define tokenizer filters.  That class
had a modify method that consumed a tokenizer and returned a tokenizer.
The class \code{ModifyTokenTokenizerFactory} is a subclass of
\code{ModifiedTokenizerFactory} that works at the token level.  Almost
all of LingPipe's tokenizer filters are instances of this subclass.

The method \code{modifyToken(String)} operates a token at a time,
returning either a modified token or \code{null} to remove the token
from the stream.  The implementation in the base class just returns
its input.

There is a similar method \code{modifyWhitespace(String)}, though
whitespaces may not be removed.  If tokens are removed, whitespaces
around them are accumulated.  For instance, if we had $w_1, t_1, w_2,
t_2, w_3$ as the stream of whitespaces and tokens, then removed token
$t_2$, the stream would be $w_1, t_1, w_2\cdot w_3$, with the last two
whitespaces being concatenated and the token being removed.

The start and end points for modified tokens will be the same as for
the underlying tokenizer.  This allows classes that operate on
modified tokens to link their results back to the string from which
they arose.

This subclass behaves exactly the same way as its superclass with
respect to serialization and thread safety.  For thread safety, both
the modify methods must both be thread safe.


\subsubsection{Example: Reversing Tokens}

As an example, we provide a demo class
\code{ReverseTokenTokenizerFactory}.  This filter simply reverses the
text of each token. Basically, the implementation involves a declaration,
constructor and definition of the modify operation,
%
\codeblock{ReverseTokenTokenizerFactory.1}
%
In addition, the class implements a serialization proxy (see
\refsec{io-serialization-proxy}).

The \code{main()} method for the demo reads a command line
argument into the variable \code{text}, then constructs a token-reversing
tokenizer factory based on an Indo-European tokenizer factory,
then displays the output,
%
\codeblock{ReverseTokenTokenizerFactory.2}
%
It also serializes and deserializes the factory and displays
the output for that, but we've not shown the code here.

The Ant target \code{reverse-tokens} calls the class's \code{main()}
method, supplying the value of property \code{text} as the command-line
argument.
%
\commandlinefollow{ant reverse-tokens}
\begin{verbatim}
The note says, 'Mr. Sutton-Smith owes $15.20.'
0123456789012345678901234567890123456789012345
0         1         2         3         4

START   END TOKEN
    0     3  |ehT|
    4     8  |eton|
    9    13  |syas|
...
   39    44  |02.51|
   44    45  |.|
   45    46  |'|
\end{verbatim}
%
The results are the same positions as for the Indo-European tokenizer,
only with the content of each token reversed.  We have not shown the
serialized and then deserialized output; it's the same.

\subsection{The \code{LowerCaseTokenizerFactory} Class}

The \code{LowerCaseTokenizerFactory} class modifies tokens by
converting them to lower case.  This could've just as easily been
upper case -- it just needs to be normalized.

Case normalization is often applied in contexts where the case of
words doesn't matter, or provides more noise than information.  For
instance, search engines typically store case-normalized tokens in
their reverse index.  And classifiers often normalize words for
case (and other properties).

Instances are constructed by wrapping a base tokenizer factory.
Optionally, an instance of \code{Locale} to define the specific
lowercasing operations.  If no locale is specified,
\code{Locale.ENGLISH} is used.  (This ensures consistent behavior
across platforms, unlike using the platform's default locale.)

Whitespace and start and end positions of tokens are defined
as by the base classifier.

The class is used like other filters.  For instance, to create a
tokenizer factory that returns lowercase tokens produced the
Indo-European tokenizer using German lowercasing, just use
%
\begin{verbatim}
TokenizerFactory f = IndoEuropeanTokenizerFactory.INSTANCE;
Locale loc = Locale.GERMAN;
TokenizerFactory fact = new LowerCaseTokenizerFactory(f,loc);
\end{verbatim}

Instances will be serializable if the base tokenizer factory is
serializable.  Serialization stores the locale using its built-in
serialization, as well as the base tokenizer factory.  Instances will
be thread safe if the base tokenizer factory is thread safe.


\subsection{The \code{StopTokenizerFactory} Class}

The \code{StopTokenizerFactory} provides a tokenizer factory filter
that removes tokens that appear in a specified set of tokens.  The set
of tokens removed is typically called a \stringmention{stop list}.
The stop list is case sensitive for this class, but stoplisting is
often applied after case normalization.

It is common in token-based classifiers and search engines to further
normalize input by removing very common words.  For instance, the
words \stringmention{the} or \stringmention{be} in English documents
typically convey very little information themselves (they can convey
more information in context).  Sometimes this is done for efficiency
reasons, in order to store fewer tokens in an index or in statistical
model parameter vectors.  It may also help with accuracy, especially
in situations with limited amounts of training data.  In small data
sets, the danger is that these stop words will become associated with
some categories more than others at random and thus randomly bias
the resulting system.

Stop lists themselves are represented as sets of tokens.  Stoplisting
tokenizers are constructed by wrapping a base tokenizer and providing
a stop list.  For instance, to remove the words \stringmention{the} and
\stringmention{be}, we could use
%
\begin{verbatim}
Set<String> stopSet = CollectionUtils.asSet("the","be");
TokenizerFactory f1 = IndoEuropeanTokenizerFactory.INSTANCE;
TokenizerFactory f2 = new LowerCaseTokenizerFactory(f1);
TokenizerFactory f3 = new StopTokenizerFactory(f2,stopSet);
\end{verbatim}
%
The first line uses the static utility method \code{asSet()}
in the LingPipe utility class \code{CollectionUtils}.  This
method is defined with signature 
%
\begin{verbatim}
static <E> HashSet<E> asSet(E... es);
\end{verbatim}
%
so that it takes a variable-length number of \code{E} objects and
returns a hash set of that type.  Because Java performs type inference
on method types (like the \code{<E>} generic in \code{asSet()}),
we don't need to specify a type on the method call.

We then wrap an Indo-European tokenizer factory in a lowercasing and
then a stoplisting filter.  Operationally, what will happen is that
the Indo-European tokenizer will tokenize, the resulting tokens will
be converted to lower case, and then stop words will be removed.  The
calls start out in reverse, though, with \code{f3} called to tokenize,
which then calls \code{f2} and filters the resulting tokens, where
\code{f2} itself calls \code{f1} and then filters its results.

\subsubsection{What's a Good Stop List?}

The utility of any given stop list will depend heavily on the
application.  It often makes sense in an application to run several
different stop lists to see which one works best.  

Usually words on the stop list will be of relatively high
frequency.  One strategy is to print out the top few hundred
or thousand words in a corpus sorted by frequency and inspect
them by hand to see which ones are ``uninformative'' or potentially
misleading.

\subsubsection{Built-in For English}

LingPipe provides a built-in stop list for English.  The
\code{EnglishStopTokenizerFactory} class extends the
\code{StopTokenizerFactory} class, plugging in the following
stoplist for English:
%
\begin{quote}
a, be, had, it, only, she, was, about, because, has, its, of, some,
we, after, been, have, last, on, such, were, all, but, he, more, one,
than, when, also, by, her, most, or, that, which, an, can, his, mr,
other, the, who, any, co, if, mrs, out, their, will, and, corp, in,
ms, over, there, with, are, could, inc, mz, s, they, would, as, for,
into, no, so, this, up, at, from, is, not, says, to
\end{quote}
%
Note that the entries are all lowercase.  It thus makes sense
to run this filter to a tokenizer factory that produces only
lowercase tokens.  

This is a fairly large stop list.  It's mostly function words, but
also includes some content words like \stringmention{mr},
\stringmention{corp}, and \stringmention{last}.  Also note that
some words are ambiguous, which is problematic for stoplisting.  For
instance, the word \stringmention{can} acts as both a modal auxiliary
verb (presently possible) and a noun (meaning among other things, a
kind of container).


\subsection{The \code{RegExFilteredTokenizerFactory} Class}

The \code{RegExFilteredTokenizerFactory} class provides a general
filter that removes all tokens that do not match a specified regular
expression.  For a token to be retained, it must match the specified
regular expression (in the sense of \code{match()} from
\code{Matcher}).

An instance of \code{RegExFilteredTokenizerFactory} may be constructed
from a base tokenizer factory and a \code{Pattern} (from
\code{java.util.regex}).  For instance, we could retain only tokens
that started with capital letters (as defined by Unicode) using
%
\begin{verbatim}
TokenizerFactory f = ...;
Pattern p = Pattern.compile("\p{Lu}.*");
TokenizerFactory f2 = new RegExFilteredTokenizerFactory(f,p);
\end{verbatim}
%
We could just as easily retain only tokens that didn't start with an
uppercase letter by using the regex \code{[\^{\bk}p\{Lu\}].*} instead
of the one supplied.

Although stop lists could be implemented this way, it would not be
efficient.  For one thing, regex matching is not very efficient for
large disjunctions; set lookup is much faster.  Also, at the present
time, the class creates a new \code{Matcher} for each token,
generating a large number of short-lived objects for garbage
collection.%
%
\footnote{A more efficient implementation would reuse a matcher for
each token in a given tokenization.  The only problem is that it can't
be done naturally and thread-safely by extending
\code{ModifyTokenTokenizerFactory}.  It would be possible to do this
by extending \code{ModifiedTokenizerFactory}.}
%

Like the other filter tokenizers, a regex filtered tokenizer is
thread safe and serializable if its base tokenizer factory is.


\subsection{The \code{TokenLengthTokenizerFactory} Interface}

The class \code{TokenLengthTokenizerFactory} implements a tokenizer
filter that removes tokens that do not fall in a prespecified length
range.  In real text, tokens can get very long.  Fror instance,
mistakes in document formatting can produce a document where all
the words are concatenated without space.  If you look at large
amounts of text (in the gigabytes range), especially if you're
looking at unedited sources like the web, you will find longer
and longer tokens used on purpose.  It's not unusual to see
\stringmention{no}, \code{noo}, and \code{nooooooo!}.  Look
at enough text, and someone will add 250 lowercase O characters
after the \charmention{n}.

The length range is supplied in the constructor, which takes a
tokenizer factory, minimum token length, and maximum token length.
Like the other filters, a length filtering tokenizer will be
serializable and thread safe if the base tokenizer factory is.  And
like some of the other filters, this could be implemented very easily,
but not as efficiently, with regular expressions (in this case the
regex \code{.\{\codeVar{m},\codeVar{n}\}} would do the
trick).

\subsection{The \code{WhitespaceNormTokenizerFactory} Class}

When interpreting HTML documents on the web, whitespace (outside of
\code{<pre>} environments) comes in two flavors, no space and some
space.  Any number of spaces beyond will be treated the same way for
rendering documents as a single space.

This is such a reasonable normalization scheme that we often normalize
our input texts in exactly the same way.  This is easy to do with a
regex, replacing a string \code{text} with
\code{text.replaceAll("{\bk}{\bk}s+"," ")}, which says to replace
any non-empty sequence of spaces with a single space. 

It's not always possible to normalize inputs.  You may need to link
results back to some original text that's not under your control.  In
these cases, it's possible to modify the whitespaces produced by the
tokenizer.  This will not have any effect on the underlying text or
the spans of the tokens themselves.  Unfortunately, very few of
LingPipe's classes pay attention to the whitespace, and for some that
do, it's the token bounds that matter, not the values returned by
\code{nextWhitespace()}.

A \code{WhitespaceNormTokenizerFactory} is constructed from a base
tokenizer factory.  Like the other filter tokenizer factories, it will
be serializable if the base tokenizer factory is serializable and will
be thread safe if the base tokenizer factory is thread safe.


\section{Morphology-Sensitive Tokenization} 

In some applications, the differences between different morphological
variants of the same base word are not particularly informative.  For
instance, should the query \searchquery{new car} provide a different
search result than \searchquery{new cars}?  The difference is that one
query uses the singular form \stringmention{car} and the other the
plural \stringmention{cars}.  


\subsection{Inflectional Morphology}

Inflectional morphology is concerned with how the basic paradigmatic
variants of a word are realized.  For instance, in English, the two
nouns \code{computer} and \code{computers} differ in number, the first
being singular, the second plural.  These show up in different
syntactic contexts --- we say \stringmention{one computer} and
\stringmention{two computers}, but not \stringmention{one computers}
and \stringmention{two computer}.%
%
\footnote{These ungrammatical forms like \stringmention{one computers}
will appear in text.  You'll find just about everything in free text
and even in fairly tightly edited text like newswire if you look at
enough of it.}
%
This is an instance of agreement --- the grammatical number of the
noun must agree with the number of the determiner.  In English, there
are only two numbers, singular and plural, but other languages, like
Icelandic and Sanskrit, have duals (for two things), and some
languages go even further, defining numbers for three objects, or
distinguishing many from a few.

In English, the verbs \code{code}, \code{codes}, \code{coded}, and
\code{coding} are all inflectional variants of the same underlying word,
differing in person, number, tense, and aspect.  We get agreement
between verbs and subject nouns in English for these features.  For
instance, a person might say \stringmention{I code} in referring to
themselves, but \stringmention{she codes} when referring to another
woman coding.  Other languages, such as French or Russian, also have
agreement for gender, which can be natural (male person versus female
person versus versus non-person) or grammatical (as in French).
Natural gender is based on what is being talked about.  That's the
distinction we get in English between the relative pronouns
\stringmention{who}, which is used for people and other animate
objects, and \stringmention{what}, which is used for inanimate
objects.  For instance, we say \stringmention{who did you see?} if
we expect the answer to be a person, and 
\stringmention{what did you see?} if we want a thing.

Most languages have fairly straightforward inflectional morphology,
defining a few variants of the basic nouns and verbs.  The actual
morphological operations may be as simple as adding an affix, but even
this is complicated by boundary effects.  For instance, in English
present participle verbs, we have
\stringmention{race}/\stringmention{racing},
\stringmention{eat}/\stringmention{eating}, and \stringmention{run}/\stringmention{running}.
In the first case, we delete the final \charmention{e} before adding
the suffix \stringmention{-ing}, in the second case, we just append
the suffix, and in the third case, we insert an extra \charmention{n}
before adding the suffix.  The same thing happens with number for
nouns, as with \stringmention{boy}/\stringmention{boys} versus
\stringmention{box}/\stringmention{boxes}, in which the first case
appends the suffix \charmention{s} whereas the second adds an
\charmention{e} before the suffix (the result of which, \stringmention{boxes},
is two syllables).

It is also common to see internal vowel changes, as in the English
alternation \stringmention{run}/\stringmention{ran}.  Languages
like Arabic and Hebrew take this to the extreme with their
templatic morphology systems in which a base form consists of
a sequence of consonants and an inflected form mixes in vowles.


\subsection{Derivational Morphology}

Derivational morphology, in contrast to inflectional morphology,
involves modifying a word to create a new word, typically with a
different function.  For instance, from the verb \stringmention{run}
we can derive the noun \stringmention{runner} and from the adjective
\stringmention{quick} we can derive the adverb
\stringmention{quickly}.  After we've derived a new form, we can
inflect it, with singular form \stringmention{runner} and plural form
\stringmention{runners}.

Inflectional morphology is almost always bounded to a finite set of
possible inflected forms for each base word and category.  Derivational
morphology, on the other hand, is unbounded.  We can take the noun
\stringmention{fortune}, derive the adjective \stringmention{fortunate},
the adjective \stringmention{unfortunate}, and then the adverb
\stringmention{unfortunately}.  

Sometimes there's zero derivational morphology, meaning not that
there's no derivational morphology, but that it has no surface effect.
For instance, we can turn most nouns in English into verbs, especially
in the business world, and most verbs into nouns (where they are
called gerunds).  For instance, I can say \stringmention{the running
of the race} where the verb \stringmention{running} is used as a noun,
or \stringmention{Bill will leverage that deal}, where the noun
\stringmention{leverage} (itself an inflected form of
\stringmention{lever}) may be used as a verb.  In American English,
we're also losing the adjective/adverb distinction, with many speakers
not even bothering to use an adverbial form of an adjective, saying
things like \stringmention{John runs fast}.  

\subsection{Compounding}

Some languages, like German, allow two nouns to be combined
into a single word, written with no spaces, as in combining
\stringmention{schnell} (fast) and \stringmention{Zug} (train)
into the compound \stringmention{schnellzug}.  This is a relatively
short example involving two nouns. Comnpounds can then be 


\subsection{Languages Vary in Amount of Morphology}

Languages are not all the same.  Some code up lots of information in
their morphological systems \eg{Russian}, and some not much at all
\eg{Chinese}.  Some languages allow very long words consisting of
lots of independent morphemes \eg{Turkish, Finnish}, whereas most
only allow inflection and derivation.


\subsection{The \code{PorterStemmerTokenizerFactory} Class}

