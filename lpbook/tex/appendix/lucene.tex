\chapter{The Lucene Search Library}\label{appendix:lucene}

Apache Lucene is a search library written in Java.  Due to its
performance, configurability and generous licensing terms, it has
become very popular.  In this section, we'll provide a hello-world
type example to help you get started with Lucene.

Lucene provides search over documents.  Documents are added to the
index and documents are retrieved by searching over the index.  A
document is essentially a collection of fields, where a field
supplies a field name and value.  

What actually gets indexed is terms produced from documents through an
analysis process.  For instance, a title field like
\stringmention{Molecular Biology, 2nd Edition} might yield indexing
terms \stringmention{molecul}, \stringmention{biolog},
\stringmention{2}, and \stringmention{edition}.  The index structure
provides the reverse mapping from terms back to documents.


\section{Fields}

All fields in Lucene are instances of the \code{Fieldable} interface
in the package \code{org.apache.lucene.document}.  Numeric fields are
still marked as experimental, and we are mainly interested in text, so
we concentrate on the other built-in concrete implementation of
\code{Fieldable}, the final class \code{Field}.  This implementation
can store both binary and character data, but we concentrate on the latter.

\subsection{Field Names and Values}

Each constructor for a field requires a name for the field.  At search
time, user searches are directed at particular fields.  For instance,
a MEDLINE citation might store the name of the article, the journal in
which it was published, the authors of the article, the date of
publication, the text of the article's abstract, and a list of topic
keywords (drawn from MeSH).  Each of these fields would get a
different name, and at search time, the client could specify that it
was searching for authors or titles or both, potentially restricting
to a date range and set of journals.

We will focus on text fields.  The value for a text field may be
supplied as a Java \code{String} or \code{Reader}.%
%
\footnote{We recommend not using a \code{Reader}, because the policy
  on closing such readers is confusing.  It's up to the client to
  close, but the close can only be done after the document has been
  added to an index.  Making fields stateful in this way introduces a
  lifecycle management problem that's easily avoided.  Very rarely
  will documents be such that a file or network-based reader may be
  used as is in a field; usually such streams are parsed into fields
  before indexing, eliminating any performance advantage readers might
  have.}
%
The value for a binary (blob) field is supplied as a byte array slice,
and the indexing flags are preset.


\subsection{Indexing Flags}

Because documents are overloaded for use during indexing and search,
fields were designed to store information about how they are indexed
and/or stored.  This information is provided to a field during
construction in the form of a number of flags.  These flags are
implemented as nested enum instances in the \code{Field} class.

\subsubsection{The \code{Field.Store} Enum}

All fields are marked as to whether their raw value is stored in the
index or not.  Storing raw values allows you to retrieve them at
search time, but may consume substantial space.

There are two instances of the enum \code{Field.Store}, namely
\code{Field.Store.YES} and \code{Field.Store.NO}, with the
obvious interpretations.

\subsubsection{The \code{Field.Index} Enum}

All fields are also marked as to whether they are indexed or not.  A
field must be indexed in order for it to be searchable.  Non-indexed
fields may be stored.  It makes no sense to have a non-stored,
non-indexed field.

The value \code{Field.Index.NO} turns off indexing for a field.  The
other values all turn on indexing for a field.  Where they vary is how
the terms that are indexed are pulled out of the field value.  The
standard value for indexed field is \code{Field.Index.ANALYZED}, which
will run the analyzer over the raw value to produce terms to index
(see \refsec{lucene-analysis}).  The value
\code{Field.Index.NOT\_ANALYZED} causes the field to be treated a
single term; it's primarily used for identifier fields that are not
decomposed into searchable terms.

\subsubsection{The \code{Field.TermVector} Enum}

The final specification on a field is whether to store term vectors or
not, and if they are stored, what specifically to store.  Term vectors
are an advanced data structure encapsulating the geometric search
structure of a document's terms (the pieces extracted from a field by
an analyzer).  The default is not to store term vectors, with flag
\code{Field.TermVector.NO}.  We will use that option as we will not be
demonstrating the use of term vectors.


\subsection{Constructing Fields}

A field requires all of its components to be specified in the
constructor.  Even so, fields are defined to be mutable so that their
values, but not their field names, may be reset after construction.
The explicit constructor for text fields is
\code{Field(String,String,Field.Store,Field.Index,Field.TermVector)},
where the first two strings are field and value, and the remaining
three are the enum flags discsussed in the previous section.%
%
\footnote{There is also a constructor with an additional boolean
  argument that lets you control whether a field is interned or not
  (see \refsec{string-intern}).  The default in the unmarked
  constructors is to intern all field names.}
%
There are also a ragne of utility constructors that provide default
values for the flags.  There is also a constructor that takes a
\code{TokenStream} rather than a string.%
%
\footnote{It's not clear what the use case is for this constructor.
Users must be careful to make sure the token stream provided is
consistent with whatever analysis will happen at query time.}

\subsection{Field Getters}

Once we have a field, we can access the components of it such as its
name, value, whether its indexed, stored, or tokenized, and whether
term vectors are stored.  These methods are all specified in the
\code{Fieldable} interface.  For instance, \code{name()} returns
a field's name, and \code{stringValue()} its value.%
%
\footnote{For fields constructed with a \code{Reader} for a value,
the method \code{stringValue()} returns \code{null}.  Instead,
the method \code{readerValue()} must be used.  Similarly, the
methods \code{tokenStreamValue()} and \code{binaryValue()} are
used to retrieve values of fields constructed with token streams
or byte array values.}
%

There are convenience getters derived from the flag settings.  For
instance, \code{isIndexed()} indicates if the field is indexed, and
\code{isTokenized()} indicates whether the indexing involved analysis
of some sort.  The method \code{isStored()} indicates if the value is
stored in the index, and \code{isTermVectorStored()} whether the term
vector is stored in the index.

\section{Documents}

All documents are instances of the final class \code{Document}, in
package \code{org.apache.lucene.document}.  

\subsection{Constructing Documents} 

Documents are constructed using a zero-arg constructor \code{Document()}.
After construction, they a document may be modified with a range of
setters. 

\subsection{Populating Documents}

Once a document is constructed, the method \code{add(Fieldable)} is
used to add fields to the document.  

Lucene does not in any way constrain document structures.  An index
may store a heterogeneous set of documents, with any number of
different fields which may vary by document in arbitrary ways.  It is
up to the user to enforce consistency at the document collection
level.  

A document may have more than one field with the same name added to
it.  All of the fields with a given name will be searchable under that
name (if the field is indexed, of course).  The behavior is conceptually
similar to what you'd get from concatenating all the field values,
but cross-field searchers are not allowed.  

\subsection{Accessing Fields in Documents}

The \code{Document} class provides a means of getting fields by name.
The method \code{getFieldable(String)} returns the field for a
specified name.  If there's no field with the specified name, it
returns \code{null} rather than raising an exception.

The return type is \code{Fieldable}, but this interface provides
nearly the same list of methods as \code{Field} itself, so there is
rarely a need to cast a fieldable to a field.  

If there is more than one field in a document with the same name, the
simple method \code{getFieldable(String)} only returns the first one
added.  The method \code{getFieldables(String)} returns an array of
all fields in a document with the given name.  It's costly to
construct arrays at run time (in both space and time for allocation
and garbage collection), so if there is only a single value, the
simpler method is preferable.



\subsection{Document Demo}

We provide a simple demo class, \code{DocDemo}, which illustrates
the construction, setting and accessing of fields and documents.
The \code{main()} method starts by constructing a document and
populating it.
%
\codeblock{DocDemo.1}
%
After constructing the document, we add a sequence of fields,
includign a title field, two author fields, a field for the name of
the journal, several fields storing mesh terms, and a field storing
the document's PubMed identifier.  These terms are all stored
and analyzed other than the identifier, which is not analyzed.

After constructing the document, we loop over the fields and
inspect them.
%
\codeblock{DocDemo.2}
%
Note that the access is through the \code{Fieldable} interface.

\subsubsection{Running the Demo}

The Ant target \code{doc-demo} runs the demo.  
%
\commandlinefollow{ant doc-demo}
\begin{verbatim}
name=title value=Fast and Accurate Read Alignment
     indexed=true store=true tok=true termVecs=false
name=author value=Heng Li
     indexed=true store=true tok=true termVecs=false
...
name=mesh value=genomics/methods
     indexed=true store=true tok=true termVecs=false
name=mesh value=sequence alignment/methods
     indexed=true store=true tok=true termVecs=false
name=pmid value=20080505
     indexed=true store=true tok=false termVecs=false
\end{verbatim}
%
We've elided three fields, marked by ellipses.



\section{Analysis and Token Streams}\label{section:lucene-analysis}

At indexing time, Lucene employs analyzers to convert the text value
of a fields marked as analyzed to a stream of tokens.  At indexing
time, Lucene is supplied with an implementation of the abstract base
class \code{Analyzer} in package \code{org.apache.lucene.analysis}.
An analyzer maps a field name and text value to a \code{TokenStream},
in the same package, from which the terms to be indexed may be
retrieved.

\subsection{Token Streams and Attributes}

Before version 3.0 of Lucene, token streams had a string-position
oriented tokenization API, much like LingPipe's tokenizers.  Version
3.0 generalized the interface for token streams and other basic
objects using a very general pattern based on attributes of other
objects.
%
\footnote{The benefit of this pattern is not in its use, but in the
  fact that it allows easy extensions to be attached the API in a
  flexible way without breaking backward compatibility.}

\subsubsection{Code Walkthrough}

We provide a sample class \code{LuceneAnalysis} that applies an
analyzer to a field name and text input and prints out the resulting
tokens.  The work is done in a simple \code{main()} with two
arguments, the field name and the text to be analyzed.  

The first step is to create the analyzer.
%
\codeblock{LuceneAnalysis.1}
%
Here we've used Lucene's \code{StandardAnalyzer}, in package
\code{org.apache.lucene.analysis.standard}, which applies case
normalization and English stoplisting to the simple tokenizer, which
pays attention to issues like periods and e-mail addresses.  Note that
it's constructed with a constant for the Lucene version, as the
behavior has changed over time.

The standard analyzer, like almost all of Lucene's built-in analyzers,
ignores the name of the field that is passed in.  Such analyzers
essentially implement simple token stream factories, like LingPipe's
tokenizer factories.

The next step of the \code{main()} method constructs the token stream
given the string values of the command-line arguments \code{fieldName}
and \code{text}.  
%
\codeblock{LuceneAnalysis.2}
%
We first have to create a \code{Reader}, which we do by wrapping the
input text string in a \code{StringReader} (from \code{java.io}).
Then we use the analyzer to create a token stream from the field name
and text reader.  The next three statements attach attributes to the
token stream, specifically a term attribute, offset attribute and
position increment attribute.  These are used to retrieve the text of
a term, the span of the term in the original text, and the ordinal
position of the term in the sequence of terms in the document.  The
position is given by an increment from the previous position, and
Lucene uses these values for phrase-based search \ie{searching for a
fixed sequence of tokens in the given order without intervening
material}.

The last block of code in the \code{main()} method iterates through
the tokens tream, printing the attributes of each token it finds.
%
\codeblock{LuceneAnalysis.3}
%
The while loop continually calls \code{incrementToken()} on the token
stream, which advances to the next token, returning \code{true} if
there are more tokens.  The body of the loop just pulls out the
increment, start and end positions, and term for the token.  The
rest of the code, which isn't shown, just prints these values.

\subsubsection{Running the Demo}

It may be run from the Ant target \code{lucene-analysis}, with
the arguments provided by properties \code{field.name} and 
\code{text} respectively.

\commandlinefollow{ant -Dfield.name=foo -Dtext="Mr.\ Sutton-Smith will pay \$1.20 for the book." lucene-analysis}
\begin{verbatim}
Mr. Sutton-Smith will pay $1.20 for the book.
012345678901234567890123456789012345678901234
0         1         2         3         4

 INCR (START,   END) TERM         INCR (START,   END) TERM
    1 (    0,     2) mr              2 (   22,    25) pay
    1 (    4,    10) sutton          1 (   27,    31) 1.20
    1 (   11,    16) smith           3 (   40,    44) book
\end{verbatim}
%
The terms are all lowercased, and non-word-internal punctuation has
been removed.  The stop words \charmention{will}, \charmention{for}
and \charmention{the} are also removed from the output.  Unlike
punctuation, when a stop word is removed, it causes the increment
between terms to be larger.  For instance, the increment between
\charmention{smith} and \charmention{pay} is 2, because the stopword
\charmention{will} was removed between them.  The start (inclusive)
and end (exclusive) positions of the extracted terms is also shown.


\section{Directories}\label{section:lucene-directory}

Lucene provides a storage abstraction on top of Java in the abstract
base class \code{Directory} in the \code{org.apache.lucene.store}
package.  Directories provide an interface that's similar to an
operating systems' file system.

\subsection{Types of Directory}

The \code{FSDirectory} abstract base class extends \code{Directory} to
support implementations based on a file system.  This is the most
common way to create a directory in Lucene.  The implementation
\code{RAMDirectory} supports in-memory directories, which are efficient,
but less scalable than file-system directories.  The \code{DbDirectory}
class uses Java Database Connectivity (JDBC) to support a transactional
and persistent directory on top of a database.  There are also more
specialized implementations.

\subsection{Constructing Directories}

An instance of a file-system directory may be created using the
factory method \code{FSDirectory.open(File)}, which returns an
implementation of \code{FSDirectory}.  The finer-grained factory
method \code{open(File,LockFactory)} allows a specification of how the
files on the file system will be locked.  Also, one of the three
subclasses of \code{FSDirectory} may be used for even more control
over how the bytes in the file are managed.%
%
\footnote{\code{NIOFSDirectory} uses the \code{java.nio} package, but
  suffers from a JRE bug that makes it unsuitable for use on Windows.
  The \code{SimpleFSDirectory} uses a standard \code{RandomAccessFile}
  from \code{java.io} and is safe for Windows, but is unfortunately
  over-synchronized for concurrent usage.  The \code{MMapDirectory}
  uses memory-mapped I/O and works in either Windows or Unix, though
  see the extensive qualifications about resource locking in the
  Javadoc.}


\section{Indexing}

Lucene uses the \code{IndexWriter} class in
\code{org.apache.lucene.index} to add documents to an index (and
otherwise manage an index).  Documents do not all need to be added at
once --- documents may be added to or removed from an existing index.

We provide a demo class \code{LuceneIndexing} that shows how basic
text indexing works.  The work is all done in the \code{main()} method,
whcih starts by constructing the index writer.
%
\codeblock{LuceneIndexing.1}
%
The two arguments correspond to the directory from which documents to
be indexed are read and the directory to which the Lucene index is
written.  We create a file-system-based directory using the index
directory (see \refsec{lucene-directory}).  We then create a standard
analyzer (see \refsec{lucene-analysis}).  Finally, we create an index
writer from the directory and analyzer.  The flag \code{UNLIMITED}
specifies that all of the text in a field will be indexed, not just an
initial segment of it.  This constructor creates an index if one does
not exist, or opens an existing index if an index already exists in
the specified directory.  Another constructor lets you specify that
even if an index exists, it should be overwritten with a fresh index.

Constructing the index may throw all three exceptions listed on the
\code{main()} method.  The first two exceptions are Lucene's, and both
extend \code{IOException}.  You may wish to catch them separately in
some cases, as they clearly indicate what went wrong. A
\code{CorruptIndexException} will be thrown if we attempt to open an
index that is not well formed.  A \code{LockObtainFailedException}
will be thrown if the index writer could not obtain a file lock on the
index directory.  A plain-old Java \code{IOException} will be thrown
if there is an underlying I/O error reading or writing from the files
in the directory.

The second half of the \code{main()} method reads text from files,
creates a Lucene document, then adds it to the index.
%
\codeblock{LuceneIndexing.2}
%
We keep a count of the number of characters processed in the variable
\code{numChars}.  We then loop is over all the files in the specified
document directory.  For each file, we get its name, as well as the
text (using LingPipe's \code{readFromFile()} utility method, which
converts the bytes in the file to a string using the specified
character encoding, here ASCII).  We then create a document and add
the file name as an unanalyzed field and the text as an analyzed
field.  With the four-argument \code{Field} constructor, term vectors
storage is turned off for all fields.

After creating the document, we simply use the
\code{addDocument(Document)} method of the index writer to add it
to the index.  

After indexing each file, we call the index writer's \code{optimize()}
method, which tamps the index down into a minimal file representation
that's most efficient for indexing speed.  We then close the index
writer using the \code{close()} method, which may throw an
\code{IOException}; the \code{IndexWriter} class is declared to
implement Java's \code{Closeable} interface, so we could've used
LingPipe's \code{Streams.closeSilently()} utility method to close
it and swallow any I/O exceptions raised.

We then grab the number of documents and print out aspects of
the indexing job.

\subsection{Running the Demo}

The Ant target \code{lucene-index} runs the indexing demo.  It
supplies the values of properties \code{doc.dir} and \code{index.dir}
to the program as the first two command-line arguments.  We created a
directory containing the 85 {\it Federalist Papers}, and use them
as an example document collection.
%
\commandlinefollow{ant -Ddoc.dir=../../data/federalist-papers/texts -Dindex.dir=temp.idx lucene-index}
\begin{verbatim}
Index Directory=C:\lpb\src\applucene\temp.idx
Doc Directory=C:\lpb\data\federalist-papers\texts
num docs=85
num chars=1154664
\end{verbatim}
%
Lucene's very fast.  On my workstation, it takes less than a second to
run the demo, including forking a new JVM.  The run indexed 85
documents consisting of approximately 1.1 million words total.

After indexing, we can look at the contents of the index directory.
%
\commandlinefollow{ls -l temp.idx}
\begin{verbatim}
-rwxrwxrwx 1 Admin None  355803 Aug 24 17:48 _0.cfs
-rwxrwxrwx 1 Admin None 1157101 Aug 24 17:48 _0.cfx
-rwxrwxrwx 1 Admin None      20 Aug 24 17:48 segments.gen
-rwxrwxrwx 1 Admin None     235 Aug 24 17:48 segments_2
\end{verbatim}
%
These files contain binary representations of the index.  The
\code{.cfs} file contains the index (including the position of each
token in each file).  The \code{.cfx} file contains the stored fields. 
The two segments files contain indexes into these files.  The indexes
will be stored in memory to allow fast random access to the appropirate
file contents.

With the standard analyzer, the index file is only a quarter the size of
the text file, whereas the stored field file is slightly larger than the
set of texts indexed.

\subsubsection{Duplicate Documents}

If we were to run the program again, each of the documents would be
added to the index a second time, and the number of documents reported
will be 170 (twice the initial 85).  Although a Lucene index provides
identifiers for documents that are unique (though not necessarily
stable over optimizations), nothing in the index enforces uniqueness
of document contents.  Lucene will happily create another document
with the same fields and values as another document.  It keeps them
separate internally using its own identifiers.


\section{Search}

Now that we have a Lucene index, we can use it for search.  In
order to carry out search, we need to be able to construct queries.

\subsection{Queries ande Query Parsing}

Lucene overloads the notion of search, combining exact boolean
searches with softer vector-space search methods.  This overloading is
reflected in the queries.  The atomic pieces of queries are also
complex, allowing phrase searches, fuzzy term searches, range
searches, and so on.  Furthermore, each atomic search specifying a
term must also specify the field over which it is to be run.%
%
\footnote{Unfortunately, there's no way to easily have a query search
  over all fields.  Instead, field-specific queries must be disjoined
  to achieve this effect.  Another approach is to denormalize the
  documents by creating synthetic fields that concatenate the value of
  other fields.}

\subsubsection{Constructing Queries Programatically}

Queries may be constructed programatically using the dozen or so
built-in implementations of the the \code{Query} abstract base class
from the package \code{org.apache.lucene.search}.  

The most basic query is over a single term in a single field.  This
form of query is implemented in Lucene's \code{TermQuery} class in the
package \code{org.apache.lucene.search}.  A term query is constructed
from a \code{Term}, which is found in package
\code{org.apache.lucene.index}.  A term is constructed from a field
name and text for the term, both specified as strings.

Boolean queries are misnamed in that they support both approximate
vector-space queries and proper boolean queries.  A boolean query may
be constructed with a no-argument constructor.  Queries are then added
to the boolean query, with a specification of their polarity through
an instance of \code{BooleanClause.Occur} (also in the search
package).  The values of the \code{Occur} enum are \code{MUST},
\code{MUST\_NOT} and \code{SHOULD}.  The first two are used for proper
boolean queries, requiring the term to appear or not appear in any
result.  The last value, \code{SHOULD}, is used for vector-space
queries.  With this occurrence value, Lucene will prefer results that
match the query, but may return results that do not match the query.

\subsubsection{Query Parsing}

Lucene specifies a language in which queries may be expressed.  

For instance, \code{computer NOT java} produces a query that specifies
the term \stringmention{computer} must appear in the default field and
the term \stringmention{java} must not appear.  Queries may specify
fields, as in \stringmention{text:java}, which requires the term
\stringmention{java} to appear in the \code{text} field of a document.

The full syntax specification is available from
\url{http://lucene.apache.org/java/3_0_2/queryparsersyntax.html}.  The
syntax includes basic term and field specifications, modifiers for
wildcard, fuzzy, proximity or range searches, and boolean operators
for requiring a term to be present, absent, or for combining queries
with logical operators.  Finally, sub-queries may be boosted by providing
numeric values to raise or lower their prominence relative to other
parts of the query.

A query parser is constructed using an analyzer, default field, and
Lucene version.  The default field is used for queries that do not
otherwise specify the field they search over.  It may then be used to
convert string-based queries into query objects for searching.

\subsection{Index Readers}

In order to perform search, an instance of \code{IndexReader} must be
available.  This class is complementary to the \code{IndexWriter} we
used to create an index.  

One nice feature of the reader design in Lucene is that we can
construct an instance from multiple indexes, which will then combine
their contents at search time.  From the reader's client's
perspective, the behavior is indistinguishable (other than in terms of
speed) from a combined and optimized index.  

We can even distribute these indexes over multiple machines on a
network using Java's Remote Method Invocation (RMI).  


\subsection{Index Searchers}

Lucene supplies an \code{IndexSearcher} class that performs the actual
search.  Every index searcher wraps an index reader to get a handle
on the indexed data.  Once we have an index searcher, we can supply
queries to it and enumerate results in order of their score.

\subsection{Demo Code}

We provide a simple implementation of Lucene search based on the index
we created in the last section.  The code is in the \code{main()}
method of the demo class \code{LuceneSearch}.  The method starts off
by reading in command-line arguments.
%
\codeblock{LuceneSearch.1}
%
We need the directory for the index, a string representing the query
in Lucene's query language, and a specification of the maximum number
of hits to return.  The method is declared to throw exceptions if it
finds the index is not well formed or if a query is not well formed;
it also throws general I/O exceptions if there are problems reading or
writing from the directory.

We then create a directory, index reader, index searcher, and
query parser.
%
\codeblock{LuceneSearch.2}
%
It is important to use the same analyzer in the query parser as is
used in the creation of the index.  If they don't match, queries that
should succeed will fail because terms won't match.  For instance, if
we apply stemming in the indexing to reduce \stringmention{codes} to
\stringmention{code}, then we better do the same thing for the query,
because we won't find \stringmention{codes} in the index, only its
stemmed form \code{code}.

The third block of code in the search demo uses the query parser
to parse the query, then searches the index and reports the results.
%
\codeblock{LuceneSearch.3}
%
The \code{Query} object is created by using the parser to parse the
text query.  We then use the searcher instance to search given the
query and an upper bound on the number of hits to return.  This
returns an instance of \code{TopDocs}, which encapsulates the results
of a search (through references back into the index).

We retrieve an array of \code{ScoreDoc} objects from the
\code{TopDocs} object.  These are in decreasing order of score, with
higher scores representing better matches.  We then enumerate over the
array, and for each \code{ScoreDoc} object, we pull its score out
using the public member variable \code{score}.  We then pull its
document reference number (Lucene's internal identifier for the doc)
out with the member variable \code{doc}, and then use this value to
retrieve the document from the searcher (which just delegates this
operation to its index reader internally).  Finally, with the document
in hand, we retrieve its file name.  We could've also retrieved the
text of the document, because we stored it in the index.  

\subsection{Running the Demo}

The Ant target \code{lucene-search} invokes the demo with command-line
arguments provided by the value of properties \code{index.dir},
\code{query}, and \code{max.hits}.
%
\commandlinefollow{ant -Dindex.dir=luceneIndex -Dquery="money power
  united" -Dmax.hits=15 lucene-search}
\begin{verbatim}
Index Dir=C:\lpb\src\applucene\luceneIndex
query=money power united
max hits=15
Hits (rank,score,file name)
  0 0.16  44.ascii.txt
  1 0.13  32.ascii.txt
  2 0.13  80.ascii.txt
...
 13 0.10  23.ascii.txt
 14 0.10  26.ascii.txt
\end{verbatim}
%
Lucene returns 15 results numbered 0 to 14.  We see that paper 44 is
the closest match to our query \code{money power united}.  This
document contains 4 instances of the term \stringmention{money}, over
40 instances of the term \stringmention{power}, and 8 instances of 
\stringmention{united}.

The term \stringmention{food} does not show up in any documents, so
the query \code{food} returns no hits.  If we consider the query
\code{money food}, it returns exactly the same hits as the query
\code{money}, in exactly the same order, but with lower scores.  If we
use the query \code{money +food}, we are insisting that the term
\stringmention{food} appear in any matching document.  Because it
doesn't appear in the corpus at all, the query \code{money +food} has
zero hits and \code{money -food} has the same hits with the same
scores as the query \code{money}.


\subsection{Ranking}

For scoring documents against queries, Lucene uses the complex and
highly configurable abstract base class \code{Similarity} in the
package in \code{org.apache.lucene.search}.  If nothing else is
specified, as in our simple demos, the concrete subclass
\code{DefaultSimilarity} will be used.

Similarity deals with scoring queries with \code{SHOULD} terms.
Unless a query term is prefixed with \code{+} (must occur) or \code{-}
(must not occur), it will be treated as a should-occur term.

The basic idea is that the more instances of query terms in a document
the better.  Terms are not weighted equally.  A term is weighted based
on its inverse document frequency (IDF), so that terms that occur in
fewer documents receive higher weights.  Weights may also be boosted
or lowered in the query syntax or programatically with a query object.

All else being equal, shorter documents are preferred.  The idea here
is that if there are the same number of instances of query terms in two
documents, the shorter one has a higher density of query terms, and is
thus likely to be a better match.  

There is also a component of scoring based on the percentage of the
query terms that appear in the document.  All else being equal, we
prefer documents that cover more terms in the query.





 







