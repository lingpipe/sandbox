\chapter{Latent Dirichlet Allocation}\label{chapter:lda}

Latent Dirichlet allocation (LDA) provides a statistical approach to
document clustering based on the words (or tokens) that appear in a
document.  Given a set of documents, a tokenization scheme to convert
them to bags of words, and a number of topics, LDA is able to infer
the set of topics making up the collection of documents and the
mixture of topics found in each document.  These assignments are soft
in that they are expressed as probability distributions, not absolute
decisions.

At the basis of the LDA model is the notion that each document is
generated from its own mixture of topics.  For instance, a blog post
about a dinner party might 70\% about cooking, 20\% about sustainable
farming, and 10\% about entertaining.

Applications of LDA include document similarity measurement, document
clustering, topic discovery, word association for search, feature
extraction for discriminative models such as logistic regression or
conditional random fields, classification of new documents, language
modeling, and many more.

\section{Corpora, Documents, and Tokens}

For LDA, a corpus is nothing more than a collection of documents.
Each document is modeled as a sequence of tokens, which are often
words or normalized forms of words.

Applications of LDA typically start with a real document collection
consisting of character data.  LingPipe uses tokenizer factories to
convert character sequences to sequences of tokens (see
\refchap{tokenization}).  Utilities in the LDA implementation class
help convert these to appropriate representations with the help of
symbol tables (see \refchap{symbol-tables}).

For LDA, and many other models such as traditional naive Bayes
classification and $k$-means clustering, only the count of the words
is significant, not their order.  A bag is a data structure like a
set, only with (non-negative) counts for each member.  Mathematically,
we may model a bag as a mapping from its members to their counts. Like
other models that don't distinguish word order, LDA is called a
bag-of-words model.


\section{LDA Parameter Estimation}

In this section, we will cover the steps necessary to set up the
constant parameters for an LDA model and then estimate the unknown
parameters from a corpus of text data.

\subsection{Synthetic Data Example}

We begin with a synthetic example due to Steyvers and Griffiths.%
%
\footnote{From Steyvers, Mark and Tom Griffiths. 2007. Probabilistic
  topic models. In Thomas K.~Landauer, Danielle S.~McNamara, Simon
  Dennis and Walter Kintsch (eds.), {\it Handbook of Latent Semantic
    Analysis}. Laurence Erlbaum.}
%
Synthetic or ``fake data'' models play an important role in
statistics, where they are used like unit tests for statistical
inference mechanisms.  

In this section, we will create an LDA model by fixing all of its
parameters, then use it to generate synthetic data according to the
model.  Then, we will provide the generated data to our LDA
inference algorithm and make sure that it can recover the known
structure of the data.

The implementation of our synthetic example, which illustrates
the basic inference mechanism for LDA, is in the demo class
\code{SyntheticLdaExample}.

\subsubsection{Tokens}

Steyvers and Griffiths' synthetic example involved only two topics and
five words.  The words are \stringmention{river},
\stringmention{stream}, \stringmention{bank}, \stringmention{money},
and \stringmention{loan}.  The first two words typically occur in
articles about rivers, and the last two words in articles about money.
The word \stringmention{bank}, the canonical example of an ambiguous
word in English, occurs in both types of articles.  

\subsubsection{What's a Topic?}

A topic in LDA is nothing more than a discrete probability
distribution over words.  That is, given a topic, each word has a
probability of occurring, and the sum of all word probabilities in a
topic must be one.

Steyvers and Griffiths' example involves two topics, one about
banking and one about rivers, with the following word probabilities.
%
\begin{center}
\begin{tabular}{l|rrrrr}
& \stringmention{river} 
& \stringmention{stream}
& \stringmention{bank}
& \stringmention{money}
& \stringmention{loan}
\\ \hline
\tblhead{Topic 1} & 1/3 & 1/3 & 1/3 & 0 & 0 
\\
\tblhead{Topic 2} & 0 & 0 & 1/3 & 1/3 & 1/3
\end{tabular}
\end{center}
%
Topic 1 is about water and topic 2 about money.  If a word is drawn
from topic 1, there is a 1/3 chance it is \charmention{river}, a 1/3
chance it is \charmention{stream} and a 1/3 chance it is
\charmention{bank}.  


\subsubsection{What's a Document?}

For the purposes of LDA, a document is modeled as a sequence of
tokens.  We use a tokenizer factory to convert the documents into
counts.  The identity of the tokens doesn't matter.  It turns out the
order of the tokens doesn't matter for fitting the model, though we
preserve order and identity so that we can label tokens with topics in
our final output.

The code in \code{SyntheticLdaExample} starts out defining an array of
sixteen character sequences making up the document collection that
will be clustered.  These documents were generated randomly from an
LDA model by Steyvers and Griffiths.  The texts making up the
documents are defined starting with
%
\codeblock{SyntheticLdaExample.1a}
%
and ending with
%
\codeblock{SyntheticLdaExample.1b}
%
The only reason we used concatenation is so that the lines would fit
on the pages of the book.  The complete set of documents is most
easily described in a table.
%
\begin{center}
\begin{tabular}{c|ccccc}
% & \multicolumn{5}{c}{\tblhead{Tokens}} \\
\tblhead{Doc ID}
& \tblhead{river}
& \tblhead{stream}
& \tblhead{bank}
& \tblhead{money}
& \tblhead{loan}
\\ \hline
\tblhead{0} & 0 & 0 & 4 & 6 & 6 
\\
\tblhead{1} & 0 & 0 & 5 & 7 & 4
\\
\tblhead{2} & 0 & 0 & 7 & 5 & 4 
\\
\tblhead{3} & 0 & 0 & 7 & 6 & 3 
\\
\tblhead{4} & 0 & 0 & 7 & 2 & 7
\\
\tblhead{5} & 0 & 0 & 9 & 3 & 4
\\
\tblhead{6} & 1 & 0 & 4 & 6 & 5 
\\
\tblhead{7} & 1 & 2 & 6 & 4 & 3
\\
\tblhead{8} & 1 & 3 & 6 & 4 & 2
\\
\tblhead{9} & 2 & 3 & 6 & 1 & 4
\\
\tblhead{10} & 2 & 3 & 7 & 3 & 1
\\
\tblhead{11} & 3 & 6 & 6 & 1 & 0
\\
\tblhead{12} & 6 & 3 & 6 & 0 & 1
\\
\tblhead{13} & 2 & 8 & 6 & 0 & 0
\\
\tblhead{14} & 4 & 7 & 5 & 0 & 0
\\
\tblhead{15} & 5 & 7 & 9 & 0 & 0
\end{tabular}
\end{center}
%
The first document (identifier/position 0) has four instances of
\stringmention{bank}, six of \stringmention{money} and six of
\stringmention{loan}.  Each document has sixteen tokens, but equal
sized documents is just an artifact of Steyvers and Griffiths'
example.  In general, document sizes will differ.

\subsubsection{Tokenizing Documents}

LDA deals with token identifiers in the form of a matrix, not
with string tokens.  But there's a handy utility method to produce
such a matrix from a corpus of texts and a tokenizer factory.
The first few lines of the \code{main()} method in
the \code{SyntheticLdaExample} class provide the conversion
given our static array \code{TEXTS} of character sequences:
%
\codeblock{SyntheticLdaExample.2}
%
The regex-based tokenizer factory is used with a regex defining tokens
as maximal sequences of characters in the Unicode letter class (see
\refsec{tok-regex-tokenizer-factory} for regex tokenizer factories and
\refsec{regex-unicode-classes} for Unicode regex classes).  This will
pull out each of the words in our documents.  

Next, we create a symbol table (see \refsec{symbol-map-symbol-table})
which we will use to provide integer indexes to each of the tokens.

The variable \code{minCount} will specify how many instances of a
token must be seen in the entire set of documents for it to be used in
the model.  Here we set the threshold to 1, so no tokens will be
pruned out.  For most applications, it makes sense to use a slightly
larger value.

Then, we create a two-dimensional array of document-word identifiers.
The value of \code{docWords[n]} is an array of the symbol identifiers
from the symbol table for the words in document \code{n} (which will
range from 0 (inclusive) to 15 (inclusive) for our 16-document
corpus).  

The program prints the symbol table and document-word array so
you may see what is actually produced,
%
\begin{verbatim}
symbTab={0=stream, 1=money, 2=bank, 3=loan, 4=river}

docWords[ 0] = { 2, 3, 1, 3, 3, 1, 2, 3, 2, 3, 3, 1, 2, 1, 1, 1 }
docWords[ 1] = { 3, 1, 1, 3, 2, 2, 1, 2, 1, 3, 1, 1, 2, 3, 2, 1 }
...
docWords[14] = { 4, 0, 0, 0, 4, 0, 0, 2, 2, 2, 2, 4, 4, 0, 2, 0 }
docWords[15] = { 0, 4, 4, 2, 0, 0, 0, 0, 2, 4, 4, 0, 2, 4, 0, 2 }
\end{verbatim}
%
Given the symbol table, the first document (index 0) has tokens 2
(\stringmention{bank}), 3 (\stringmention{loan}), 1
(\stringmention{money}), and so on, matching the input.  

It is this document-word matrix that is input to LDA.  

\subsection{LDA Parameters}

Before running LDA, we need to set up a number of parameters, which we
will explain after seeing the example run.  The first of these are the
model parameters,
%
\codeblock{SyntheticLdaExample.3}
%
First, we set up the number of topics, which must be fixed in advance.
Here we cheat and set it to 2, which we happen to know in this case is
the actual number of topics used to generate the documents.  In
general, we won't know the best value for the number of topics for a
particular application.

\subsubsection{Sampler Parameters}

Next, we set up the variables that control the sampler underlying
the LDA inference mechanism.
%
\codeblock{SyntheticLdaExample.4}
%
These are the number of samples we take, the number of samples thrown
away during the burnin phase, and the period between samples after
burnin (we explain these parameters in full detail below).  

The sampler also requires a pseudo-random number generator.  We use
Java's built-in pseudo-random number generator in the class
\code{Random} in \code{java.util} (see \refappendix{java-random} for
more information bout \code{Random} and using it in experimental
settings).  By providing the constructor an explicit seed, \code{43L},
the program will have the same behavior each time it is run.  Remove
the argument or provide a different seed to get different behavior.

\subsubsection{Reporting Callback Handler Parameters}

After that, we set up a handler that'll be used for reporting on the
progress of the sampler.  Specifically, we define a callback class
\code{ReportingLdaSampleHandler} (the code of which we describe
below), which will be passed to the inference method.
%
\codeblock{SyntheticLdaExample.5}
%
The callback handler will receive information about each sample
produced and provides the flexibility to perform online Bayesian
inference (which we explain below).  It's constructed with the
symbol table and takes an argument specifying the period between
reports (measured in samples).

\subsubsection{Invoking the Sampler}

At last, we are in a position to actually call the LDA inference
method, which is named after the algorithmic approach, Gibbs
sampling, which we'll also define below,
%
\codeblock{SyntheticLdaExample.6}
%
Basically, we pass it all of our parameters, including the
document-token matrix.

After the method returns, we have a sample in hand.  We use the
reporting methods in the handler to print out information about the
final sample.  It requires the final sample and parameters controlling
verbosity (printing at most 5 words per topic and 2 topics per
document, which here, are not actually constraints).
%
\codeblock{SyntheticLdaExample.7}


\section{Interpreting LDA Output}

We run the example using the Ant target \code{synthetic-lda} to
run the \code{main()} method in \code{SyntheticLdaExample}.  
a
%
\commandline{ant synthetic-lda}
%
We've already seen the output of the symbol table and the 
document-token matrix above.  

\subsection{Convergence Monitoring}

The next thing that's output is monitoring information from the
sampler.
%
\begin{verbatim}
n=     0 t=     :00 x-entropy-rate=  2.1553
n=     4 t=     :00 x-entropy-rate=  2.1528
...
n=    20 t=     :00 x-entropy-rate=  1.7837
n=    24 t=     :00 x-entropy-rate=  1.7787
...
n=   248 t=     :00 x-entropy-rate=  1.7811
n=   252 t=     :00 x-entropy-rate=  1.7946
\end{verbatim}
%
The results here are typical.  The report for the sample number.  Note
that we only get a report for every four samples because of how we
parameterized the handler.  We can report every epoch, but it's
relatively expensive.  The second column is the elapsed time.  This
example is so slow, it takes less than a second total.  The third
column is the cross-entropy rate.  This is roughly a measure of the
expected number of bits required to code a word if we were to use
the LDA model for compression.%
%
\footnote{This figure ignores the generation of the multinomial for
  the document. For use in compression, we would also need to encode a
  discrete approximation of the draw of the document's multinomial
  from the Dirichlet prior.}
%
Note that the cross-entropy starts high, at 2.1 bits, but quickly
drops to 1.8 bits.  Further note that at the converged state, LDA is
providing a better model than randomly generating words, which would
have a cross-entropy of $- \log_2 1/5 \approx 2.3$ bits.

The goal in running Gibbs samplers is to reach convergence, where
numbers like cross-entropy rates stabilize.  Note that Gibbs sampling
is not an optimization algorithm, so while the cross-entropy rates
start high and get lower, once they've reached about 1.78 in this
example, they start bouncing around.  

The number of samples used for burnin should be large enough that it
gets you to the bouncing around stage.  After that, results should be
relatively stable (see below for more discussion of LDA stability).

\subsection{Inferred Topics}

Next, we get a report of the parameters we've fit for the topic
models themselves.
%
\begin{verbatim}
TOPIC 0  (total count=104)
SYMBOL             WORD    COUNT   PROB          Z
--------------------------------------------------
     0           stream       42   0.404       4.2
     2             bank       35   0.336      -0.5
     4            river       27   0.260       3.3

TOPIC 1  (total count=152)
SYMBOL             WORD    COUNT   PROB          Z
--------------------------------------------------
     2             bank       60   0.395       0.5
     1            money       48   0.316       3.1
     3             loan       44   0.289       3.0
\end{verbatim}
%
We fixed the number of topics at 2.  LDA operates by assigning each
token to a topic (as we will see in a second).  These reports show the
most frequent words associated with each topic along with the symbol
table ID and the number of times each word was assigned to a topic.
At the top of the report is the total number of words assigned to that
topic (just a sum of the word counts).  

The probability column displays the estimated probabiltiy of a word in
a topic.  Thus if we sample a word from topic 0, it is 40.4\% likely
to be \stringmention{stream}, 33.6\% likely to be \stringmention{bank}
and 26.6\% likely to be \stringmention{river}.  Note that these
probabilities add up to 1.  Also note that these are not probabilities
of topics given words.  The probability of \stringmention{bank} in
topic 1 is only 39.5\% and thus it should be evident that these can't
be probabilities of topics given words, becuase they don't add up to 1
(33.6\% + 39.5\% = 78.1\% < 1).

These probabilities estimated from the small set of randomly generated
data are as close as could be expected to the actual probability
distributions.  With larger data sets, results will be tighter.

The order of the topics is not determined.  With a different random
seed or stopping after different numbers of samples, we will get
slightly different results.  One difference may be that the
identifiers on the topics will be switched.  This lack of
identifiability of the topic identifier is typical of clustering
models.  We return to its effect on Bayesian inference below.

The final column provides a z score for the word in the topic.  This
value measures how much more prevalent a word is in a topic than it is
in the corpus as a whole.  The units are standard deviations.  For
instance, the word \stringmention{bank} appears 95 times in the corpus
of 256 words, accounting for 95/256, or approximately 37\% of all
tokens.  In topic 0, it appears 35 times in 104 tokens, accounting for
approximately 34\% of all tokens; in topic 1, it appears 60 times in
152 words, accounting for roughly 39\% of the total number of words.
Because the prevalence of \stringmention{bank} in topic 0 is less than
in the corpus (34\% versus 37\%), its z score is negative; because
it's higher in topic 1 than in the corpus (39\% versus 37\%), the z
score is positive.   

\subsection{Inferred Token and Document Categorizations}

The final form of output is on a per-document basis (controlled
by a flag to the final output method).  This provides a report
on a document-by-document basis of how each word is assigned.
%
\begin{verbatim}
DOC 0
TOPIC    COUNT    PROB
----------------------
    1       16   0.994

bank(1) loan(1) money(1) loan(1) loan(1) money(1) bank(1) 
loan(1) bank(1) loan(1) loan(1) money(1) bank(1) money(1) 
money(1) money(1)
...
DOC 8
TOPIC    COUNT    PROB
----------------------
    1       11   0.685
    0        5   0.315

money(1) bank(1) money(1) loan(1) stream(0) bank(1) loan(1) 
bank(1) bank(1) stream(0) money(1) bank(0) bank(1) stream(0) 
river(0) money(1)
...
DOC 15
TOPIC    COUNT    PROB
----------------------
    0       16   0.994

stream(0) river(0) river(0) bank(0) stream(0) stream(0) 
stream(0) stream(0) bank(0) river(0) river(0) stream(0) bank(0) 
river(0) stream(0) bank(0)
\end{verbatim}
%
We've only shown the output from three representative documents.  For
each document, we show the topic assignment to each token in parens
after the token.  All the words in document 0 are assigned to topic 1
whereas all the word in document 15 are assigned to topic 0.  In
document 8, we see that 11 words are assigned topic 1 and 5 words
topic 0.  The word \stringmention{bank} is assigned to both topic 0
and topic 1 in the document.  This shows how LDA accounts for
documents that are about more than one topic.

The aggregate counts show how many words are assigned to each topic,
and the probability that a word from a document is chosen from the
probability.  

Even though all words were assigned to topic 1 in document 0, the
probability is still only 0.994.  This is because of the priors, which
induce smoothing, setting aside a small count for unseen topics in
each document.  This kind of smoothing is immensely helpful for
robustness in the face of noisy data, and almost all natural language
data is immensely noisy.


\section{LDA's Gibbs Samples}

The return value of the LDA estimation method is a Gibbs sample.

\subsection{Constant, Hyperparameter and Data Access}

The samples store all the argments supplied to the LDA method other
than number of samples information.  These values will be constant
for the chain of samples produced by the iterator.

The method \code{numTopics()} returns the constant number of topics
specified in the original invocation.  The methods
\code{topicWordPrior()} and \code{documentTopicPrior()} return the two
hyperparameters \ie{constant prior parameters}.

The sample object also provides access to the underlying document-word
matrix matrix through the methods \code{numDocuments()},
\code{documentLength(int)}, and \code{word(int,int)}.

\subsection{Word Topic Assignments and Probability Estimates}

What we're really after is the actual content of the sample.  Each
step of sampling contains an assignment of each token in each document
to one of the topics.  The method \code{topicSample(int,int)} returns
the \code{short}-valued topic identifier for the specified document
and token position.

These assignments of topics to words, combined with the priors'
hyperparameters, determine the probability estimates for topics and
documents we showed in the output in the previous section.  The method
\code{topicWordProb(int,int)} returns the \code{double}-valued
probability estimate of a word in a topic.  These probability
distributions fix the behavior of the topics themselves.

The method \code{documentTopicProb(int,int)} returns the
\code{double}-valued proability estimate for a topic in a document.
These probabilities summarize the topics assigned to the document.

These probability estimates, coupled with the underlying document-word
data matrix, allows us to compute \code{corpusLog2Probability()},
which is the probability of the topics and words given the models.
Note that it does not include the probabilities of the document
multinomials or the topic multinomials given their Dirichlet priors.
The main reason they're excluded is that the multinomials are not
sampled in the collapsed sampler.

The static utility method \code{dirichletLog2Prob(double[],double[])}
in the class \code{Statistics} in LingPipe's \code{stats} package may
be used to compute Dirichlet probabilities.  We may plug the maximum a
posteriori (MAP) estimates provided by the methods
\code{topicWordProb()} and \code{documentTopicProb()} into the
\code{dirichletLog2Prob()} method along with the relevant prior,
\code{topicWordPrior()} and \code{documentTopicPrior()}, respectively.



\subsection{Retrieving an LDA Model}

Each Gibbs sample contains a factory method \code{lda()} to construct
an instance of \code{LatentDirichletAllocation} based on the
topic-word distributions implied by the sample and the prior
hyperparameters.  

Although the Gibbs samples themselves are not serializable, the LDA
models they return are.  We turn to the use of an LDA instance in the
next section.
The method we used in the last section to invoke the sampler returned
a single Gibbs sample.  There is a related static estimation method in
\code{LatentDirichletAllocation} that returns an iterator over all of
the Gibbs samples.  The iterator and one-shot sampler return instances
of \code{GibbsSample}, which is a static class nested in
\code{LatentDirichletAllocation}.

Samples are produced in order by the sampler.  The poorly named method
\code{epoch()} returns the sample number, with numbering beginning
from 0.


\section{Handling Gibbs Samples}

In our running synthetic data example, we provided an instance of the
demo class \code{ReporingLdaSampleHandler} to the LDA estimation
method \code{gibbsSampler()}.  The use of the handler class follows
LingPipe's generic callback handler pattern.  For every
Gibbs sample produced by the sampler, the handler object receives
a reference to it through a callback.

\subsection{Samples are Reused}

Unlike many of our other callbacks, the Gibbs sampler callbacks
were implemented for efficiency.  Rather than getting a new
Gibbs sample object for each sample, the same instance of
\code{GibbsSample} is reused.  Thus it does not make any sense
to create a collection of the Gibbs samples for later use.  
All statistics must be computed online or accumulated outside
the Gibbs sample object for later use.

\subsection{Reporting Handler Demo}

The reporting handler demo is overloaded to serve two purposes.
First, it handles Gibbs samples as they arrive as required by
its interface.  Second, it provides a final report in a
class-specific method.

\subsubsection{Online Sample Handler}

The class declaration, member variables and constructor for the
handler are as follows.
%
\codeblock{ReportingLdaSampleHandler.1}
%
We maintain a reference to the symbol table so that our reports can
reassociate the integer identifiers with which LDA works with the
tokens from which they arose.  We see how the report period is used in
the implementation of the handler method.

The class was defined to implement the interface
\code{ObjectHandler<GibbsSample>}.  It thus requires an implementation
of the \code{void} return method \code{handle(GibbsSample)}.  Up to
the print statement, this method is implemented as follows.
%
\codeblock{ReportingLdaSampleHandler.2}
%
This is the callback method that will be called for every sample.  It
first grabs the epoch, or sample number.  If this is not an even
multiple of the report period, the method returns without performing
any action.  Otherwise, we fall through and calcualte the elapsed time
and cross-entropy rate.  Note that the cross-entropy rate as reported
here is the negative log (base 2) probability of the entire corpus of
documents divided by the number of tokens in the entire corpus.  
This cross-entropy rate is what we saw printed and what we monitor for
convergence.

\subsubsection{General Reporting}

There are additional reporting methods in the handler above the
required one that provide more detailed reporting for a sample.  They
produce the output shown in the previous section after the online
reporting of cross-entropy rates finishes and the final sample is
produced by LDA's \code{gibbsSampler()} method.

The general parameter reporting method, \code{reportParameters()},
just prints general corpus and estimator parameters from the sample.
There are two more interesting reporters, one for topics and one for
documents.  The topic reporting method, \code{reportTopics()},
begins as follows.
%
\codeblock{ReportingLdaSampleHandler.3}
%
The loop is over the topic identifiers.  Within the loop, the method
begins by assigning the number of words assigned to a topic.  It then
allocates a general LingPipe object counter to use for the word
counts.  Each word identifier is considered int he inner loop, and the
counter's value for that word is set to the number of times the word
was assigned to the current topic.  After collecting the counts in our
map, we create a list of the keys, here word indexes, ordered in
descending order of their counts.  

We continue by looping over the words by rank.
%
\codeblock{ReportingLdaSampleHandler.4}
%
We bound the number of words by the maximum given and make sure not to
overflow the model's size bounds.  Inside the loop, we just pull out
identifiers and counts, and calculate the z-score.  This is where we
need the symbol table in order to print out the actual words rather
than just their numbers.  The rest of the method is just printing.

The method used to calculate the z-score is
%
\codeblock{ReportingLdaSampleHandler.5}
%
It first calculates the maximum likelihood estimate of the word's
probabilty in the corpus, which is just the overall count of the word
divided by the number of words in the corpus.  The maximum likelihood
variance estimate is calculated as usual for a binomial outcome over
$n$ samples (here \code{wordsInCorpus}) with known probabiltiy
$\theta$ (here \code{pCorpus}), namely $n \times \theta \times (1 -
\theta)$.  The expected number of occurrences if it had the corpus
distribution is given by the number of words in the document times the
probability in the corpus.  The z-score is then the actual value minus
the expected value scaled by the expected deviation.

Returning to the general reporting, we have a method
\code{reportDocs()}, which provides a report on the documents,
optionally including a word-by-word report of topic assignments.
%
\codeblock{ReportingLdaSampleHandler.6}
%
This method enumerates all the documents in the corpus, reporting on
each one in turn.  For each topic, we follow the same pattern as we
did for the topics beofre, establishing a LingPipe object counter
mapping topic identifiers to the number of times a word in the current
document was assigned to that topic.  We then sort as before, then
loop over the ranks up to the maximum number of topics or total number
of topics in the model.
%
\codeblock{ReportingLdaSampleHandler.7}
%
For each rank, we calculate the counts and probabilities and display
them.

Finally, we report on a token-by-token basis if the flag is set.
%
\codeblock{ReportingLdaSampleHandler.8}
%
Here we just enumerate over the tokens in a document printing
the topic to which they were assigned.


\subsection{General Handlers}

In general, handlers can do anything.  For instance, they can be used
to accumulate results such as running averages, thus supporting full
Bayesian inference with a small memory footprint (only the current
sample need stay in memory).

For instance, the following handler would calculate average corpus
log 2 probability over all samples.
%
\codeblock{CorpusLog2ProbAvgHandler.1}
%
We use an instance of LingPipe's \code{OnlineNormalEstimator} for the
calculation.  It calculates averages and sample variances and standard
deviations online without accumulating the data points.  The method
\code{mAvg.mean()} returns the sample mean and
\code{mAvg.varianceUnbiased()} the unbiased sample variance.

Along these sample lines, more complex statistics such as word or
document similarities or held out data entropy estimates may be
computed online.


\section{Scalability of LDA}

LDA is a highly scalable model.  It takes an amount of time per token
that is proportional to the number of topics.  Thus with a fixed
number of topics, it scales linearly in topic size.  Tens of thousands
of documents are no problem on desktop machines.

\subsection{Wormbase Data}

In this section, we consider a larger scale LDA estimation problem, in
analyzing a collection of biomedical research paper citations related
to the model organism caenorhabditis elegans (c.~elegans), the
nematode worm.%
%
\footnote{\label{footnote:lda-wormbase-data} We downloaded the data,
  which consists of MEDLINE citations, from the Wormbase data
  repository,
  \url{ftp://ftp.wormbase.org/pub/wormbase/misc/literature/2007-12-01-wormbase-literature.endnote.gz},
  on 20 August 2010.}
%
Similar data from a slightly different source on a smaller scale
was analyzed by Blei et al.\ in an attempt to relate the topics
inferred using LDA with those added for genetics by the database
curators.%
%
\footnote{Blei et al.  analyzed a smaller set of worm citations from
  the Caenorhabditis Genetic Center (CGC) bibliography.  This data is
  available from the WormBook site at \url{http://www.wormbook.org/wli/cgcbib}.  
\begin{quote}
Blei, D.~M., K.~Franks, M.~I.~Jordan and
  I.~S.~Mian. 2006. Statistical modeling of biomedical corpora: mining
  the Caenorhabditis Genetic Center Bibliography for genes related to
  life span. {\it BMC Bioinformatics} {\bf 7}:250.
\end{quote}
Blei et al.\ considered 50 topics generated from 5225 documents using
a total of 28,971 distinct words.  }
%
An example of a citation in raw form is
%
\begin{verbatim}
%0 Journal Article
%T Determination of life-span in Caenorhabditis elegans 
   by four clock genes.
%A Lakowski, B
%A Hekimi, S
%D 1996
%V 272
%P 1010-1013
%J Science
%M WBPaper00002456
%X The nematode worm Caenorhabditis elegans is a model system 
   for the study of the genetic basis of aging. Maternal-effect
   mutations in four genes--clk-1, clk-2, clk-3, and gro-1
   --interact genetically to determine both the duration of 
   development and life-span. Analysis of the phenotypes of
   physiological clock in the worm. Mutations in certain genes 
   involved in dauer formation (an alternative larval stage
   induced by adverse conditions in which development is 
   arrested) can also extend life-span, but the life extension 
   of Clock mutants appears to be independent of these genes. 
   The daf-2(e1370) clk-1(e2519) worms, which carry life-span-
   extending mutations from two different pathways, live nearly 
   five times as long as wild-type worms.
\end{verbatim}
%
There are not actually line breaks in the \code{\%T} and \code{\%X} elements.

\subsection{Running the Demo}

First, you'll need to download the gzipped data from wormbase; see
\reffootnote{lda-wormbase-data} in this section for the URL.  It
doesn't matter where you put it; the program takes the location as
a parameter.

We created an Ant target \code{lda-worm} to run the demo.  It is
configured to provide seven command-line arguments as properties,
\code{wormbase.corpus.gz} for the location of the gzipped corpus,
\code{min.token.count} for the minimum token count in the corpus to
preserve the word in the model, \code{num.topics} for the number of
topics to generate, \code{topic.prior} and \code{word.prior} for the
priors, \code{random.seed} for the random seed, and \code{num.samples}
for the total number of samples to draw.
%
\commandline{ant -Dwormbase.corpus.gz="../../data-dist/2007-12-01-wormbase-literature.endnote.gz" -Dmin.token.count=5 -Dnum.topics=50 -Dtopic.prior=0.1 -Dword.prior=0.001 -Drandom.seed=43 -Dnum.samples=500 -Dmodel.file=wormbase.LatentDirichletAllocation -Dsymbol.table.file=wormbase.SymbolTable lda-worm > temp.txt}
%
It takes about half an hour to generate the 500 samples on my
workstation, and quite a bit more time to generate the roughly 50MB of
output reporting on the topics and the assignment of tokens in
documents.  Because there's so much output, we redirect the results to
a file; you can use the command \code{tail -f} to follow along in real
time.

The output begins by reporting on the parameters and the corpus itself,
the interesting part of which reports corpus statistics rather than
what we entered.
%
\begin{verbatim}
...
#articles=26888 #chars=43343525
Number of unique words above count threshold=28499
Tokenized.  #Tokens After Pruning=3731693
\end{verbatim}
%
We see that there are 26,888 total citations, made up of around 43
million characters.  After tokenization, there are roughly 28 thousand
distinct tokens that each appeared at least 5 times (the minimum token
count), and a total token count of around 3.7 million instances.  That
means each Gibbs sampling step is going to reassign all 3.7 million
tokens.

After the corpus statistics, it reports on the iterator's progress, as
before.
%
\begin{verbatim}
n=     0 t=     :03 x-entropy-rate= 11.6898
n=     5 t=     :21 x-entropy-rate= 11.3218
n=    10 t=     :39 x-entropy-rate= 10.9798
n=    15 t=     :57 x-entropy-rate= 10.8136
n=    20 t=    1:15 x-entropy-rate= 10.7245
...
n=   485 t=   28:50 x-entropy-rate= 10.3250
n=   490 t=   29:08 x-entropy-rate= 10.3246
n=   495 t=   29:26 x-entropy-rate= 10.3237
...
\end{verbatim}
%
We only ran for 500 samples, and at that point, we still haven't
converged to the lowest cross-entropy rate.  We ran the sampler
overnight, and it hadn't converged even after 10,000 samples.  Because
LDA's an approximate statistical process anyway, stopping before
convergence to retrieve a single sample isn't going to make much
of a difference in terms of the clustering output.%
%
\footnote{On the other hand, if we wanted to estimate posterior variance, or
  get a very tight estimate on the posterior means, we would have to
  run to convergence.}

The program then reports the topic distributions and document
distributions and token topic assignments, all based on the 500th
sample.  These look different than the ones we saw in the synthetic
example, because there are many more longer documents (though stil
short, being based on abstracts) and many more tokens.  

The first few words in the first few topics are.
%
\begin{verbatim}
     [java] TOPIC 0  (total count=70201)
     [java] SYMBOL             WORD    COUNT   PROB          Z
     [java] --------------------------------------------------
     [java]  18616          disease     1136   0.016      30.6
     [java]  10640        infection      686   0.010      25.7
     [java]  26059         pathogen      658   0.009      25.2
     [java]   4513            fatty      576   0.008      23.6
     [java]   1863         toxicity      638   0.009      23.4
     [java]  20474             drug      732   0.010      21.2
     [java]  26758           immune      394   0.006      19.0
...
     [java]  23394        alzheimer      163   0.002      10.6
     [java]   5756            agent      216   0.003      10.6
     [java]  20684         clinical      128   0.002      10.4

     [java] TOPIC 1  (total count=88370)
     [java] SYMBOL             WORD    COUNT   PROB          Z
     [java] --------------------------------------------------
     [java]  17182    transcription     3813   0.043      51.3
     [java]   6619           factor     4383   0.050      49.7
     [java]    717          binding     1913   0.022      25.0
     [java]  21676  transcriptional     1215   0.014      24.6
     [java]  17200       regulatory     1347   0.015      24.6
     [java]  22813          element     1433   0.016      22.7
     [java]  12394             zinc      594   0.007      20.6
     [java]  13680           target     1423   0.016      19.7
     [java]  11864           finger      597   0.007      19.2
...
     [java]  11043  tissue-specific      188   0.002       8.2
     [java]   9171          hypoxia      111   0.001       7.9
     [java]  22550              dna      829   0.009       7.6
\end{verbatim}
%
Overall, the topics seem coherenent and well separated.  

Blei et al.'s paper analyzing the same data was concerned with seeing
that topics about genes grouped genes that participate in the same
pathways to show up in the same topic.  This is easy to see in the data,
with genes that operate together being highly clustered in the output.
For instance, topic 10 is about sensory-motor genes,
%
\begin{verbatim}
     [java] TOPIC 10  (total count=37931)
     [java] SYMBOL             WORD    COUNT   PROB          Z
     [java] --------------------------------------------------
     [java]  12604            touch     1785   0.047      41.3
     [java]  23411            mec-4      926   0.024      30.1
     [java]  15424           unc-86      620   0.016      24.6
     [java]  23416            mec-3      577   0.015      23.8
     [java]  16331          channel     1547   0.041      23.3
     [java]  20518              ion      771   0.020      22.5
     [java]  22644              deg      327   0.009      17.9
     [java]  23421            mec-7      309   0.008      17.4

\end{verbatim}
%
whereas topic 21 is about genes that degrade RNA, including top tokens
such as \stringmention{rnai}, \stringmention{interference},
\stringmention{dsrna}, \stringmention{silencing} and the gene anmes
\stringmention{ego-1} and \stringmention{rde-1} and
\stringmention{rrf-1}, all of which are involved in RNA silencing through
interference.

The documents are longer and more diverse in topics than in the synthetic
example.  For instance, here's a document that contains many words assigned
to topic 10, which was shwon above.
%
{\small
\begin{verbatim}
DOC 1301

TOPIC    COUNT    PROB     TOPIC    COUNT    PROB
----------------------     ----------------------
   10       36   0.354         8        2   0.021
   44       33   0.325        23        2   0.021
    2       14   0.138        11        1   0.011
   31        4   0.040        17        1   0.011
   20        3   0.030        26        1   0.011

mec-4(10) gene(44) member(10) family(2) gen(10) mutate(10) induce(44)
neuronal(10) degeneration(10) dominant(10) mutation(10) mec-4(10)
gene(11) needed(10) mechanosensation(10) cause(10) touch-receptor(10)
neuron(10) degenerate(10) deg-1(10) another(31) gene(2) mutate(10)
induce(10) neuronal(10) degeneration(10) similar(2) sequence(20)
mec-4(10) defin(2) new(2) gene(2) family(2) cross-hybridizing(8)
sequenc(8) detectable(17) spec(20) raising(44) possibility(2)
degenerative(10) condition(23) organism(20) caused(10) mutation(31)
similar(2) gen(10) dominant(10) mec-4(10) mutation(10) affect(10)
amino(2) acid(2) effect(23) amino-acid(2) substitution(2) position(2)
suggest(10) steric(10) hindrance(10) induce(44) degenerative(10)
state(26) ad(44) department(44) biological(44) scienc(44)
columbia(44) university(44) new(31) york(44) new(31) york(44) fau(44)
driscoll(10) mau(44) driscoll(10) mfau(44) chalfie(10) mau(44)
chalfie(10) mla(44) engsi(44) genbank(44) genbank(44) genbank(44)
genbank(44) genbank(44) genbank(44) genbank(44) genbank(44)
genbank(44) genbank(44) journal(44) articlecy(44) englandta(44)
naturejid(44) imsb(44)
\end{verbatim}}

\subsection{Code Walkthrough}\label{section:lda-worm-code}

The code is all in the class \code{LdaWorm}.  The main differences
from the synthetic demo include the parser for the compressed data and
a custom tokenizer for the Wormbase data.  

The method to read the corpus into an array of character sequences is
based on LingPipe's \code{FileLineReader} utility (see
\refsec{io-file-line-reader}).  It just scans the lines and
accumulates texts and adds them to an accumulating list.

The tokenization scheme is more interesting.  We create it with
the following method.
%
\codeblock{LdaWorm.1}
%
It creates a regex-based tokenizer that includes hyphens, letters and
numbers as parts of tokens.  We include hyphens and allow alphanumeric
mixtures to include tokens for gene or protein names such as
\stringmention{p53} and \stringmention{daf-19}, which would otherwise be
separated.  It also requires at least two characters.

It then immediately filters out tokens that are not at least two
characters long and don't contain at least one letter.  This filter
could've been rolled into the base regex, but these kinds of
exclusions quickly get clunky when mixed with alternations like in
our base tokenizer factory.  

We then convert the tokens to lower case and remove standard English
stop words.  We also remove custom stop words we discovered from
looking at the distribution of tokens in the corpus.  This list
contains domain-specific words like \stringmention{elegan} (remember
the ``plural'' stripping) and non-topical words from the biomedical
research domain such as \stringmention{however} and
\stringmention{although}.  We also remove the names of numbers, such
as \stringmention{eleven} and Roman numerals, such as \code{viii},
which are prevalent in this data.  Note that we have already lowercased
before stoplisting, so the stop list consists only of lowercase
entries.

The last filter is our own custom stemmer, which was designed to
follow the approach to removing English plural markers outlined in
Blei et al.'s paper.  Here's the code.
%
\codeblock{LdaWorm.2}
%
This filter fairly crudely strips off suffixes consisting of simple
English plural markers, ensuring that what's left behind is at least
two characters long.  It uses relucatant quantifiers on the contexts
ensuring the match of the plural suffix is as long as possible.  Thus
we map \stringmention{fixes} to \stringmention{fix} not
\stringmention{fixe}, and map \stringmention{theories} to
\stringmention{theori}, not \stringmention{theorie}.  In contrast, we
reduce \stringmention{lies} to \stringmention{lie}, because we can't
match the first group with a vowel if we pull of \stringmention{-ies},
but we map \stringmention{entries} to \stringmention{entr}.  

This crude stemmer will overstrip in the situation where a word ends
with one of these suffixes but is not plural.  It just checks for
suffixes and strips them off.  If the result is too short or no longer
contains a vowel, we return the original stem.  (The restriction to
ASCII is OK here because the documents were all normalized to ASCII by
the bibliography curators.)



\section{Understanding the LDA Model Parameters}

Given a tokenizer and corpus, there are three key parameters that
control the behavior the LDA model: the number of topics and the two
priors.

\subsection{Number of Topics}

The number of topics parameter controls the number of topics into
which the model factors the data.  The main effect of increasing the
number of topics is that the topics may become more fine grained.  If
there are only 10 topics to categorize all the worlds' news, we will
not discover small topics like American baseball or Australian
politics.  

As the number of topics increases, the amount of data available
to estimate each topic goes down.  If the corpus is very large,
it may be possible to fit a large number of topics effectively.

Which number of topics works best will depend on the application and
the size and the shape of data.  In the end, like any clustering or
other exploratory data analysis technique, the best solution is to use
trial and error.  Explore a range of different topic sizes, such as
10, 50, 100, and 500.

The time to process each token is proportional to the total number of
topics.  It takes ten times as long to generate a sample for a
500-topic model than a 50-topic model.

\subsubsection{Document-Topic and Topic-Word Priors}

The two priors control how diffuse the models are.  Technically,
as we explain in \refsec{lda-model}, the priors are additive
smoothing terms for the model of topics in a document and
the model of words in a topic.

In LDA, each document is modeled as a mixture of different topics.
When the document-topic prior is low, the model is encouraged to model
a document using fewer topics than when the document-topic prior is
high.  What is going on probabilistically is that the prior adds to
the probability of all topics, bringing the distribution over topics
in a document closer to uniform (where all topics have equal
probability).

Similarly, each topic is modeled as a distribution over words.  When
the topic-word prior is high, each topic is encouraged to be more
balanced in the probabilities it assigns to each word.  As with the
document-topic prior, it moves the distribution of words in topics
closer to the uniform distribution.

As with the number of topics, it will likely take some experimentation
to find reasonable priors for a problem.  A reasonable starting point
for both priors is the inverse number of outcomes.  For instance, with
50 topics, 1/50 is a good starting prior for the document-topic prior,
and with 30,000 words, 1/30,000 is a reasonable starting point for the
topic-word prior.  It's unlikely you'll want much smaller priors than
these, but raising the priors above these levels will lead to smoother
topic and document models, and may be more reasonable for some
applications.


\section{LDA Instances for Multi-Topic Classification}

We can use instances of the \code{LatentDirichletAllocation}
class to perform classification on documents that were not
in the training corpus.   These new documents are classified
with multiple topics and each word is assigned to a topic
just as when LDA is applied to a corpus of documents.  

\subsection{Constructing LDA Instances}

An instance of LDA represents a particular parameterization of
LDA's multi-topic document model.  Specifically, it stores the
document-topic prior (the ``Dirichlet'' part of LDA) and a set
of topic models in the form of distributions over words.

\subsubsection{Direct Construction}

The LDA class has a single constructor,
\code{LatentDirichletAllocation(double,double[])}, which takes a
document-topic prior and array of topic-word distributions.  You'll
notice that this does not include a symbol table or tokenizer.  LDA
runs purely as a multinomial model, and all symbol processing and
tokenization happen externally.

With an external means of generating distributions over words (say,
with a traditional naive Bayes classifier or K-means clusterer), we
can create an LDA instance from the word distributions (and
document-topic prior).

\subsubsection{Construction from a Sample}

More commonly, we will construct an LDA instance using a Gibbs sample
generated from a corpus using the static method \code{gibbsSample()}.
We saw an example of this in the stability evaluate example in
\refsec{lda-stability}.  

At the end of the Wormbase example in \refsec{lda-worm-code}, after we'd generated
the Gibbs sample and assigned it to variable \code{sample}, we snuck
the following lines of code.
%
\codeblock{LdaWorm.1}
%
This shows the production of an LDA instance from a sample using the
\code{lda()} method on the sample.  Then we use LingPipe's abstract
extenalizable utility (see \refsec{io-abstract-externalizable}) to
write its serialized form to a file.  We also serialize the map symbol
table to file.

We will be able to deserialize this model from disk and use it to
classify new documents that were not in the corpus used to generate
the chain of samples.

\subsection{Demo: Classifying with LDA}

In the sample class \code{LdaClassifier}, we show how a serialized LDA
model and symbol table We provide a sample program that illustrates
the procedure for using an LDA model and symbol table for classification.

\subsubsection{Code Walkthrough}

The code is all in the \code{main()} method, and starts by assigning a
model file \code{modelFile}, symbol table file \code{symbolTableFile},
text \code{text}, and random seed \code{randomSeed} from the
command-line parameters.  We then deserialize the two models and
recreate the tokenizer factory.
%
\codeblock{LdaClassifier.1}
%
We have to cast the deserialized object and suppress the warnings the
unchecked cast would otherwise generate.  Deserialize also throws a
\code{ClassNotFoundException}, which the \code{main()} method is also
declared to throw in addition to an \code{IOException}.

We next marshal the parameters needed for classification and
run the clasisfier.
%
\codeblock{LdaClassifier.2}
%
We use the tokenizer factory and symbol table to convert the text to a
sequence of token identifiers in the symbol table, using the static
utility method \code{tokenizeDocument()} built into LDA.  We then set
parameters of the topic estimation method, including the number of
samples, burnin, sample lag and random number generator, all of which
have the same interpretation as for the main LDA Gibbs sampling
method.  Finally, we call LDA's \code{bayesTopicEstimate()} method
with the tokens and Gibbs sampling parameters.

What happens under the hood is that we run a chain of Gibbs samples
just like we did for LDA itself.  But rather than returning an
iterator over the samples or a single sample, we return an average
over all the samples.  Because the topics are fixed by the LDA model,
this produces (an approximation of) the Bayesian estimate of the topic
distribution for the text.  Usually we don't need many samples because
we don't need multiple decimal places of accuracy.

What we get back is an array of doubles \code{topicDist}, indexed by
topic.  Topic \code{k} has probability \code{topicDist[k]}.  The
rest of the code just prints out the top 10 of these in order of
probability. 


\subsubsection{Running the Demo}

The Ant target \code{lda-classify} runs the classification demo,
using properties \code{model.file}, \code{symbol.table.file},
\code{text}, and \code{random.seed} as command-line arguments.

For an example, we chose a MEDLINE citation about C.~elegans that
was more recent than the corpus used to estimate the LDA model.%
%
\footnote{Brauchle M., K.~Kiontke, P.~MacMenamin, D.~H.~Fitch, and F.~Piano. 2009.
Evolution of early embryogenesis in rhabditid nematodes. {\it Dev Biol.} {\bf 335}(1).
\doi{10.1016/j.ydbio.2009.07.033}.}
%
\begin{quote}\small
Evolution of early embryogenesis in rhabditid nematodes. The
cell-biological events that guide early-embryonic development occur
with great precision within species but can be quite diverse across
species. How these cellular processes evolve and which molecular
components underlie evolutionary changes is poorly understood. To
begin to address these questions, we systematically investigated early
embryogenesis, from the one- to the four-cell embryo, in 34 nematode
species related to C. elegans. We found 40 cell-biological characters
that captured the phenotypic differences between these species. By
tracing the evolutionary changes on a molecular phylogeny, we found
that these characters evolved multiple times and independently of one
another. Strikingly, all these phenotypes are mimicked by single-gene
RNAi experiments in C. elegans. We use these comparisons to
hypothesize the molecular mechanisms underlying the evolutionary
changes. For example, we predict that a cell polarity module was
altered during the evolution of the Protorhabditis group and show that
PAR-1, a kinase localized asymmetrically in C. elegans early embryos,
is symmetrically localized in the one-cell stage of Protorhabditis
group species. Our genome-wide approach identifies candidate
molecules—and thereby modules—associated with evolutionary changes in
cell-biological phenotypes.
\end{quote}
%

We set the property \code{text} to this value in the Ant file, but do
not show it on the command line.  We run the command as follows.
%
\commandlinefollow{ant -Dmodel.file=wormbase.LatentDirichletAllocation -Dsymbol.table.file=wormbase.SymbolTable -Drandom.seed=42 lda-classify}
\begin{verbatim}
 Topic    Pr           Topic    Pr     
------ -----          ------ -----
    20 0.310               3 0.017
     6 0.254              38 0.015
    25 0.229              35 0.014
    21 0.077               7 0.005
    26 0.024              32 0.004
\end{verbatim}

Like LDA as a whole (see \refsec{lda-stability}), we get similar
results for the bigger topics with different random seeds.  Even with
the small number of samples, 100, we used here.  For instance, using
seed 195334, we get very similar values for the top 8 topics.
%
\commandlinefollow{ant lda-classify -Drandom.seed=195334 ...}
\begin{verbatim}
 Topic    Pr           Topic    Pr     
------ -----          ------ -----
    20 0.308              26 0.025
     6 0.252              38 0.015
    25 0.237              35 0.014
    21 0.063               9 0.009
     3 0.027              13 0.007
\end{verbatim}

Let's take a look at the top words in the top three topics, which
account for 80\% of the tokens in the document or so.
%
\begin{verbatim}
TOPIC 20  (total count=94511)
SYMBOL             WORD    COUNT   PROB          Z
--------------------------------------------------
 22828           intron     1527   0.016      35.8
 19980         briggsae     1438   0.015      32.7
 10114             spec     1911   0.020      29.3
  8465         splicing     1002   0.011      29.0
  8619           splice      812   0.009      27.5
 15121             exon     1129   0.012      26.3
  3883        evolution      921   0.010      23.4
 13935          sequenc     1599   0.017      23.4
 15569          spliced      566   0.006      22.2
 10613         sequence     2043   0.022      20.6

TOPIC 6  (total count=147967)
SYMBOL             WORD    COUNT   PROB          Z
--------------------------------------------------
 10528            model     2535   0.017      30.7
 13442         research      925   0.006      27.2
 27073           system     2621   0.018      27.1
  3378          network     1035   0.007      25.2
 22863         organism     1742   0.012      24.5
 26328          biology      910   0.006      24.2
 11563           review      537   0.004      22.0
  5439             data     1707   0.012      21.8
 18566             tool      654   0.004      21.7
  8185      information      938   0.006      21.4


TOPIC 25  (total count=89810)
SYMBOL             WORD    COUNT   PROB          Z
--------------------------------------------------
 27843          spindle     2629   0.029      50.1
 16152           embryo     4915   0.055      47.0
 17879       microtubul      971   0.011      30.4
  8200       cytokinesi      868   0.010      27.9
 20537         polarity     1272   0.014      26.0
 23475              par      680   0.008      25.5
  3098      microtubule      729   0.008      25.1
 18067       centrosome      655   0.007      25.0
 18114           cortex      640   0.007      24.7
  7615         cortical      592   0.007      23.3
\end{verbatim}
%
These topics are reasonable.  Topic 20 is largely about evolution as
it relates to splicing and introns, topic 6 is about systems biology
and this is definitely a systems biology paper, and topic 25 is about
embryos and the family of \stringmention{par} genes that control them
(including the gene \stringmention{par-1} mentioned in the article at
rank 20 or so.

With different random seeds, the LDA models produced from a corpus
will vary.  This will also affect document similarlity.  It's possible
to use full Bayesian analysis to average over all this uncertainty
(see \refsec{lda-bayesian-inference}).



\subsection{Bayesian Inference with an LDA Model}\label{section:lda-bayesian-inference}

In the last section, we showed how to provide the Bayesian point
estimate for the topic distributions.  This is a reasonable estimate
in that it minimizes the expected difference between the estimates and
the ``true'' values (given the model).  We could then plug these
estimates in to reason about the document.  

The basic problem with point estimates is that they do not encode
the variance of the estimate.  Some of the numbers may be more or
less certain than others, and this can affect downstream inference.

The LDA class also provides support for full Bayesian inference
in classification.%
%
\footnote{What we discuss uses a point estimate of the LDA model.
If we used more than one LDA model, we could start producing
Bayesian estimaes one level back.  The problem with doing that is
that the topics are not identified in LDA, so we couldn't use multiple
Gibbs samples of LDA parameters for classification directly.}
%
For Bayesian inference, we collect a group of samples, reason
about each sample, then average the results.  This may seem like
what we did in the last section, but there we averaged over the
Gibbs samples before performing inference.  With full Bayesian
inference, we do the averaging of the result of reasoning with
over each Gibbs sample.

To retrieve a set of Gibbs samples, the LDA method
\code{sampleTopics()}, taking exactly the same arguments in the same
sequence as \code{bayestopicEstimat()}, produces an array of samples,
each assigning each token to a single topic.  Each member of the
two-dimensional array assigns each token in the document to
a topic.

The point estimation method we described in the last section for
classification could be reconstructed by averaging the result of
sampling (with the priors used for estimation).  Roughly, we just count
up all the words assigned to a given topic and divide by the total
number of words; in reality, we also apply the prior.


\section{Comparing Documents with LDA}

After running LDA on a corpus, or on fresh documents using the
classifier, it's possible to use the output of LDA to compare
documents to each other.  Each document gets a distribution over
topics.  These may be compared to each other using KL-divergence in
exactly the same way as we compared topics to each other (see
\refsec{lda-stability}).

The benefit of comparing documents this way is that two documents that
share few or even no words may be considered similar by being about
the same topic.  For instance, a document mentioning
\stringmention{p53} (a tumor suppressor gene) will be related to a
document mentioning \stringmention{tumor} and
\stringmention{suppressor}, even if the documents have no other words
in common.  This is because \stringmention{p53} often mentioned in the
same documents as \stringmention{tumor} and
\stringmention{suppressor}, so they will belong to similar topics.

Comparing documents is a situation in which we can usefully apply full
Bayesian inference (see \refsec{lda-bayesian-inference}), because it
doesn't depend on the identity of topics across samples when comparing
documents.  


\section{Stability of Samples}\label{section:lda-stability}

Because LDA uses a random initialization, the results will not be the
same each time it is run with a different random seed.  For instance,
a roughly similar topic in two runs may be topic number 17 in one
run and topic 25 in another run.  The topics may even drift over time
in very, very long chains of samples.

The issue is then one of whether the topics are stable independent of
their numbering.  It turns out that across runs, many topics tend to
be relatively stable, but other topics will vary.  This idea could even
be used to decide how many topics to select, restricting the number to
those that are stable.

Steyvers and Griffiths tested the stability of topics by running LDA
over the same corpus from two different seeds, then comparing the set
of topics that are produced.  Their basic approach is to line up
the topics in the two samples based on the similarity of their
probability distributions.   In this section, we replicate their
approach with a demo.  The code is in demo class \code{LdaTopicSimilarity}.

\subsection{Comparing Topics with Symmetrized KL Divergence}

Because topics are nothing more than probability distributions over
tokens, to compare topics, we need a way to compare probabiilty
distributions.  Here, we follow Steyvers and Griffiths in using
symmetrized Kullback-Leibler (KL) divergence (see
\refsec{stats-symmetrized-kl-divergence} for definitions and
examples).

Steyvers and Griffiths produced two chains of Gibbs samples, both of
which they ran for a large nubmer of iterations.  They then compared
the last samples from each chain to each other.  Specifically, they
compared every every topic in the final sample from chain 1 against
every topic in the final sample from chain 2.  

The basis of their comparison was a greedy alignment.  Conceptually,
if we put the pairwise scores in order from lowest divergence to
highest, we walk down the list of pairs, retaining every pair for
which neither topic has been consumed.  That way, every topic only
shows up once on each side of the pairing.  The choices are greedy in
that we always select the least divergent pair of topics to add next
from among the topics that haven't yet been assigned.


\subsection{Code Walkthrough}

The code to run the example is in \code{LdaTopicSimilarity}.  Most of the
work is done in the \code{main()} method of the command.  It's declared
to throw not only an I/O exception, but also an interrupted exception,
because we are going to run each chain in its own thread.  
%
\codeblock{LdaTopicSimilarity.1}
%
After that, the method parses the text, creates the tokenizer factory,
and extracts the tokens the same way as in our earlier Wormbase demo,
using the utility methods introduced in the demo class \code{LdaWorm}.

We run two chains concurrently in their own threads.  We basically
just create the runnable from the doc-token matrix and random
seed, create threads for teh runnable, start the threads, join
them so our \code{main()} method's thread waits for the newly spawned
threads to complete.  We then extract the LDA model from the result,
where they are stored in the member variable \code{mLda}.  Finally,
we compute the scores, then print them out.
%
\codeblock{LdaTopicSimilarity.2}
%
The real work's in the runnable and in the similarity method.

The runnable that executes in each thread is defined to simply
generate 199 Gibbs samples, then set the 200th as the value of
the member variable \code{mLda}.
%
\codeblock{LdaTopicSimilarity.4}
%
We save the doc-token matrix and the random generated from the seed.
The \code{run()} method uses the other Gibbs sampling method in LDA,
which returns an iterator over samples.  It's simpler to configure
because there's nothing about number of epochs or reporting with callbacks.
We then generate number of samples minus one samples in a for loop,
then take another sample and assign it to \code{mLda} for later use.

The similarity method which computes the greedy alignment between
topics in the two LDA instances is as follows.
%
\codeblock{LdaTopicSimilarity.3}
%
It starts by walking over the pairs of topics, extracting the
distribution for a specified topic using the method
\code{wordProbabilities(int)}, which is defined on the LDA object.
We compute the divergence using the symmetrized KL divergence in
LingPipe's \code{Statistics} utility class.  

We keep track of the pairs of topics and their scores using
a nested static class, \code{TopicSim}, defined in the same file.
%
\codeblock{LdaTopicSimilarity.5}
%
The constructor stores the indices of the topics and the divergence
between their topics.  The class is defined to implement LingPipe's
\code{Scored} interface to allow easy sorting.  

Once we have the set of pairs, we sort it using Java's static
\code{sort()} utility method in the utility class \code{Collections}.
This ensures the pairs are sorted in order of increasing divergence.

Next, we create boolean arrays to keep track of which topics have
already been assigned.  All values start with default \code{false}
values, then they are set to \code{true} when the topic with the index
is used.  This makes sure we don't double-assign topics in our
alignment across samples.  We then just walk over the pairs in
increasing order of divergence, and if both their topics are not
already assigned, we assign the topics to each other, set the score,
and set the fact that we've assigned.  Then, we just return the result.

\subsection{Running the Demo}

The demo is hard coded for everything but the path to the compressed
data, which is specified as in the simple LDA example.  The Ant target
\code{lda-topic-sim} invokes the demo, specifying the path to the
compressed corpus as the value of the property \code{wormbase.corpus.gz}.
%
\commandline{ant -Dwormbase.corpus.gz="../../data-dist/2007-12-01-wormbase-literature.endnote.gz" lda-topic-sim}
\begin{verbatim}
 0   3.0     14   5.1     29   7.2     44  11.9
 1   3.2     15   5.2     31   7.3     45  12.0
 2   3.5     16   5.3     32   7.5     46  12.6
 3   3.8     17   5.6     33   7.6     47  13.1
 4   4.0     18   5.7     34   7.9     48  14.5
 5   4.2     19   6.1     35   8.0     49  15.7
 6   4.4     20   6.2     36   8.1
 7   4.4     21   6.3     37   8.3
 8   4.5     22   6.4     38   8.9
 9   4.7     23   6.5     39   9.4
10   4.8     24   6.8     40   9.9
11   5.0     25   6.9     41   9.9
12   5.0     26   7.0     42  10.3
13   5.0     27   7.1     43  11.6
\end{verbatim}    
%
The most closely related topics from the two samples drawn from
different chains have a KL-divergence of roughly 3 bits, down to
almost 16 bits for the last pair of topics.  You can also try running
for more epochs or with different random seeds, though these are
hard-coded into the demo.  As the estimates converge to their stationary
states, you'll see the top topics coming closer together and the
bottom topics moving further apart in this greedy alignment.

If you really care about comparing topics, this program is easily
modified to print out the identifiers of the topics linked
this way.  By printing the topics as we did before, you can
compare them by eye.
    


\section{The LDA Model}\label{section:lda-model}

LDA is a kind of probabilistic model known as a generative model.
Generative models provide step-by-step characterizations of how to
generate a corpus of documents.%
%
\footnote{LDA does not model the number of documents in a corpus
or the number of tokens in each document.  These values must be
provided as constants.  This is usually not a problem because
the corpus is given as data.}
%
This setup seems strange to many people at first because we are in
practice presenting LDA with a collection of documents and asking it
to infer the topic model, not the other way around.  Nevertheless, the
probabilistic model is generative.

LDA generates each document independently based on the model
parameters.  To generate a document, LDA first generates a topic
distribution for that document.%
%
\footnote{The topic distribution for a document is generated from
a Dirichlet distribution, which is where the model gets its name.}
%
This happens before any of the words in a document are generated.  At
this stage, we might only know that a document is 70\% about politics
and 30\% about the economy, not what words it contains.

After generating the topic distribution for a document, we generate
the words for the document.  

In the case of LDA, we suppose that the number of documents and the
number of tokens in each document is given.  This means they're
not part of 

That means we don't try
to predict how long each document is or use document-length
information for relating documents.  Furthermore, LDA does not We also do not attempt to model
the size of the corpus.


\subsection{Generalized ``Documents''}

Although LDA is framed in terms of documents and words, it turns out
it only uses the identities of the tokens.  As such, it may be applied
to collections of ``documents'' consisting of any kind of count data,
not just bags of words.  For instance, LDA may be applied to RNA
expression data.

