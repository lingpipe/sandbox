\chapter{Naive Bayes Classifiers}\label{chap:naive-bayes}

So-called naive Bayes classifiers are neither naive nor, under their
usual realization, Bayesian.  In this chapter, we will focus on
LingPipe's implementation of the traditional naive Bayes classifier,
only returning the other naive Bayes-like classifier after we have
covered language models and the general language-model classifiers.

After covering the basic modeling assumptions of naive Bayes, we
provide a simple example to help you get started.  The rest of the
chapter considers the details of the API, how to tune and evaluate a
naive Bayes model, and how to use it in a semi-supervised setting with
small amounts of training



\section{Introduction to Naive Bayes}

The theory of naive Bayes classifiers is based on several fairly
restrictive assumptions about the classification problem.  This
section lays out the basics.

\subsection{Texts as Bags of Words}

Although naive Bayes may be applied to arbitrary multivariate count
data, LingPipe implements text classifiers, where the objects being
classified are implementations of Java's \code{CharSequence} interface
\eg{\code{String} or \code{StringBuilder}}.

For LingPipe's naive Bayes implementations, texts are represented as
so-called bags of words.%
%
\footnote{In general, these ``words'' will be arbitrary tokens, but
  ``bag of words'' is the usual terminology.  In statistical parlance,
  a bag corresponds to a multinomial outcome.}
%
LingPipe uses a tokenizer factory to convert a character sequence into
a sequence of tokens.  The order of these tokens doesn't matter for
naive Bayes or other classifiers that operate over bags of words.

A bag of words is like a set of words in that the order doesn't
matter, but is unlike a set in that the count does matter.  For
instance, using a whitespace-based tokenizer, the strings
\stringmention{hee hee haw} and \stringmention{hee haw hee} produce
the same bag of words, namely \stringmention{hee} appears twice and
\stringmention{haw} once.  These strings produce a different bag of
words than \stringmention{hee haw}, which only has a count of one for
\stringmention{hee}.


\subsection{Exhaustivity and Exclusivity and Text Genre}

Naive Bayes classifiers require two or more categories into which
input texts are categorized.  These categories must be both exhaustive
and mutually exclusive.

For example, a news site such as Bing's or Google's, might classify a
news story as to whether it belongs in the category U.S., World,
Entertainment, Sci/Tech, Business, Politics, Sports, or Health.  As
another example, a research consortium might want to classify MEDLINE
citations mentioning mice as to whether they mention the effects of
any specific gene or not (the former class being useful for those
researching the genetic landscape of mice).  As a third example, a
marketing firm interested in a particular brand might classify a blog
post into four categories: positive toward brand, negative toward
brand, neutral toward brand, doesn't mention brand.  As a fourth
example, we could classify a patients discharge summaries (long texts
written by care givers) as to whether it indicates the patient is a
smoker or not.

Note that in each case, the texts under consideration were a
particular kind, such as newswire stories, MEDLINE citations about
mice, general blog posts, or patient discharge summaries.  We can
refer to the set of documents of this type as a genre or the domain of
the classifier.

The requirement of exhaustiveness is relative to texts that are drawn
from the genre under consideration.  We don't try to classify sports
stories as to whether they are about genomics or not or have a
positive sentiment toward the food quality.  

Often, this genre boundary can be moved by reconceptualizing the
classifier and training it on broader or narrower data types.  For
instance, the second example was restricted to MEDLINE citations about
mice, and doesn't consider full-length research articles or scientific
news stories, or even MEDLINE citations not about mice.  The third
example, in contrast, classifies all blog entries, but has a category
``doesn't mention brand'' to deal with posts not about the brand in
question.

In practice, classifiers may be applied to texts drawn from a
different genre from which they were trained.  For instance, we could
take blog sentiment classifiers and try to apply them to hotel
reviews.  Or we could apply MEDLINE citation classifiers to the full
texts of research articles.  In these cases, accuracy is almost always
worse on out-of-domain texts than in-domain texts.  For instance, we
could apply our blog sentiment classifier to product reviews in
magazines, but we would not expect it to work as well in that context
as for the kinds of blogs over which it was trained.  Similarly, we
could apply the mouse genetics classifier to full-length journal
articles, but it would likely not perform as well as for the citations
over which it was trained.

\subsection{Training Data: Natural versus Handmade}

To train naive Bayes classifiers and other supervised classifiers, we
require training data in the form of labeled instances.  In the case
of text classifiers, these consist of a sequence of texts paired with
their unique categories.  

For naive Bayes, and indeed for most statistical classifiers, the
training data should be drawn at random from the same distribution as
the test data will be drawn from.  Naive Bayes uses information about
which categories are most prevalent as well as what words are likely
to show up in which category.

For instance, we would create training data for news story section
headings by gathering news articles of the kind we'd like to classify
and assigning them to categories by hand. 

Sometimes we can find data already labeled for us.  For instance, we
could scrape a news aggregation or set of newspaper sites, recording
the category under which the article was listed (and perhaps
converting it back to the set of categories we care about).  These
might have originally arisen by hand labeling in the newspaper site,
but are most likely automatically generated by a news aggregator.
Similarly, we could examine the Medical Subject Heading (MeSH) tags
applied to MEDLINE citations by its curators to see if they were
marked as being about genomics and about mice.  

Sometimes we can gather data from less directly labeled sources.  For
instance, we can find positive and negative restaurant reviews by
examining how many stars a user assigned to them.  Or find blog posts
about food by creating a set of tags and searching for blogs with
those tags.  Or even by doing a web search with some terms chosen
for each category.

Our training data is almost always noisy, even if labeled by task
specialists by hand.  Naive Bayes is particularly robust to noisy
training data.  This produces a quality-quantity tradeoff when
creating the data.  High-quality data labeling is very labor
intensive, and its often good enough to get



\subsection{Generative Story}

Naive Bayes classifiers are based on a probabilistic model of a corpus
of texts with category-specific content.  Models like naive Bayes are
called ``generative'' in the machine learning literature because they
are richly specified enough to generate whole corpora.  In contrast,
classifiers like logistic regression are not generative in the sense
of being able to generate a corpus from the model.

The way in which naive Bayes represents a corpus, each document is
provided with a single category among a set of possible categories.
We suppose there is a fixed set of $K > 1$ categories and that each
document belongs to exactly one category.  To generate a document, we
first generate its category based on a probability distribution
telling us the prevalence of documents of each category in the
collection.

Then, given the category of a document, we generate the words in the
document according to a category-specific distribution over words.
The words are generated independently from one another given the
document's category.  The conditional independence of words given the
document category is almost always violated by natural language texts.
that is why the naive Bayes model is often erroneously called
``naive'' (the error is in labeling the model itself naive rather
than its application in a given setting).

Note that we do not model the selection of the number of topics $K$,
the number of documents $N$, or the number of words $M_n$ in the
$n$-th document; these are given as constants.  

Given a top-level distribution over categories along with a
distribution over words for each category, it is straightforward to
generate a corpus of documents.  But we can go one better and also
generate the category prevalence distribution and the
category-specific distributions over words.  In order to do so, we
need a distribution over these distributions, which is known as a
prior in Bayesian statistics.

The standard assumption is that the distribution over categories is
drawn from a symmetric Dirichlet prior and the distributions over
words for each category are all drawn from a symmetric Dirichlet
prior.  In practice, this amounts to additive smoothing for our
parameter estimates for the category distributions.  In natural
language processing, the most common priors are Laplace priors, which
adds 1, Jeffreys priors, which add 0.5, and uniform priors, which add
0.  The uniform priors lead to traditional maximum likelihood
estimates, which are dangerous in practice for reasons we explain
below.


\section{Getting Started with Naive Bayes}

LingPipe has two naive Bayes implementations.  In this section, we
focus on the traditional implementation of naive Bayes, which is found
in the LingPipe class \code{TradNaiveBayes} in the package
\code{com.aliasi.classify}.  

\subsection{Laugh Classification: His or Hers?}

Our example involves classifying laughs based on whether they were
produced by a man (his) or his wife (hers).  The manly laugh consists
of more ``haw'' and less ``hee.''  The training data we will use has
three examples of laughs from him and her.  His laughs are
\stringmention{haw}, \stringmention{haw hee haw}, and
\stringmention{haw haw}.  Her laughs in the training data are
\stringmention{haw hee}, \stringmention{hee hee hee haw}, and
\stringmention{haw}.  Note that the single word \stringmention{haw}
shows up as a laugh for her and for him.  Naive Bayes and all of our
other classifiers can handle this kind of ``inconsistent'' training
data, which is not actually inconsistent under a probabilistic model.
It's a matter of who's most likely to utter what, not that they
can't utter the same laughs.

\subsection{Setting up the Classifier}

The basic functionality of the naive Bayes classifier class can be
gleaned from a simple demo program which shows how the model is
trained and how it is run.  We provide such an example in the class
\code{TradNbDemo}, which consists of a single \code{main()} method.
The method starts by assigning the input arguments, in this case
a single argument representing the text to be classified.
%
\codeblock{TradNbDemo.1}

The next step is to set up the classifier itself, the
minimal constructor for which requires a tokenizer factory
and set of categories represented as strings.
%
\codeblock{TradNbDemo.2}
%
The regular expression \code{{\bk}P\{Z\}+} (see
\refsec{regex-unicode-classes}) produces a tokenizer factory that
defines tokens to be maximal sequences of characters which Unicode
considers not whitespace.  We have used the \code{asSet()} method in
LingPipe's \code{CollectionUtils} class to define a set of strings
consisting of the categories his and hers.  The classifier is
constructed using the categories and tokenizer factory.


\subsection{Providing Training Data}

At this point, we are ready to train the classifier using training
data.  For the demo, we just hard code the training data.  Each
training datum consists of an instance of the class
\code{Classified<CharSequence>}.  These training data are passed to
the classifier one at a time using its \code{handle()} method (see
\refsec{corpus-handlers} for a general overview of LingPipe handlers).
Because it implements \code{handle(Classified<CharSequence>)}, the
naive Bayes classifier class is able to implement the interface
\code{ObjectHandler<Classified<CharSequence>>}, which is convenient if
we want to supply the classifier as a callback to either a parser or a
corpus.

The order in which training data is provided to a naive
Bayes classifier is irrelevant, so we will provide all the
training data from the lady's laugh before the gentleman's.
%
\codeblock{TradNbDemo.3}
%
We start by creating a classification, \code{hersCl}, using the
category name literal \code{"hers"}.  A base LingPipe classification
extends the \code{Classification} class in the
\code{com.aliasi.classify} package.  These are used as the results of
classification.  Classifications are immutable, so they may be reused
for training, and thus we only have one.

After creating the classification, we create a list of training texts
using Java's built-in utility \code{asList()} from the \code{Arrays}
utility class.  Note that we used a list rather than a set because we
can train on the same item more than once.  For instance, the woman in
question may have hundreds of laughs in a training set with lots of
duplication.  Training on the same text again adds new information
about the proportion of laughs of different kinds. 

The final statement is a for-each loop, which iterates over the texts,
wraps them in an instance of \code{Classified<CharSequence>}, and
sends them to the classifier via its
\code{handle(Classified<CharSequence>)} method.  This is where the
actual learning takes place.  Wehn the handle method is called, the
classifier tokenizes the text and keeps track of the counts of each
token it sees for each category, as well as the number of instances
of each category.  

We do the same thing for training the classifier for his laughs,
so there's no need to show that code.

\subsection{Performing Classification}

Once we've trained our classifier with examples of his laughs and
her laughs, we are ready to classify a new instance.  This is
done with a single call to the classifiers \code{classify(CharSequence)}
method.
%
\codeblock{TradNbDemo.4}
%
Note that the result is an instance of \code{JointClassification},
which is the richest classification result defined in LingPipe.  The
joint classification provides a ranking of results, and for each
provides the conditional probability of the category given the text as
well as the log (base 2) of the joint probability of the category and
the text.  These are pulled off by iterating over the ranks and
then pulling out the rank-specific values.  After that, we print them
in code that is not shown.

\subsection{Running the Demo}

The code is set up to be run from Ant using the target \code{nb-demo},
reading the value of property \code{text} for the text to be classified.
for instance, if we want to classify the laugh \stringmention{hee hee},
we would call it as follows.
%
\commandlinefollow{ant -Dtext="hee hee" nb-demo}
\begin{verbatim}
Input=|hee hee|
Rank= 0  cat=hers  p(c|txt)=0.87  log2 p(c,txt)= -2.66
Rank= 1  cat= his  p(c|txt)=0.13  log2 p(c,txt)= -5.44
\end{verbatim}
%
Our classifier estimates an 87\% chance that \stringmention{hee hee} was
her laugh and not his, and thus it is the top-ranked answer (note that
we count from 0, as usual).  We will not worry about the joint
probabilities for now.  

\subsection{Unknown Tokens}

The naive Bayes model as set up in LingPipe's
\code{TradNaiveBayesClassifier} class follows the standard practice
of ignoring all tokens that were not seen in the training data.  So
if we set the text to \stringmention{hee hee foo}, we get exactly
the same output, because the token \stringmention{foo} is simply
ignored.

This may be viewed as a defect of the generative model, because it
doesn't generate the entire text.  We have the same problem if the
tokenizer reduces, say by case normalizing or stemming or stoplisting
--- we lose the connection to the original text.  Another way of
thinking of naive Bayes is as classifying bags of tokens drawn
from a known set. 


\section{Independence, Overdispersion and Probability Attenuation}

Although naive Bayes returns probability estimates for categories
given texts, these probability estimates are typically very poorly
calibrated for all but the shortest texts.  The underlying problem is
the independence assumption underlying the naive Bayes model, which
is not satisfied with natural language text.  

A simple example should help.  Suppose we have newswire articles about
sports.  For instance, as I write this, the lead story on the {\it New
  York Times} sports page is about the player Derek Jeter's contract
negotiations with the New York Yankees baseball team.  This article
mentions the token \stringmention{Jeter} almost 20 times, or about
once every 100 words or so.  If the independence assumptions
underlying naive Bayes were correct, the odds against this happening
would be astronomical.  Yet this article is no different than many
other articles about sports, or about any other topic for that matter,
that focus on a small group of individuals.

In the naive Bayes model, the probability of seeing the word
\stringmention{Jeter} in a document is conditionally independent of
the other words in the document.  The words in a document are
not strictly independent of each other, but they are independent
of each other given the category of the document.  In other words,
the naive Bayes model assumes that in documents about sports,
the word \stringmention{Jeter} occurs at a constant rate.  In
reality, the term \stringmention{Jeter} occurs much more frequently
about baseball, particularly ones about the Yankees.

The failure of the independence assumption for naive Bayes manifests
itself in the form of inaccurate probability assignments.  Luckily,
naive Bayes classifiers work better for first-best classification than
one might expect given the violation of the assumptions on which the
model is based.  What happens is that the failed independence
assumption mainly disturbs the probabiilty assignments to different
categories given the texts, not the rankings of these categories.

Using our example from the previous section of the his and hers
laughter classifier, we can easily demonstrate the effect.  The
easiest way to see this is duplicating the input.  For instance,
consider classifying \stringmention{hee hee haw}, using
%
\commandlinefollow{ant -Dtext="hee hee haw" nb-demo}
\begin{verbatim}
Rank= 0  cat=hers  p(c|txt)=0.79
Rank= 1  cat= his  p(c|txt)=0.21
\end{verbatim}
%
If we simply double the input to \stringmention{hee hee haw hee hee haw},
note how the probability estimates become more extreme.
%
\commandlinefollow{ant -Dtext="hee hee haw hee hee haw" nb-demo}
\begin{verbatim}
Rank= 0  cat=hers  p(c|txt)=0.94
Rank= 1  cat= his  p(c|txt)=0.06
\end{verbatim}
%
Just by duplicating the text, our estimate of the laugh being hers
jumps from 0.79 to 0.94.  The same thing happens when Derek Jeter's
called out twenty times in a news story.

Because naive Bayes is using statistical inference, it is reasonable
for the category probability estimates to become more certain when
more data is observed.  The problem is just that certainty grows
exponentially with more data in naive Bayes, which is a bit too fast.
As a result, naive Bayes typically grossly overestimates or
underestimates probabilities of categories for documents.  And
the effect is almost always greater for longer documents.

\section{Tokens, Counts and Sufficient Statistics}

Document length per se is not itself a factory in naive Bayes models.
It only comes into play indirectly by adding more tokens.  In general,
there are only two pieces of information that the naive Bayes classifier
uses for training:
%
\begin{enumerate}
\item The bag of tokens for each category derived from combining all
  training examples, and
\item The number of training examples per category.
\end{enumerate}
%
As long as we hold the number of examples per category constant, we
can rearrange the positions of tokens in documents.  For instance,
we could replace his examples
%
\codeblock{FragmentsNb.1}
%
with
%
\codeblock{FragmentsNb.2}
%
of even
%
\codeblock{FragmentsNb.3}
%
with absolutely no effect the resulting classifier's behavior.  Neither
the order of tokens or their arrangement into documents is considered.

The latter example shows that the empty string is a perfectly good
training example; although it has no tokens (under most sensible
tokenizers anyway), it does provide an example of him laughing and ups
the overall probability of the laugher being him rather than her.
That is, the last sequence of three training example is not equivalent to
using a single example with the same tokens,
%
\codeblock{FragmentsNb.4}



\section{Unbalanced Category Probabilities}

In the simple example of laugh classification, we used the same number
of training examples for his laughs and her laughs.  The quantity
under consideration is the number of times \code{handle()} was
called, that is the total number of texts used to train each category,
not the number of tokens.

The naive Bayes classifier uses the information gained from the number
of training instances for each category.  If we know that she is more
likely to laugh than him, we can use that information to make better
predictions.  

In the example above, with balanced training sizes, if we provide no
input, the classifier is left with only prevalence to go by, and
returns a 50\% chance for him or her, because that was the balance of
laughs in the training set.  The following invocation uses a text
consisting of a single space (no tokens).%
%
\footnote{The program accepts the empty string, but Ant's notion of
  command-line arguments don't; empty string values for the \code{arg}
  element for the \code{java} task are just ignored, as in
  \code{<arg value=""/>}.}
%
\commandlinefollow{ant -Dtext=" " nb-demo}
\begin{verbatim}
Rank= 0  cat=hers  p(c|txt)=0.50
Rank= 1  cat= his  p(c|txt)=0.50
\end{verbatim}
%

Now let's consider what happens when we modify our earlier demo
to provide two training examples for her and three for him.  The
resulting code is in class \code{CatSkew}, which is identical
to our earlier example except for the data,
%
\codeblock{TradNbSkewedTrain.1}
%
\codeblock{TradNbSkewedTrain.2}
%
Note that we have used exactly the same tokens as the first
time to train each category.  


Now if we input a text with no tokens, we get a different estimate.
%
\commandlinefollow{ant -Dtext=" " cat-skew}
\begin{verbatim}
Rank= 0  cat= his  p(c|txt)=0.58
Rank= 1  cat=hers  p(c|txt)=0.42
\end{verbatim}
%
You might be wondering why the resulting estimate is only 58\% likely
to be his laugh when 60\% of the training examples were his.  The
reason has to do with smoothing, to which we turn in the next section.

It is important to keep in mind that the training data









