\chapter{Character Language Models}\label{chap:char-lm}

A \techdef{language model} is a model that assigns probabilities to
strings.  Traditionally, these strings are either sequences of bytes,
sequences of characters, or sequences of tokens.  LingPipe provides
language model implementations for sequences of characters and
sequences of tokens.  In this chapter, we introduce the basic
interfaces and character language models.  In \refchap{token-lm},
we introduce token language models.

Although language models may be applied to arbitrary sequences, we are
going to focus on character sequences that make up natural language
text, This text may come from anywhere, including e-mail or instant
messages, blogs, newspapers, fiction novels, emergency room discharge
summaries, job applicant resume's, etc.

The basic operation of language models in LingPipe is to provide them
training data consisting of a sequence of texts.  After they are
trained, language models may be used to estimate the probability of
texts that were not in the training data.  

\section{Applications of Language Models}\label{section:char-lm-apps}

LingPipe uses character language models as the basis of several other
modules, including classification, taggers, chunkers, spelling
correction.  There are further applications for token language models
discussed in \refchap{token-lm}.  In all cases, these applications use
one or more language models to estimate different kinds of text.  For
instance, in language-model based classification, there is a language
model for each category, and texts are classified into the category
which assigns them the highest probability.  In chunkers, separate
language models might be constructed for person names, company names,
place names, and for text that's not part of a name; these are then
combined to analyze new text for names of entities.  For spelling
correction, a single language model characterizes the correct
spellings and is used to weigh alternative corrections against each
other.


One of the major applications of language models is in speech
recognition, where they play the same role as the language model in
spelling correction.  

Another application of language models is compression.  Using a
technique such as arithmetic coding, language models provide
state-of-the-art compression.  Although LingPipe does not implement an
arithmetic coder for compression, we will discuss the connection to
compression below when we discuss empirical cross-entropy.

Yet another application of language models is in characterizing
languages or sublanguages.  For instance, we can compare the entropy
of news stories in the {\it New York Times} with biomedical abstracts
in MEDLINE, the surprising result being that MEDLINE is more
predictable at the text level than the {\it Times}).  We can also
compare the entropy of Chinese and English, measuring the average
information content of a Chinese character (of which there are tens of
thousands) with the information content of an English character (of
which there are dozens).


\section{The Basics of $N$-Gram Language Models}

LingPipe's character- and token-based language models are based on
\techdef{$n$-grams}.  An $n$-gram is nothing more than a sequence of
$n$ symbols.  In this chapter, we consider character-based $n$-grams;
in \refchap{token-lm}, we consider tokens as symbols.

The basis of an $n$-gram model is storing counts for $n$-grams seen in
a training corpus of text.  These are then used to infer the
probability of a new sequence of symbols.  They key technical feature
of $n$-gram models is that they model the probability of a character
in a sequence based only on the previous $n-1$ characters.  Models
such as these with finite memories are called Markov chains.  

The chain rule allows us to model the probability of a sequence of
characters based on a model of a single character following a sequence
of characters before it.  For instance, with the character sequence
\stringmention{tooth} using 3-grams (trigrams), we decompose the
total probability of a sequence into the product of a sequence of
character probabilities, with
%
\begin{equation}
p(\stringmention{tooth})
= p(\stringmention{t})
\times
p(\stringmention{o}|\stringmention{t})
\times
p(\stringmention{o}|\stringmention{to})
\times
p(\stringmention{t}|\stringmention{oo})
\times
p(\stringmention{h}|\stringmention{ot}).
\end{equation}
%
To prevent the possibilty of underflow from multiplying long sequences
of numbers less than 1, we almost always work on the log scale.  This
also has the pleasant property of converting multiplication to
addition, yielding
%
\begin{equation}
\log p(\stringmention{tooth})
= \log p(\stringmention{t})
+
\log p(\stringmention{o}|\stringmention{t})
+ 
\log p(\stringmention{o}|\stringmention{to})
+
\log p(\stringmention{t}|\stringmention{oo})
+ 
\log p(\stringmention{h}|\stringmention{ot}).
\end{equation}


\section{Character-Level Language Models and Unicode}

LingPipe's character-based language models are models of sequences of
Java \code{char} values, which as you should recall from
\refsec{char-primitive}, represent UTF-16 byte pairs.  When LingPipe's
language models were coded, every Unicode code point could be coded in
a single UTF-16 byte pair.  As we discussed, this is enough to cover
Unicode's basic multilingual plane (BMP), which includes most
characters for most languages.%
%
\footnote{As of Unicode 4.1, there are characters with code points
  outside of the BMP.  To code such characters requires two 16-bit
  \code{char} values, known as a surrogate pair.  For the purposes of
  LingPipe's language models, characters are 16 bit UTF-16 values and
  a code point beyond the BMP is considered two characters.  What this
  will mean in practice is that LingPipe's language models will assign
  non-zero probability to sequences of \code{char} values that do not
  correspond to legal UTF-16 encodings, and thus don't correspond to
  Unicode strings.  This is internally consistent and does not affect
  most of the applications of language models.  The exception is
  untokenized spelling correction, where we have to be careful not to
  suggest corrections that are illegal sequences of \code{char}
  values.  In other cases, we just lose some modeling efficiency (in
  the statistical sense of efficiency, not the run-time sense).}




\section{Language Model Interfaces}\label{section:char-lm-interfaces}

There is a rich set of interfaces for language models and related
support classes in the LingPipe package \code{com.aliasi.lm}.  The
basic interfaces are in the \code{LanguageModel} interface, most
being defined as nested interfaces.

\subsection{Predictive: \code{LanguageModel}}

The top-level language model interface, \code{LanguageModel}, defines
methods for estimating probabilities of sequence of characters.  It
defines two methods for estimating the log (base 2) probability of a
sequence of characters, one based on arrays and one based on the
character sequence interface.
%
\begin{verbatim}
double log2Estimate(char[] cs, int start, int end);
double log2Estimate(CharSequence cs);
\end{verbatim}
%
The first method uses an indexed character array to define the slice
of characters from \code{cs[start]} to \code{cs[end-1]}.  For most
methods, the character slice method is more efficient and the
character-sequence method defined by first converting the sequence to
an array of characters.

The reason estimates are returned as logarithms is that we would
otherwise run into underflow.  If each character has an average
probabilty of 0.25 (roughly what they have in open-domain English text
like a newspaper), the probability of a sequence of 600 tokens has a
$2^{-1200}$ probabilty, which is too small to fit into a Java
double-precision floating point value (the minimum value for which is
$2^{-1074}$).

\subsection{Trainable: \code{LanguageModel.Dynamic}}

Language models that may be trained by supplying example text
implement the interface \code{LanguageModel.Dynamic}.  This interface
extends the corpus interface \code{ObjectHandler<CharSequence>},
which specifies the method
%
\begin{verbatim}
void handle(CharSequence cs);
\end{verbatim}
%
The character sequence is considered as training data and used to fit
the model parameters.  See \refsec{corpus-handlers} for more
information on \code{ObjectHandler} and how it fits into corpus
patterns.

There are four other methods named \code{train()}, the first two
of which simply allow training with character sequences.
%
\begin{verbatim}
void train(CharSequence cs);
void train(CharSequence cs, int count);
\end{verbatim}
%
The first method is a legacy method duplicating the functionality of
\code{handle()}.  The second method takes an integer count value which
determines the number of times the character sequence is used for
training.  Although implemented more efficiently, a training call
with a count is equivalent to calling the basic training method the
count number of times.

The last two methods just repeat the character sequence methods
with character slices.


\subsection{Conditional Probabilitiess: \code{LanguageModel.Conditional}}

Conditional language models predict the probability of a character
given the previous characters.  The interface
\code{LanguageModel.Conditional} specifies the method
%
\begin{verbatim}
double log2ConditionalEstimate(CharSequence);
\end{verbatim}
%
This method returns the probabilty of the last character in
the sequence given the initial characters in the sequence.  For
instance, \code{log2ConditionalEstimate("abcd")} returns the
probability of the character \charmention{d} following the
sequence of characters stringmention{abc}.  There is also
a method working on character slices.


\subsection{Bounded: \code{LanguageModel.Sequence}}

The interface \code{LanguageModel.Sequence} is just a marker
interface, meaning that it does not specify any new methods other than
that inherited from its superintervace, \code{LanguageModel}.

Sequence-based language model implementations are normalized so
that the sum of the probability of all strings of all lengths is one.%
%
\footnote{Because it's just a marker interface, the requirement is
  conventional rather than expressed in Java's type language.  This is
  similar to interface requirements on collections in the
  \code{java.util} core library.}
%
In symbols, this amounts to
%
\begin{equation}
\sum_{n \geq 0} \hspace*{8pt} \sum_{\text{\code{cs.length()} == n}} \hspace*{3pt} 2^{\text{\code{log2Prob(cs)}}} = 1.
\end{equation}

\subsection{Unbounded: \code{LanguageModel.Process}}

The other type of language model is a process language model, declared
by implementing the marker interface \code{LanguageModel.Process}.  A
process language model treats a sequence of characters as being
generated as a kind of random process that generates a character at a
time without a distinct beginning or end.  

Because they don't model the beginning or end of a string, process
language models are normalized by length so that the sum of the
probabilities of all strings of a given length is one.  In symbols,
for every \code{n}, we have
%
\begin{equation}
\sum_{\text{\code{cs.length()} == n}} \hspace*{3pt} 2^{\text{\code{log2Prob(cs)}}} = 1.
\end{equation}
%


\section{Process Character Language Models}

As we saw in the last section, language models have a particularly
simple interface.  We just give them text to train and then they
can model new text. 

The simplest language model implementation supplied by LingPipe is
\code{NGramProcessLM}, in package \code{com.aliasi.lm}.  This class
implements several interfaces, including several of the basic language
model interfaces, \code{LanguageModel}, the unbounded interface,
\code{LanguageModel.Process}, the trainable interface
\code{LanguageModel.Dynamic}, and conditional prediction interface
\code{LanguageModel.Conditional} (see \refsec{} for descriptions
of these interfaces).

In addition to the LM interfaces, the \code{NGramProcessLM} class also
implements \code{ObjectHandler<CharSequence>} (see
\refsec{corpus-handlers} for a description of the handler interface).
This allows language models to be integrated into the general
parser/handler/corpus interfaces (see \refchap{corpus} for more
details).

Furthermore, process character LMs are both serializable (see
\refsec{io-object-data-io}) and compilable (see
\refsec{io-compilable}).  As we will see later in this section,
compilation produces a very efficient, but no longer dynamic
version of the language model, whereas serialization will allow
it to be written out and read back in.


\subsection{Basic LM Training and Inference Demo}

Perhaps the simplest way to get a feel for how language models work is
to code one up.  We provide an example in the class
\code{ProcessLmDemo} in this chapter's package,
\code{com.lingpipe.book.charlm}.  

\subsubsection{Code Walkthrough}

As with all of our demos, it's set up as a simple \code{main()} method
that reads its arguments from the command line.  Specifically, we need
an n-gram length (\code{ngram}), a text on which to train
(\code{textTrain}) and a text on which to test (\code{textTest}),
which are supplied as the first three arguments.  The training and
evaluation are then almost trivial.
%
\codeblock{ProcessLmDemo.1}
%
We start by creating a new instance of an \code{NGramProcessLM}
supplying the n-gram length to the constructor.  We then take the
training text string and pass it to the language model's
\code{handle(CharSequence)} method, which adds the text to the
training data.  Finally, we compute the log (base 2) of the
probaiblity of the test text using the language model's
\code{log2Estimate(CharSequence)} method.  

\subsubsection{Running the Demo}

The Ant target \code{process-demo} runs the command, using
the property \code{ngram} for $n$-gram length, the property
\code{text.train} for the training text, and the property
\code{text.test} for the test text.  It is thus run as
follows.
%
\commandlinefollow{ant -Dngram=5 -Dtext.train="abracadabra" -Dtext.test="candelabra" process-demo}
\begin{verbatim}
ngram=5    train=|abracadabra|    test=|candelabra|
log2 p(test|train)=-69.693
\end{verbatim}
%
The command-line arguments are first printed (with vertical bars to
denote the boundaries of the strings) and then the log probability of
the test sequence given the training data is shown.

One way of thinking of language models is as measuring a kind
of text similarity.  The more similar the test text is to the
training text, the higher its probability.  For instance, consider

If the text is very predictable, the log2 probability will be lower
(larger here means smaller absolute value, because logarithms of
values in $[0,1]$ are always negative).  For instance, consider
the following case.
%
\commandlinefollow{ant -Dngram=2 -Dtext.train="ababababab" -Dtext.test="ababab" process-demo}
\begin{verbatim}
ngram=2    train=|ababababab|    test=|ababab|
log2 p(test|train)=-3.060
\end{verbatim}

In general, shorter texts are more predictable.  For instance,
consider the same training text as above with a test text
that's a subsequence of the test text above:
%
\commandlinefollow{ant -Dngram=2 -Dtext.train="ababababab" -Dtext.test="abab" process-demo}
\begin{verbatim}
ngram=2    train=|ababababab|     test=|abab|
log2 p(test|train)=-2.419
\end{verbatim}
%
Note that -2.419 is greater than -3.060.  In linear probability terms,
that's $2^{-2.419} = 0.225$, whereas $2^{-3.060} = 0.120$.  In other
words, the model, after training, says that \stringmention{abab} is
almost twice as likely as \stringmention{ababab}.

For process language models, which don't model boundaries, substrings
always have higher (log) probability.  This will not necessarily be
the case for the boundary language models we consider in the next section.

A language model will assign probabilities to any texts of any length.
The characters don't need to have been seen in the training data.  For
instance, consider our first example above, where the characters
\charmention{e}, \charmention{n}, and \charmention{l} were new to the
training data.  Characters that were not seen in the training data tend
to have fairly low probabilities.  For instance, consider
%
\commandlinefollow{ant -Dngram=2 -Dtext.train="ababababab" -Dtext.test="xyxy" process-demo}
\begin{verbatim}
ngram=2    train=|ababababab|    test=|xyxy|
log2 p(test|train)=-71.229
\end{verbatim}
%
Again, it's clear why we use the log scale --- $2^-{71}$ is a very low
probability indeed.

The $n$-gram length is the most important of the tuning parameters,
the rest of which we turn to in the next section.  A longer $n$-gram
will provide a tighter model of the training text than shorter
$n$-grams.  The longer the $n$-gram, the higher the probability
assigned to the training text itself as well as to very similar text
(again measuring by $n$-gram frequency).  For instance, consider

\commandlinefollow{ant -Dngram=2 -Dtext.train="abcdef" -Dtext.test="abcdef" process-demo}
\begin{verbatim}
ngram=2    train=|abcdef|    test=|abcdef|
log2 p(test|train)=-11.334
\end{verbatim}

and

\commandlinefollow{ant -Dngram=3 -Dtext.train="abcdef" -Dtext.test="abcdef" process-demo}
\begin{verbatim}
ngram=3    train=|abcdef|    test=|abcdef|
log2 p(test|train)=-10.884
\end{verbatim}

The improvement is relatively small.  If we had repeated the training
text many times or if it had been longer, the effect would be
stronger.  Here, if you actually go up to 4-grams, the probability
estimate gets worse.  The reason for this is the default smoothing
parameters, which we discuss in \refsec{char-lm-smoothing}.

One thing to beware of is that increasing the $n$-gram size tends to
have a dramatic impact on the amount of memory required.  This is
apparent when you consider that for basic English, there are only a
few dozen characters and thus only a few dozen unigrams (1-grams).
But there are thousands of bigrams (2-grams), and millions of
5-grams and 6-grams.  Even with 40 characters, there are over
2.5 million potential 4-grams.  Luckily, text is fairly predictable,
so the set of $n$-grams actually observed tends to grow more
slowly than this worst case may lead you to believe.  


\section{Language Model Smoothing by Interpolation}\label{section:char-lm-smoothing}

Maximum likelihood estimation would provide weights for characters
that are based only on the empirically observed counts in a training
set.  The problem with this is that we are likely to run into
sequences of characters in new test data that were not in the training
data.  We would like to assign these unseen sequences non-zero
probabilities so that the documents we observe in testing are assigned
non-zero probabilities.

\subsection{The Problem with Maximum Likelihood}

For example, suppose we're trying to model the probability of the
letter \charmention{r} following the sequence \stringmention{the new
  cha} with a 7-gram model.  In a pure maximum likelihood 7-gram, the
probability of seeing the letter \charmention{r} will be given by
dividing the number of times \stringmention{ew char} occurred in the
training corpus by the number of times \stringmention{ew cha} showed
up followed by any character.  For instance, there might be two
instances of \stringmention{ew chai}, perhaps as a part of
\stringmention{new chair} (furniture) or \stringmention{new chai}
(beverage) and one of \stringmention{ew chas}, perhaps as part of
\stringmention{grew chastened}, in the corpus.  In this case, the
probability of the next letter being \charmention{i} is estimated at
2/3 and the probability of the next letter being \charmention{s} at
1/3.  Note that there is no probability left over for the possibility
of \charmention{r} being next.  Thus our model says the string we
observe during testing is impossible.  

\subsection{Witten-Bell Smoothing}

To get around this problem, we use a a technique known as
\techdef{interpolative smoothing} to ensure that every sequence of
characters is assigned a non-zero probability.  

LingPipe uses an approached to smoothing developed by Ian Witten and Timothy Bell.%
%
\footnote{Witten, Ian H.\ and Timothy C.~Bell. 1991. The zero-frequency
  problem: estimating the probabilities of novel events in adaptive
  text compression. {\it IEEE Transactions on Information Theory} {\bf 37}(4).}
%
The idea behind Witten-Bell smoothing for $n$-grams involves blending
order $n$ predictions with order $n-1$ predictions, all the way down
to a basic uniform distribution at order 0.  Thus even if we haven't
ever seen \stringmention{ew char} in the corpus, we will still assign
it a non-zero probability.  That probability will be a weighted
average of the 7-gram estimate of \charmention{r} given
\stringmention{ew cha}, the 6-gram estimate of \charmention{r} given
\stringmention{w cha}, the 5-gram estimate of \charmention{r} given
\stringmention{ cha} (with an initial space), the 4-gram estimate of
\charmention{r} given \stringmention{cha}, the 3-gram estimate of
\charmention{r} given \stringmention{ha}, the 2-gram estimate of
\charmention{r} given \stringmention{a}, the 1-gram estimate of
\charmention{r} given \stringmention{} (empty context).  

\subsection{Bottoming Out at the Uniform Model}

This recursion bottoms out at the uniform estimate, which we consider
a 0-gram estimate for the sake of consistency.  In the uniform
estimate, each character is given equal probabilty.  If there are 256
possible characters, as in Latin 1, then each has a 1/256 probability
estimate in the uniform base case.  

Because we have a uniform base case, LingPipe's character language
models require the number of possible characters to be specified in
the constructor.  The default is \code{Character.MAX\_VALUE}, which is
$2^{16}-1$, which is a slight overestimate of the number of Unicode
UTF-16 values (see \refsec{char-primitive}).  The number of characters
can be set lower if we know we are dealing with ASCII or Latin 1 or
another restricted subset of Unicode characters.

\subsection{Weighting the Models}

We have so far said that we take a weighted average of the predictions
of an $n$-gram, $(n-1)$-gram, down to a uniform model.  What we haven't
said is how the weights for each model are determined.  We'll provide
precise mathematical definitions in \refsec{char-lm-math}.  For now, we
will provide a more descriptive explanation.

The first thing to note is that the weighting is context dependent in
that it depends on the characters in the context from which we are
predicting the next character.  That is, if we are predicting the next
character from context \stringmention{ew char} we will likely have
different weightings than if we predict from context \stringmention{t
  of th}.

The weight assigned to the order $n$-gram model in the interpolated
model is based on two factors determined from the context for the
prediction and one parameter in the constructor.  For deciding how
heavily to weight an $n$-gram context in prediction, we take into
account how many times that context has been seen in the training
data.  The more we have seen it in the training data, the more heavily
it is weighted.  The second factor for weighting is how many different
outcomes we've seen given that context.  If we have a context with
many different outcomes, we will weight it less for prediction.  

In addition to these contextual factors, there is also a parameter
given to the constructor (with variable named \code{lambdaFactor})
that determines how much to weight higher-order contexts relative to
lower-order contexts.  The higher the value of this factor, the more
agressively we smooth.  In other words, with high values of the lambda
factor, the more heavily the lower-order $n$-grams are weighted.


\section{The Mathematical Model}\label{section{char-lm-math}}

The mathematics of character language models are straightforward.
We'll begin with the process language model and move onto the
bounded language model.

\subsection{Strings}

First we'll establish some notation for talking about strings
mathematically.  Suppose we have an alphabet $\Sigma$ of characters.%
%
\footnote{In practice, $\Sigma$ will be a subset of Unicode code
  points.}
%
Let $\Sigma^n$ be the set of all sequences of such characters of
length $n$, which are called strings.  We let $\Sigma^* = \bigcup_{n
  \in \nats} \Sigma^n$ be the set of all strings.  We'll let
$\emptystring$ be the \techdef{empty string}, which is defined as the
unique sequence of length 0.


\subsubsection{The Chain Rule}

The basic idea of $n$-gram language models is
to use the chain rule to factor the probabiltiy of a string into
the product of the probabilities of each of its characters given
the characters that came before it.  In symbols, suppose we have
a string $\sigma = s_1,\ldots,s_k$ of length $k$.  The
chain rule tells us we can compute its probability as
%
\begin{equation}
p(s_1,\ldots,s_k) = p(s_1) \times p(s_2|s_1) \times \cdots \times p(s_k|s_1,\ldots,s_{k-1}).
\end{equation}
%

\subsection{Markov Chains}

The point of using a limited-context-length $n$-gram is that you only
look back at most $(n-1)$ characters.  This gives us
%
\begin{equation}
p(s_1,\ldots,s_k) 
= p(s_1) \times p(s_2|s_1) \times \cdots \times p(s_k|s_{k-n+1},\ldots,s_{k-1}).
\end{equation}
%
For instance, if we have a 2-gram model and string \stringmention{abra},
our model is
%
\begin{equation}
p(\stringmention{abra}) 
= p(\stringmention{a}) 
\times p(\stringmention{b}|\stringmention{a}) 
\times p(\stringmention{r}|\stringmention{b}) 
\times p(\stringmention{a}|\stringmention{r}).
\end{equation}
%
Assuming that our history is bounded in this way to a finite history
leads to what is known as a \techdef{Markov model}, with a sequence of
characters generated from it called a \techdef{Markov chain}.  In a
Markov model of character string generation, each character is
generated based on finitely many previous characters.  Note that being
Markovian doesn't say how the probabilty of the next character is
estimated, but only that it depends on at most a bounded finite
history.

\subsection{Training Data Sequence Counts}

Now suppose we have seen training data in the form of a set of $M$
strings $\sigma_1,\ldots,\sigma_M$.  Given such a training set, let
$\cnt{s}$ be the number of times the string $s$ appeared as a
substring of one of the training strings $\sigma_m$.  For instance, if
the training strings are \stringmention{abe} and
\stringmention{abracadabra}, the substring \stringmention{ab} shows up
3 times, once in the first string and twice in the second.

Let $\extcnt{\sigma}$ be the count of all extensions by one character
of the string $\sigma$, that is
%
\begin{equation}
\extcnt{\sigma} = \sum_{s' \in \Sigma} \cnt{\sigma \cdot s}.
\end{equation}
%
For instance, in the training data, the count of \stringmention{ra} is
2, but the extension count of \stringmention{ra} is only 1, because
the second training string ends with \stringmention{ra} which is not
extended.


\subsection{Maximum Likelihood Estimates}

We'll use the notation $\pmle(s_n|s_1,\ldots,s_{n-1})$ for
the maximum likelihood estimate of the conditional probabilty of a
seeing the character $s_n$ given the previous $(n-1)$ characters
$s_1,\ldots,s_{n-1}$.  The maximum likelihood estimate model is
derived in the usual way, by counting in the training data.
%
\begin{equation}
\pmle(s_n|s_1,\ldots,s_{n-1})
= \frac{\cnt{s_1,\ldots,s_{n-1},s_{n}}}
       {\extcnt{s_1,\ldots,s_{n-1}}}.
\end{equation}
%
That is, we count up all the times we've seen the character in
question following the string and divide by the number of times
the string was extended by any character.

These maximum likelihood estimates will almost certainly assign zero
probability to many characters in even relatively modest length
$n$-grams. 

\subsection{Witten-Bell Smoothing}

We follow Witten and Bell's approach to smoothing these maximum
likelihood estimates through interpolation.  The basic definition is
recursive, interpolating the maximum likelihood estimate at length $n$
with the smoothed estimate of length $(n-1)$.  In symbols, we let
$\lambda_{s_1,\ldots,s_{n-1}} \in [0,1]$ be the weight of the maximum
likelihood estimate for predicting the next character given that the
previous characters were $s_1,\ldots,s_{n-1}$.  The recursive case
of the definition is then given by
%
\begin{equation}
p(s_n|s_1,\ldots,s_{n-1})
= \lambda_{s_1,\ldots,s_{n-1}} \times \pmle(s_n|s_1,\ldots,s_{n-1})
+
(1 - \lambda_{s_1,\ldots,s_{n-1}}) \times p(s_n|s_1,\ldots,s_{n-2}).
\end{equation}
%
On each application of this rule, the context is shortened by one
character.

The recursion bottoms out by interpolating with the uniform
distribution $\punif(s_n)$, where if $N$ is the total number
of characters, then 
%
\begin{equation}
\punif(s_n) = 1/N.
\end{equation}
%
The exact form of the base case interpolates with the 1-gram
model.
%
\begin{equation}
p(s_n) = \lambda_{\emptystring} \times \pmle(s_n)
+ (1 - \lambda_{\emptystring}) \times \punif(s_n).
\end{equation}


\subsection{Process Language Models}

A process language model assigns probabilities to strings
in such a way that the probabilities of all strings of a given
length sums to 1.%
%
\footnote{The terminology, which is not standard in the
  language-modeling literature, derives from the fact that generating
  the sequence of characters forms what is known as a \techdef{random
    process.}}
%
For process language models, if we let $p(s)$ be the probability of a
string $s$, then
%
\begin{equation}
\sum_{s \in \Sigma^n} p(s) = 1.
\end{equation}
%


\subsection{Sequence Language Models}

Unlike the process language models, for which the probabilities of all
strings of a given length sum to 1, a bounded language model is a
proper model of arbitrary character sequences.  Thus a bounded
language model, with probability $p(s)$, is such that
%
\begin{equation}
\sum_{s \in \Sigma^n} p(s) = 1.\label{equation:lm-seq-norm}
\end{equation}
%
Thus we think of sequence language models as generating finite
sequences of characters then stopping; a process language model
goes on generating characters indefinitely.

We follow fairly standard (though often undocumented) procedure of
definining bounded language models in terms of process language
models.  Suppose we are definining sequences over the character set
$\Sigma$.  We will choose a new character, $b$, called the boundary
character, such that $b \not\in \Sigma$.  We then define a process
language model over the alphabet $\Sigma \cup \setext{b}$ and use it
to define our sequence language model distribution.

We define the probability of a string $s = s_1,\ldots,s_M \in \Sigma^*$
as
%
\begin{equation}
p_B(s_1,\ldots,s_M) = p(s_1,\ldots,s_M,b|b),\label{equation:lm-seq-def}
\end{equation}
%
where $p_B$ is the sequence language model probability distribution.
In words, we start with the boundary character $b$ as context, then
generate our string $s = s_1,\ldots,s_M$, then generate a final
boundary character $b$.  Note that we chose $s$ from the unaugmented
set of strings  $\Sigma^*$.  

Because we now have a termination condition for our character
generating process, it is fairly straightforward to verify that the
process generates a probability distribution normalized over all
strings, as shown in \refeq{lm-seq-norm}.  We start with the boundary
character, though don't generate it, which is why it's in the context
in \refeq{lm-seq-def}.  We then continue generating, at each point
deciding whether to generate the boundary character $b$ and stop or to
generate a non-boundary character and keep going.  Because the sum of
each next character in the augmented alphabet $\Sigma \cup \setext{b}$
sums to 1.0, the normalization in \refeq{lm-seq-norm} holds.











