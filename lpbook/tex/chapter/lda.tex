\chapter{Latent Dirichlet Allocation}\label{chapter:lda}

Latent Dirichlet allocation (LDA) provides a statistical approach to
document clustering based on the words (or tokens) that appear in a
document.  Given a set of documents, a tokenization scheme to convert
them to bags of words, and a number of topics, LDA is able to infer
the set of topics making up the collection of documents and the
mixture of topics found in each document.  These assignments are soft
in that they are expressed as probability distributions, not absolute
decisions.

At the basis of the LDA model is the notion that each document is
generated from its own mixture of topics.  For instance, a blog post
about a dinner party might 70\% about cooking, 20\% about sustainable
farming, and 10\% about entertaining.

Applications of LDA include document similarity measurement, document
clustering, topic discovery, word association for search, feature
extraction for discriminative models such as logistic regression or
conditional random fields, classification of new documents, language
modeling, and many more.

\section{Corpora, Documents, and Tokens}

For LDA, a corpus is nothing more than a collection of documents.
Each document is modeled as a sequence of tokens, which are often
words or normalized forms of words.

Applications of LDA typically start with a real document collection
consisting of character data.  LingPipe uses tokenizer factories to
convert character sequences to sequences of tokens (see
\refchap{tokenization}).  Utilities in the LDA implementation class
help convert these to appropriate representations with the help of
symbol tables (see \refchap{symbol-tables}).

For LDA, and many other models such as traditional naive Bayes
classification and $k$-means clustering, only the count of the words
is significant, not their order.  A bag is a data structure like a
set, only with (non-negative) counts for each member.  Mathematically,
we may model a bag as a mapping from its members to their counts. Like
other models that don't distinguish word order, LDA is called a
bag-of-words model.


\section{LDA Parameter Estimation}

In this section, we will cover the steps necessary to set up the
constant parameters for an LDA model and then estimate the unknown
parameters from a corpus of text data.

\subsection{Synthetic Data Example}

We begin with a synthetic example due to Steyvers and Griffiths.%
%
\footnote{From Steyvers, Mark and Tom Griffiths. 2007. Probabilistic
  topic models. In Thomas K.~Landauer, Danielle S.~McNamara, Simon
  Dennis and Walter Kintsch (eds.), {\it Handbook of Latent Semantic
    Analysis}. Laurence Erlbaum.}
%
Synthetic or ``fake data'' models play an important role in
statistics, where they are used like unit tests for statistical
inference mechanisms.  

In this section, we will create an LDA model by fixing all of its
parameters, then use it to generate synthetic data according to the
model.  Then, we will provide the generated data to our LDA
inference algorithm and make sure that it can recover the known
structure of the data.

The implementation of our synthetic example, which illustrates
the basic inference mechanism for LDA, is in the demo class
\code{SyntheticLdaExample}.

\subsubsection{Tokens}

Steyvers and Griffiths' synthetic example involved only two topics and
five words.  The words are \stringmention{river},
\stringmention{stream}, \stringmention{bank}, \stringmention{money},
and \stringmention{loan}.  The first two words typically occur in
articles about rivers, and the last two words in articles about money.
The word \stringmention{bank}, the canonical example of an ambiguous
word in English, occurs in both types of articles.  

\subsubsection{What's a Topic?}

A topic in LDA is nothing more than a discrete probability
distribution over words.  That is, given a topic, each word has a
probability of occurring, and the sum of all word probabilities in a
topic must be one.

Steyvers and Griffiths' example involves two topics, one about
banking and one about rivers, with the following word probabilities.
%
\begin{center}
\begin{tabular}{l|rrrrr}
& \stringmention{river} 
& \stringmention{stream}
& \stringmention{bank}
& \stringmention{money}
& \stringmention{loan}
\\ \hline
\tblhead{Topic 1} & 1/3 & 1/3 & 1/3 & 0 & 0 
\\
\tblhead{Topic 2} & 0 & 0 & 1/3 & 1/3 & 1/3
\end{tabular}
\end{center}
%
Topic 1 is about water and topic 2 about money.  If a word is drawn
from topic 1, there is a 1/3 chance it is \charmention{river}, a 1/3
chance it is \charmention{stream} and a 1/3 chance it is
\charmention{bank}.  


\subsubsection{What's a Document?}

For the purposes of LDA, a document is modeled as a sequence of
tokens.  We use a tokenizer factory to convert the documents into
counts.  The identity of the tokens doesn't matter.  It turns out the
order of the tokens doesn't matter for fitting the model, though we
preserve order and identity so that we can label tokens with topics in
our final output.

The code in \code{SyntheticLdaExample} starts out defining an array of
sixteen character sequences making up the document collection that
will be clustered.  These documents were generated randomly from an
LDA model by Steyvers and Griffiths.  The texts making up the
documents are defined starting with
%
\codeblock{SyntheticLdaExample.1a}
%
and ending with
%
\codeblock{SyntheticLdaExample.1b}
%
The only reason we used concatenation is so that the lines would fit
on the pages of the book.  The complete set of documents is most
easily described in a table.
%
\begin{center}
\begin{tabular}{c|ccccc}
% & \multicolumn{5}{c}{\tblhead{Tokens}} \\
\tblhead{Doc ID}
& \tblhead{river}
& \tblhead{stream}
& \tblhead{bank}
& \tblhead{money}
& \tblhead{loan}
\\ \hline
\tblhead{0} & 0 & 0 & 4 & 6 & 6 
\\
\tblhead{1} & 0 & 0 & 5 & 7 & 4
\\
\tblhead{2} & 0 & 0 & 7 & 5 & 4 
\\
\tblhead{3} & 0 & 0 & 7 & 6 & 3 
\\
\tblhead{4} & 0 & 0 & 7 & 2 & 7
\\
\tblhead{5} & 0 & 0 & 9 & 3 & 4
\\
\tblhead{6} & 1 & 0 & 4 & 6 & 5 
\\
\tblhead{7} & 1 & 2 & 6 & 4 & 3
\\
\tblhead{8} & 1 & 3 & 6 & 4 & 2
\\
\tblhead{9} & 2 & 3 & 6 & 1 & 4
\\
\tblhead{10} & 2 & 3 & 7 & 3 & 1
\\
\tblhead{11} & 3 & 6 & 6 & 1 & 0
\\
\tblhead{12} & 6 & 3 & 6 & 0 & 1
\\
\tblhead{13} & 2 & 8 & 6 & 0 & 0
\\
\tblhead{14} & 4 & 7 & 5 & 0 & 0
\\
\tblhead{15} & 5 & 7 & 9 & 0 & 0
\end{tabular}
\end{center}
%
The first document (identifier/position 0) has four instances of
\stringmention{bank}, six of \stringmention{money} and six of
\stringmention{loan}.  Each document has sixteen tokens, but equal
sized documents is just an artifact of Steyvers and Griffiths'
example.  In general, document sizes will differ.

\subsubsection{Tokenizing Documents}

LDA deals with token identifiers in the form of a matrix, not
with string tokens.  But there's a handy utility method to produce
such a matrix from a corpus of texts and a tokenizer factory.
The first few lines of the \code{main()} method in
the \code{SyntheticLdaExample} class provide the conversion
given our static array \code{TEXTS} of character sequences:
%
\codeblock{SyntheticLdaExample.2}
%
The regex-based tokenizer factory is used with a regex defining tokens
as maximal sequences of characters in the Unicode letter class (see
\refsec{tok-regex-tokenizer-factory} for regex tokenizer factories and
\refsec{regex-unicode-classes} for Unicode regex classes).  This will
pull out each of the words in our documents.  

Next, we create a symbol table (see \refsec{symbol-map-symbol-table})
which we will use to provide integer indexes to each of the tokens.

The variable \code{minCount} will specify how many instances of a
token must be seen in the entire set of documents for it to be used in
the model.  Here we set the threshold to 1, so no tokens will be
pruned out.  For most applications, it makes sense to use a slightly
larger value.

Then, we create a two-dimensional array of document-word identifiers.
The value of \code{docWords[n]} is an array of the symbol identifiers
from the symbol table for the words in document \code{n} (which will
range from 0 (inclusive) to 15 (inclusive) for our 16-document
corpus).  

The program prints the symbol table and document-word array so
you may see what is actually produced,
%
\begin{verbatim}
symbTab={0=stream, 1=money, 2=bank, 3=loan, 4=river}

docWords[ 0] = { 2, 3, 1, 3, 3, 1, 2, 3, 2, 3, 3, 1, 2, 1, 1, 1 }
docWords[ 1] = { 3, 1, 1, 3, 2, 2, 1, 2, 1, 3, 1, 1, 2, 3, 2, 1 }
...
docWords[14] = { 4, 0, 0, 0, 4, 0, 0, 2, 2, 2, 2, 4, 4, 0, 2, 0 }
docWords[15] = { 0, 4, 4, 2, 0, 0, 0, 0, 2, 4, 4, 0, 2, 4, 0, 2 }
\end{verbatim}
%
Given the symbol table, the first document (index 0) has tokens 2
(\stringmention{bank}), 3 (\stringmention{loan}), 1
(\stringmention{money}), and so on, matching the input.  

It is this document-word matrix that is input to LDA.  

\subsection{LDA Parameters}

Before running LDA, we need to set up a number of parameters, which we
will explain after seeing the example run.  The first of these are the
model parameters,
%
\codeblock{SyntheticLdaExample.3}
%
First, we set up the number of topics, which must be fixed in advance.
Here we cheat and set it to 2, which we happen to know in this case is
the actual number of topics used to generate the documents.  In
general, we won't know the best value for the number of topics for a
particular application.

\subsubsection{Sampler Parameters}

Next, we set up the variables that control the sampler underlying
the LDA inference mechanism.
%
\codeblock{SyntheticLdaExample.4}
%
These are the number of samples we take, the number of samples thrown
away during the burnin phase, and the period between samples after
burnin (we explain these parameters in full detail below).  

The sampler also requires a pseudo-random number generator.  We use
Java's built-in pseudo-random number generator in the class
\code{Random} in \code{java.util} (see \refappendix{java-random} for
more information bout \code{Random} and using it in experimental
settings).  By providing the constructor an explicit seed, \code{43L},
the program will have the same behavior each time it is run.  Remove
the argument or provide a different seed to get different behavior.

\subsubsection{Reporting Callback Handler Parameters}

After that, we set up a handler that'll be used for reporting on the
progress of the sampler.  Specifically, we define a callback class
\code{ReportingLdaSampleHandler} (the code of which we describe
below), which will be passed to the inference method.
%
\codeblock{SyntheticLdaExample.5}
%
The callback handler will receive information about each sample
produced and provides the flexibility to perform online Bayesian
inference (which we explain below).  It's constructed with the
symbol table and takes an argument specifying the period between
reports (measured in samples).

\subsubsection{Invoking the Sampler}

At last, we are in a position to actually call the LDA inference
method, which is named after the algorithmic approach, Gibbs
sampling, which we'll also define below,
%
\codeblock{SyntheticLdaExample.6}
%
Basically, we pass it all of our parameters, including the
document-token matrix.

After the method returns, we have a sample in hand.  We use the
reporting methods in the handler to print out information about the
final sample.  It requires the final sample and parameters controlling
verbosity (printing at most 5 words per topic and 2 topics per
document, which here, are not actually constraints).
%
\codeblock{SyntheticLdaExample.7}


\section{Interpreting LDA Output}

We run the example using the Ant target \code{synthetic-lda} to
run the \code{main()} method in \code{SyntheticLdaExample}.  
a
%
\commandline{ant synthetic-lda}
%
We've already seen the output of the symbol table and the 
document-token matrix above.  

\subsection{Convergence Monitoring}

The next thing that's output is monitoring information from the
sampler.
%
\begin{verbatim}
n=     0 t=     :00 x-entropy-rate=  2.1553
n=     4 t=     :00 x-entropy-rate=  2.1528
...
n=    20 t=     :00 x-entropy-rate=  1.7837
n=    24 t=     :00 x-entropy-rate=  1.7787
...
n=   248 t=     :00 x-entropy-rate=  1.7811
n=   252 t=     :00 x-entropy-rate=  1.7946
\end{verbatim}
%
The results here are typical.  The report for the sample number.  Note
that we only get a report for every four samples because of how we
parameterized the handler.  We can report every epoch, but it's
relatively expensive.  The second column is the elapsed time.  This
example is so slow, it takes less than a second total.  The third
column is the cross-entropy rate.  This is roughly a measure of the
expected number of bits required to code a word if we were to use
the LDA model for compression.%
%
\footnote{This figure ignores the generation of the multinomial for
  the document. For use in compression, we would also need to encode a
  discrete approximation of the draw of the document's multinomial
  from the Dirichlet prior.}
%
Note that the cross-entropy starts high, at 2.1 bits, but quickly
drops to 1.8 bits.  Further note that at the converged state, LDA is
providing a better model than randomly generating words, which would
have a cross-entropy of $- \log_2 1/5 \approx 2.3$ bits.

The goal in running Gibbs samplers is to reach convergence, where
numbers like cross-entropy rates stabilize.  Note that Gibbs sampling
is not an optimization algorithm, so while the cross-entropy rates
start high and get lower, once they've reached about 1.78 in this
example, they start bouncing around.  

The number of samples used for burnin should be large enough that it
gets you to the bouncing around stage.  After that, results should be
relatively stable (see below for more discussion of LDA stability).

\subsection{Inferred Topics}

Next, we get a report of the parameters we've fit for the topic
models themselves.
%
\begin{verbatim}
TOPIC 0  (total count=104)
SYMBOL             WORD    COUNT   PROB          Z
--------------------------------------------------
     0           stream       42   0.404       4.2
     2             bank       35   0.336      -0.5
     4            river       27   0.260       3.3

TOPIC 1  (total count=152)
SYMBOL             WORD    COUNT   PROB          Z
--------------------------------------------------
     2             bank       60   0.395       0.5
     1            money       48   0.316       3.1
     3             loan       44   0.289       3.0
\end{verbatim}
%
We fixed the number of topics at 2.  LDA operates by assigning each
token to a topic (as we will see in a second).  These reports show the
most frequent words associated with each topic along with the symbol
table ID and the number of times each word was assigned to a topic.
At the top of the report is the total number of words assigned to that
topic (just a sum of the word counts).  

The probability column displays the estimated probabiltiy of a word in
a topic.  Thus if we sample a word from topic 0, it is 40.4\% likely
to be \stringmention{stream}, 33.6\% likely to be \stringmention{bank}
and 26.6\% likely to be \stringmention{river}.  Note that these
probabilities add up to 1.  Also note that these are not probabilities
of topics given words.  The probability of \stringmention{bank} in
topic 1 is only 39.5\% and thus it should be evident that these can't
be probabilities of topics given words, becuase they don't add up to 1
(33.6\% + 39.5\% = 78.1\% < 1).

These probabilities estimated from the small set of randomly generated
data are as close as could be expected to the actual probability
distributions.  With larger data sets, results will be tighter.

The order of the topics is not determined.  With a different random
seed or stopping after different numbers of samples, we will get
slightly different results.  One difference may be that the
identifiers on the topics will be switched.  This lack of
identifiability of the topic identifier is typical of clustering
models.  We return to its effect on Bayesian inference below.

The final column provides a z score for the word in the topic.  This
value measures how much more prevalent a word is in a topic than it is
in the corpus as a whole.  The units are standard deviations.  For
instance, the word \stringmention{bank} appears 95 times in the corpus
of 256 words, accounting for 95/256, or approximately 37\% of all
tokens.  In topic 0, it appears 35 times in 104 tokens, accounting for
approximately 34\% of all tokens; in topic 1, it appears 60 times in
152 words, accounting for roughly 39\% of the total number of words.
Because the prevalence of \stringmention{bank} in topic 0 is less than
in the corpus (34\% versus 37\%), its z score is negative; because
it's higher in topic 1 than in the corpus (39\% versus 37\%), the z
score is positive.   

\subsection{Inferred Token and Document Categorizations}

The final form of output is on a per-document basis (controlled
by a flag to the final output method).  This provides a report
on a document-by-document basis of how each word is assigned.
%
\begin{verbatim}
DOC 0
TOPIC    COUNT    PROB
----------------------
    1       16   0.994

bank(1) loan(1) money(1) loan(1) loan(1) money(1) bank(1) 
loan(1) bank(1) loan(1) loan(1) money(1) bank(1) money(1) 
money(1) money(1)
...
DOC 8
TOPIC    COUNT    PROB
----------------------
    1       11   0.685
    0        5   0.315

money(1) bank(1) money(1) loan(1) stream(0) bank(1) loan(1) 
bank(1) bank(1) stream(0) money(1) bank(0) bank(1) stream(0) 
river(0) money(1)
...
DOC 15
TOPIC    COUNT    PROB
----------------------
    0       16   0.994

stream(0) river(0) river(0) bank(0) stream(0) stream(0) 
stream(0) stream(0) bank(0) river(0) river(0) stream(0) bank(0) 
river(0) stream(0) bank(0)
\end{verbatim}
%
We've only shown the output from three representative documents.  For
each document, we show the topic assignment to each token in parens
after the token.  All the words in document 0 are assigned to topic 1
whereas all the word in document 15 are assigned to topic 0.  In
document 8, we see that 11 words are assigned topic 1 and 5 words
topic 0.  The word \stringmention{bank} is assigned to both topic 0
and topic 1 in the document.  This shows how LDA accounts for
documents that are about more than one topic.

The aggregate counts show how many words are assigned to each topic,
and the probability that a word from a document is chosen from the
probability.  

Even though all words were assigned to topic 1 in document 0, the
probability is still only 0.994.  This is because of the priors, which
induce smoothing, setting aside a small count for unseen topics in
each document.  This kind of smoothing is immensely helpful for
robustness in the face of noisy data, and almost all natural language
data is immensely noisy.


\section{LDA's Gibbs Samples}

The return value of the LDA estimation method is a Gibbs sample.

\subsection{Constant, Hyperparameter and Data Access}

The samples store all the argments supplied to the LDA method other
than number of samples information.  These values will be constant
for the chain of samples produced by the iterator.

The method \code{numTopics()} returns the constant number of topics
specified in the original invocation.  The methods
\code{topicWordPrior()} and \code{documentTopicPrior()} return the two
hyperparameters \ie{constant prior parameters}.

The sample object also provides access to the underlying document-word
matrix matrix through the methods \code{numDocuments()},
\code{documentLength(int)}, and \code{word(int,int)}.

\subsection{Word Topic Assignments and Probability Estimates}

What we're really after is the actual content of the sample.  Each
step of sampling contains an assignment of each token in each document
to one of the topics.  The method \code{topicSample(int,int)} returns
the \code{short}-valued topic identifier for the specified document
and token position.

These assignments of topics to words, combined with the priors'
hyperparameters, determine the probability estimates for topics and
documents we showed in the output in the previous section.  The method
\code{topicWordProb(int,int)} returns the \code{double}-valued
probability estimate of a word in a topic.  These probability
distributions fix the behavior of the topics themselves.

The method \code{documentTopicProb(int,int)} returns the
\code{double}-valued proability estimate for a topic in a document.
These probabilities summarize the topics assigned to the document.

These probability estimates, coupled with the underlying document-word
data matrix, allows us to compute \code{corpusLog2Probability()},
which is the probability of the topics and words given the models.
Note that it does not include the probabilities of the document
multinomials or the topic multinomials given their Dirichlet priors.
The main reason they're excluded is that the multinomials are not
sampled in the collapsed sampler.

The static utility method \code{dirichletLog2Prob(double[],double[])}
in the class \code{Statistics} in LingPipe's \code{stats} package may
be used to compute Dirichlet probabilities.  We may plug the maximum a
posteriori (MAP) estimates provided by the methods
\code{topicWordProb()} and \code{documentTopicProb()} into the
\code{dirichletLog2Prob()} method along with the relevant prior,
\code{topicWordPrior()} and \code{documentTopicPrior()}, respectively.



\subsection{Retrieving an LDA Model}

Each Gibbs sample contains a factory method \code{lda()} to construct
an instance of \code{LatentDirichletAllocation} based on the
topic-word distributions implied by the sample and the prior
hyperparameters.  

Although the Gibbs samples themselves are not serializable, the LDA
models they return are.  We turn to the use of an LDA instance in the
next section.
The method we used in the last section to invoke the sampler returned
a single Gibbs sample.  There is a related static estimation method in
\code{LatentDirichletAllocation} that returns an iterator over all of
the Gibbs samples.  The iterator and one-shot sampler return instances
of \code{GibbsSample}, which is a static class nested in
\code{LatentDirichletAllocation}.

Samples are produced in order by the sampler.  The poorly named method
\code{epoch()} returns the sample number, with numbering beginning
from 0.


\section{Handling Gibbs Samples}

In our running synthetic data example, we provided an instance of the
demo class \code{ReporingLdaSampleHandler} to the LDA estimation
method \code{gibbsSampler()}.  The use of the handler class follows
LingPipe's generic callback handler pattern.  For every
Gibbs sample produced by the sampler, the handler object receives
a reference to it through a callback.

\subsection{Samples are Reused}

Unlike many of our other callbacks, the Gibbs sampler callbacks
were implemented for efficiency.  Rather than getting a new
Gibbs sample object for each sample, the same instance of
\code{GibbsSample} is reused.  Thus it does not make any sense
to create a collection of the Gibbs samples for later use.  
All statistics must be computed online or accumulated outside
the Gibbs sample object for later use.

\subsection{Reporting Handler Demo}

The reporting handler demo is overloaded to serve two purposes.
First, it handles Gibbs samples as they arrive as required by
its interface.  Second, it provides a final report in a
class-specific method.

\subsubsection{Online Sample Handler}

The class declaration, member variables and constructor for the
handler are as follows.
%
\codeblock{ReportingLdaSampleHandler.1}
%
We maintain a reference to the symbol table so that our reports can
reassociate the integer identifiers with which LDA works with the
tokens from which they arose.  We see how the report period is used in
the implementation of the handler method.

The class was defined to implement the interface
\code{ObjectHandler<GibbsSample>}.  It thus requires an implementation
of the \code{void} return method \code{handle(GibbsSample)}.  Up to
the print statement, this method is implemented as follows.
%
\codeblock{ReportingLdaSampleHandler.2}
%
This is the callback method that will be called for every sample.  It
first grabs the epoch, or sample number.  If this is not an even
multiple of the report period, the method returns without performing
any action.  Otherwise, we fall through and calcualte the elapsed time
and cross-entropy rate.  Note that the cross-entropy rate as reported
here is the negative log (base 2) probability of the entire corpus of
documents divided by the number of tokens in the entire corpus.  
This cross-entropy rate is what we saw printed and what we monitor for
convergence.

\subsubsection{General Reporting}

There are additional reporting methods in the handler above the
required one that provide more detailed reporting for a sample.  They
produce the output shown in the previous section after the online
reporting of cross-entropy rates finishes and the final sample is
produced by LDA's \code{gibbsSampler()} method.

The general parameter reporting method, \code{reportParameters()},
just prints general corpus and estimator parameters from the sample.
There are two more interesting reporters, one for topics and one for
documents.  The topic reporting method, \code{reportTopics()},
begins as follows.
%
\codeblock{ReportingLdaSampleHandler.3}
%
The loop is over the topic identifiers.  Within the loop, the method
begins by assigning the number of words assigned to a topic.  It then
allocates a general LingPipe object counter to use for the word
counts.  Each word identifier is considered int he inner loop, and the
counter's value for that word is set to the number of times the word
was assigned to the current topic.  After collecting the counts in our
map, we create a list of the keys, here word indexes, ordered in
descending order of their counts.  

We continue by looping over the words by rank.
%
\codeblock{ReportingLdaSampleHandler.4}
%
We bound the number of words by the maximum given and make sure not to
overflow the model's size bounds.  Inside the loop, we just pull out
identifiers and counts, and calculate the z-score.  This is where we
need the symbol table in order to print out the actual words rather
than just their numbers.  The rest of the method is just printing.

The method used to calculate the z-score is
%
\codeblock{ReportingLdaSampleHandler.5}
%
It first calculates the maximum likelihood estimate of the word's
probabilty in the corpus, which is just the overall count of the word
divided by the number of words in the corpus.  The maximum likelihood
variance estimate is calculated as usual for a binomial outcome over
$n$ samples (here \code{wordsInCorpus}) with known probabiltiy
$\theta$ (here \code{pCorpus}), namely $n \times \theta \times (1 -
\theta)$.  The expected number of occurrences if it had the corpus
distribution is given by the number of words in the document times the
probability in the corpus.  The z-score is then the actual value minus
the expected value scaled by the expected deviation.

Returning to the general reporting, we have a method
\code{reportDocs()}, which provides a report on the documents,
optionally including a word-by-word report of topic assignments.
%
\codeblock{ReportingLdaSampleHandler.6}
%
This method enumerates all the documents in the corpus, reporting on
each one in turn.  For each topic, we follow the same pattern as we
did for the topics beofre, establishing a LingPipe object counter
mapping topic identifiers to the number of times a word in the current
document was assigned to that topic.  We then sort as before, then
loop over the ranks up to the maximum number of topics or total number
of topics in the model.
%
\codeblock{ReportingLdaSampleHandler.7}
%
For each rank, we calculate the counts and probabilities and display
them.

Finally, we report on a token-by-token basis if the flag is set.
%
\codeblock{ReportingLdaSampleHandler.8}
%
Here we just enumerate over the tokens in a document printing
the topic to which they were assigned.


\subsection{General Handlers}

In general, handlers can do anything.  For instance, they can be used
to accumulate results such as running averages, thus supporting full
Bayesian inference with a small memory footprint (only the current
sample need stay in memory).

For instance, the following handler would calculate average corpus
log 2 probability over all samples.
%
\codeblock{CorpusLog2ProbAvgHandler.1}
%
We use an instance of LingPipe's \code{OnlineNormalEstimator} for the
calculation.  It calculates averages and sample variances and standard
deviations online without accumulating the data points.  The method
\code{mAvg.mean()} returns the sample mean and
\code{mAvg.varianceUnbiased()} the unbiased sample variance.

Along these sample lines, more complex statistics such as word or
document similarities or held out data entropy estimates may be
computed online.


\section{Scalability of LDA}

LDA is a highly scalable model.  It takes an amount of time per token
that is proportional to the number of topics.  Thus with a fixed
number of topics, it scales linearly in topic size.  Tens of thousands
of documents are no problem on desktop machines.

\subsection{Wormbase Data}

In this section, we consider a larger scale LDA estimation problem, in
analyzing a collection of biomedical research paper citations related
to the model organism caenorhabditis elegans (c.~elegans), the
nematode worm.%
%
\footnote{\label{footnote:lda-wormbase-data} We downloaded the data,
  which consists of MEDLINE citations, from the Wormbase data
  repository,
  \url{ftp://ftp.wormbase.org/pub/wormbase/misc/literature/2007-12-01-wormbase-literature.endnote.gz},
  on 20 August 2010.}
%
Similar data from a slightly different source on a smaller scale
was analyzed by Blei et al.\ in an attempt to relate the topics
inferred using LDA with those added for genetics by the database
curators.%
%
\footnote{Blei et al.  analyzed a smaller set of worm citations from
  the Caenorhabditis Genetic Center (CGC) bibliography.  This data is
  available from the WormBook site at \url{http://www.wormbook.org/wli/cgcbib}.  
\begin{quote}
Blei, D.~M., K.~Franks, M.~I.~Jordan and
  I.~S.~Mian. 2006. Statistical modeling of biomedical corpora: mining
  the Caenorhabditis Genetic Center Bibliography for genes related to
  life span. {\it BMC Bioinformatics} {\bf 7}:250.
\end{quote}
Blei et al.\ considered 50 topics generated from 5225 documents using
a total of 28,971 distinct words.  }
%
An example of a citation in raw form is
%
\begin{verbatim}
%0 Journal Article
%T Determination of life-span in Caenorhabditis elegans 
   by four clock genes.
%A Lakowski, B
%A Hekimi, S
%D 1996
%V 272
%P 1010-1013
%J Science
%M WBPaper00002456
%X The nematode worm Caenorhabditis elegans is a model system 
   for the study of the genetic basis of aging. Maternal-effect
   mutations in four genes--clk-1, clk-2, clk-3, and gro-1
   --interact genetically to determine both the duration of 
   development and life-span. Analysis of the phenotypes of
   physiological clock in the worm. Mutations in certain genes 
   involved in dauer formation (an alternative larval stage
   induced by adverse conditions in which development is 
   arrested) can also extend life-span, but the life extension 
   of Clock mutants appears to be independent of these genes. 
   The daf-2(e1370) clk-1(e2519) worms, which carry life-span-
   extending mutations from two different pathways, live nearly 
   five times as long as wild-type worms.
\end{verbatim}
%
There are not actually line breaks in the \code{\%T} and \code{\%X} elements.

\subsection{Running the Demo}

First, you'll need to download the gzipped data from wormbase; see
\reffootnote{lda-wormbase-data} in this section for the URL.  It
doesn't matter where you put it; the program takes the location as
a parameter.

We created an Ant target \code{lda-worm} to run the demo.  It is
configured to provide seven command-line arguments as properties,
\code{wormbase.corpus.gz} for the location of the gzipped corpus,
\code{min.token.count} for the minimum token count in the corpus to
preserve the word in the model, \code{num.topics} for the number of
topics to generate, \code{topic.prior} and \code{word.prior} for the
priors, \code{random.seed} for the random seed, and \code{num.samples}
for the total number of samples to draw.
%
\commandline{ant -Dwormbase.corpus.gz=../../data-dist/2007-12-01-wormbase-literature.endnote.gz -Dmin.token.count=5 -Dnum.topics=50 -Dtopic.prior=0.1 -Dword.prior=0.001 -Drandom.seed=42 -Dnum.samples=500 lda-worm}
%
The output begins by reporting on the parameters and the corpus itself.
%
\begin{verbatim}
Citation file
     =..\..\data-dist\2007-12-01-wormbase-literature.endnote.gz
Minimum token count=5
Number of topics=50
Topic prior in docs=0.1
Word prior in topics=0.0010
Burnin epochs=0
Sample lag=1
Number of samples=200
#articles=26888 #chars=43343525
Number of unique words above count threshold=27882
Tokenized.  #Tokens After Pruning=3798050
\end{verbatim}
%
We see that there are 26,888 total citations, made up of around 43
million characters.  After tokenization, there are 27,882 tokens that
appeared at least 5 times (the minimum token count), and a total token
count of around 3.8 million instances.  That means each Gibbs sampling
step is going to reassign all 3.8 million tokens.

We configured the reporter to only print out each five epochs (not
in the input parameters, so the output after this looks as follows.
%
\begin{verbatim}

\end{verbatim}


\subsection{Code Walk Through}

The code is all in the class \code{LdaWorm}.  The main differences
from the synthetic demo include the parser for the compressed data and
a custom tokenizer for the Wormbase data.  

The method to read the corpus into an array of character sequences is
based on LingPipe's \code{FileLineReader} utility (see
\refsec{io-file-line-reader}).  It just scans the lines and
accumulates texts and adds them to an accumulating list.

The tokenization scheme is more interesting.  We create it with
the following method.
%
\codeblock{LdaWorm.1}
%
It creates a regex-based tokenizer that includes hyphens, letters and
numbers as parts of tokens.  We include hyphens and allow alphanumeric
mixtures to include tokens for gene or protein names such as
\charmention{p53} and \code{daf-19}, which would otherwise be
separated.  It also requires at least two characters.

It then immediately filters out tokens that are not at least two
characters long and don't contain at least one letter.  This filter
could've been rolled into the base regex, but these kinds of
exclusions quickly get clunky when mixed with alternations like in
our base tokenizer factory.  

We then convert the tokens to lower case and remove standard English
stop words.  We also remove custom stop words we discovered from
looking at the distribution of tokens in the corpus.  This list
contains domain-specific words like \stringmention{elegan} (remember
the ``plural'' stripping) and non-topical words from the biomedical
research domain such as \stringmention{however} and
\stringmention{although}.  We also remove the names of numbers, such
as \stringmention{eleven} and Roman numerals, such as \code{viii},
which are prevalent in this data.  Note that we have already lowercased
before stoplisting, so the stop list consists only of lowercase
entries.

The last filter is our own custom stemmer, which was designed to
follow the approach to removing English plural markers outlined in
Blei et al.'s paper.  Here's the code.
%
\codeblock{LdaWorm.2}
%
This filter fairly crudely strips off suffixes consisting of simple
English plural markers.  It will overstrip in the situation where a
word ends with one of these suffixes but is not plural.  It just
checks for suffixes and strips them off.  If the result is too short
or no longer contains a vowel, we return the original stem.  (The
restriction to ASCII is OK here because the documents were all
normalized to ASCII by the bibliography curators.)





\section{The Generative Model}

LDA is a kind of probabilistic model known as a generative model.
Generative models provide step-by-step characterizations of how to
generate a corpus of documents.%
%
\footnote{LDA does not model the number of documents in a corpus
or the number of tokens in each document.  These values must be
provided as constants.  This is usually not a problem because
the corpus is given as data.}
%
This setup seems strange to many people at first because we are in
practice presenting LDA with a collection of documents and asking it
to infer the topic model, not the other way around.  Nevertheless, the
probabilistic model is generative.

LDA generates each document independently based on the model
parameters.  To generate a document, LDA first generates a topic
distribution for that document.%
%
\footnote{The topic distribution for a document is generated from
a Dirichlet distribution, which is where the model gets its name.}
%
This happens before any of the words in a document are generated.  At
this stage, we might only know that a document is 70\% about politics
and 30\% about the economy, not what words it contains.

After generating the topic distribution for a document, we generate
the words for the document.  

In the case of LDA, we suppose that the number of documents and the
number of tokens in each document is given.  This means they're
not part of 

That means we don't try
to predict how long each document is or use document-length
information for relating documents.  Furthermore, LDA does not We also do not attempt to model
the size of the corpus.


\subsection{Generalized ``Documents''}

Although LDA is framed in terms of documents and words, it turns out
it only uses the identities of the tokens.  As such, it may be applied
to collections of ``documents'' consisting of any kind of count data,
not just bags of words.  For instance, LDA may be applied to RNA
expression data.

