\chapter{Character Language Models}\label{chap:char-lm}

A \techdef{language model} is a model that assigns probabilities to
strings.  Traditionally, these strings are either sequences of bytes,
sequences of characters, or sequences of tokens.  LingPipe provides
language model implementations for sequences of characters and
sequences of tokens.  In this chapter, we introduce the basic
interfaces and character language models.  In \refchap{token-lm},
we introduce token language models.

Although language models may be applied to arbitrary sequences, we are
going to focus on character sequences that make up natural language
text, This text may come from anywhere, including e-mail or instant
messages, blogs, newspapers, fiction novels, emergency room discharge
summaries, job applicant resume's, etc.

The basic operation of language models in LingPipe is to provide them
training data consisting of a sequence of texts.  After they are
trained, language models may be used to estimate the probability of
texts that were not in the training data.  

\section{Applications of Language Models}\label{section:char-lm-apps}

LingPipe uses character language models as the basis of several other
modules, including classification, taggers, chunkers, spelling
correction.  There are further applications for token language models
discussed in \refchap{token-lm}.  In all cases, these applications use
one or more language models to estimate different kinds of text.  For
instance, in language-model based classification, there is a language
model for each category, and texts are classified into the category
which assigns them the highest probability.  In chunkers, separate
language models might be constructed for person names, company names,
place names, and for text that's not part of a name; these are then
combined to analyze new text for names of entities.  For spelling
correction, a single language model characterizes the correct
spellings and is used to weigh alternative corrections against each
other.


One of the major applications of language models is in speech
recognition, where they play the same role as the language model in
spelling correction.  

Another application of language models is compression.  Using a
technique such as arithmetic coding, language models provide
state-of-the-art compression.  Although LingPipe does not implement an
arithmetic coder for compression, we will discuss the connection to
compression below when we discuss empirical cross-entropy.

Yet another application of language models is in characterizing
languages or sublanguages.  For instance, we can compare the entropy
of news stories in the {\it New York Times} with biomedical abstracts
in MEDLINE, the surprising result being that MEDLINE is more
predictable at the text level than the {\it Times}).  We can also
compare the entropy of Chinese and English, measuring the average
information content of a Chinese character (of which there are tens of
thousands) with the information content of an English character (of
which there are dozens).


\section{The Basics of $N$-Gram Language Models}

LingPipe's character- and token-based language models are based on
\techdef{$n$-grams}.  An $n$-gram is nothing more than a sequence of
$n$ symbols.  In this chapter, we consider character-based $n$-grams;
in \refchap{token-lm}, we consider tokens as symbols.

The basis of an $n$-gram model is storing counts for $n$-grams seen in
a training corpus of text.  These are then used to infer the
probability of a new sequence of symbols.  They key technical feature
of $n$-gram models is that they model the probability of a character
in a sequence based only on the previous $n-1$ characters.  Models
such as these with finite memories are called Markov chains.  

The chain rule allows us to model the probability of a sequence of
characters based on a model of a single character following a sequence
of characters before it.  For instance, with the character sequence
\stringmention{tooth} using 3-grams (trigrams), we decompose the
total probability of a sequence into the product of a sequence of
character probabilities, with
%
\begin{equation}
p(\stringmention{tooth})
= p(\stringmention{t})
\times
p(\stringmention{o}|\stringmention{t})
\times
p(\stringmention{o}|\stringmention{to})
\times
p(\stringmention{t}|\stringmention{oo})
\times
p(\stringmention{h}|\stringmention{ot}).
\end{equation}
%
To prevent the possibilty of underflow from multiplying long sequences
of numbers less than 1, we almost always work on the log scale.  This
also has the pleasant property of converting multiplication to
addition, yielding
%
\begin{equation}
\log p(\stringmention{tooth})
= \log p(\stringmention{t})
+
\log p(\stringmention{o}|\stringmention{t})
+ 
\log p(\stringmention{o}|\stringmention{to})
+
\log p(\stringmention{t}|\stringmention{oo})
+ 
\log p(\stringmention{h}|\stringmention{ot}).
\end{equation}


\section{Character-Level Language Models and Unicode}

LingPipe's character-based language models are models of sequences of
Java \code{char} values, which as you should recall from
\refsec{char-primitive}, represent UTF-16 byte pairs.  When LingPipe's
language models were coded, every Unicode code point could be coded in
a single UTF-16 byte pair.  As we discussed, this is enough to cover
Unicode's basic multilingual plane (BMP), which includes most
characters for most languages.%
%
\footnote{As of Unicode 4.1, there are characters with code points
  outside of the BMP.  To code such characters requires two 16-bit
  \code{char} values, known as a surrogate pair.  For the purposes of
  LingPipe's language models, characters are 16 bit UTF-16 values and
  a code point beyond the BMP is considered two characters.  What this
  will mean in practice is that LingPipe's language models will assign
  non-zero probability to sequences of \code{char} values that do not
  correspond to legal UTF-16 encodings, and thus don't correspond to
  Unicode strings.  This is internally consistent and does not affect
  most of the applications of language models.  The exception is
  untokenized spelling correction, where we have to be careful not to
  suggest corrections that are illegal sequences of \code{char}
  values.  In other cases, we just lose some modeling efficiency (in
  the statistical sense of efficiency, not the run-time sense).}




\section{Language Model Interfaces}\label{section:char-lm-interfaces}

There is a rich set of interfaces for language models and related
support classes in the LingPipe package \code{com.aliasi.lm}.  The
basic interfaces are in the \code{LanguageModel} interface, most
being defined as nested interfaces.

\subsection{Predictive: \code{LanguageModel}}

The top-level language model interface, \code{LanguageModel}, defines
methods for estimating probabilities of sequence of characters.  It
defines two methods for estimating the log (base 2) probability of a
sequence of characters, one based on arrays and one based on the
character sequence interface.
%
\begin{verbatim}
double log2Estimate(char[] cs, int start, int end);
double log2Estimate(CharSequence cs);
\end{verbatim}
%
The first method uses an indexed character array to define the slice
of characters from \code{cs[start]} to \code{cs[end-1]}.  For most
methods, the character slice method is more efficient and the
character-sequence method defined by first converting the sequence to
an array of characters.

The reason estimates are returned as logarithms is that we would
otherwise run into underflow.  If each character has an average
probabilty of 0.25 (roughly what they have in open-domain English text
like a newspaper), the probability of a sequence of 600 tokens has a
$2^{-1200}$ probabilty, which is too small to fit into a Java
double-precision floating point value (the minimum value for which is
$2^{-1074}$).

\subsection{Trainable: \code{LanguageModel.Dynamic}}

Language models that may be trained by supplying example text
implement the interface \code{LanguageModel.Dynamic}.  This interface
extends the corpus interface \code{ObjectHandler<CharSequence>},
which specifies the method
%
\begin{verbatim}
void handle(CharSequence cs);
\end{verbatim}
%
The character sequence is considered as training data and used to fit
the model parameters.  See \refsec{corpus-handlers} for more
information on \code{ObjectHandler} and how it fits into corpus
patterns.

There are four other methods named \code{train()}, the first two
of which simply allow training with character sequences.
%
\begin{verbatim}
void train(CharSequence cs);
void train(CharSequence cs, int count);
\end{verbatim}
%
The first method is a legacy method duplicating the functionality of
\code{handle()}.  The second method takes an integer count value which
determines the number of times the character sequence is used for
training.  Although implemented more efficiently, a training call
with a count is equivalent to calling the basic training method the
count number of times.

The last two methods just repeat the character sequence methods
with character slices.


\subsection{Conditional Probabilitiess: \code{LanguageModel.Conditional}}

Conditional language models predict the probability of a character
given the previous characters.  The interface
\code{LanguageModel.Conditional} specifies the method
%
\begin{verbatim}
double log2ConditionalEstimate(CharSequence);
\end{verbatim}
%
This method returns the probabilty of the last character in
the sequence given the initial characters in the sequence.  For
instance, \code{log2ConditionalEstimate("abcd")} returns the
probability of the character \charmention{d} following the
sequence of characters stringmention{abc}.  There is also
a method working on character slices.


\subsection{Bounded: \code{LanguageModel.Sequence}}

The interface \code{LanguageModel.Sequence} is just a marker
interface, meaning that it does not specify any new methods other than
that inherited from its superintervace, \code{LanguageModel}.

Sequence-based language model implementations are normalized so
that the sum of the probability of all strings of all lengths is one.%
%
\footnote{Because it's just a marker interface, the requirement is
  conventional rather than expressed in Java's type language.  This is
  similar to interface requirements on collections in the
  \code{java.util} core library.}
%
In symbols, this amounts to
%
\begin{equation}
\sum_{n \geq 0} \hspace*{8pt} \sum_{\text{\code{cs.length()} == n}} \hspace*{3pt} 2^{\text{\code{log2Prob(cs)}}} = 1.
\end{equation}

\subsection{Unbounded: \code{LanguageModel.Process}}

The other type of language model is a process language model, declared
by implementing the marker interface \code{LanguageModel.Process}.  A
process language model treats a sequence of characters as being
generated as a kind of random process that generates a character at a
time without a distinct beginning or end.  

Because they don't model the beginning or end of a string, process
language models are normalized by length so that the sum of the
probabilities of all strings of a given length is one.  In symbols,
for every \code{n}, we have
%
\begin{equation}
\sum_{\text{\code{cs.length()} == n}} \hspace*{3pt} 2^{\text{\code{log2Prob(cs)}}} = 1.
\end{equation}
%


\section{Process Character Language Models}

As we saw in the last section, language models have a particularly
simple interface.  We just give them text to train and then they
can model new text. 

The simplest language model implementation supplied by LingPipe is
\code{NGramProcessLM}, in package \code{com.aliasi.lm}.  This class
implements several interfaces, including several of the basic language
model interfaces, \code{LanguageModel}, the unbounded interface,
\code{LanguageModel.Process}, the trainable interface
\code{LanguageModel.Dynamic}, and conditional prediction interface
\code{LanguageModel.Conditional} (see \refsec{} for descriptions
of these interfaces).

In addition to the LM interfaces, the \code{NGramProcessLM} class also
implements \code{ObjectHandler<CharSequence>} (see
\refsec{corpus-handlers} for a description of the handler interface).
This allows language models to be integrated into the general
parser/handler/corpus interfaces (see \refchap{corpus} for more
details).

Furthermore, process character LMs are both serializable (see
\refsec{io-object-data-io}) and compilable (see
\refsec{io-compilable}).  As we will see later in this section,
compilation produces a very efficient, but no longer dynamic
version of the language model, whereas serialization will allow
it to be written out and read back in.


\subsection{Basic LM Training and Inference Demo}

Perhaps the simplest way to get a feel for how language models work is
to code one up.  We provide an example in the class
\code{ProcessLmDemo} in this chapter's package,
\code{com.lingpipe.book.charlm}.  

\subsubsection{Code Walkthrough}

As with all of our demos, it's set up as a simple \code{main()} method
that reads its arguments from the command line.  Specifically, we need
an n-gram length (\code{ngram}), a text on which to train
(\code{textTrain}) and a text on which to test (\code{textTest}),
which are supplied as the first three arguments.  The training and
evaluation are then almost trivial.
%
\codeblock{ProcessLmDemo.1}
%
We start by creating a new instance of an \code{NGramProcessLM}
supplying the n-gram length to the constructor.  We then take the
training text string and pass it to the language model's
\code{handle(CharSequence)} method, which adds the text to the
training data.  Finally, we compute the log (base 2) of the
probaiblity of the test text using the language model's
\code{log2Estimate(CharSequence)} method.  

\subsubsection{Running the Demo}

The Ant target \code{process-demo} runs the command, using
the property \code{ngram} for $n$-gram length, the property
\code{text.train} for the training text, and the property
\code{text.test} for the test text.  It is thus run as
follows.
%
\commandlinefollow{ant -Dngram=5 -Dtext.train="abracadabra" -Dtext.test="candelabra" process-demo}
\begin{verbatim}
ngram=5    train=|abracadabra|    test=|candelabra|
log2 p(test|train)=-69.693
\end{verbatim}
%
The command-line arguments are first printed (with vertical bars to
denote the boundaries of the strings) and then the log probability of
the test sequence given the training data is shown.

One way of thinking of language models is as measuring a kind
of text similarity.  The more similar the test text is to the
training text, the higher its probability.  For instance, consider

If the text is very predictable, the log2 probability will be lower
(larger here means smaller absolute value, because logarithms of
values in $[0,1]$ are always negative).  For instance, consider
the following case.
%
\commandlinefollow{ant -Dngram=2 -Dtext.train="ababababab" -Dtext.test="ababab" process-demo}
\begin{verbatim}
ngram=2    train=|ababababab|    test=|ababab|
log2 p(test|train)=-3.060
\end{verbatim}

In general, shorter texts are more predictable.  For instance,
consider the same training text as above with a test text
that's a subsequence of the test text above:
%
\commandlinefollow{ant -Dngram=2 -Dtext.train="ababababab" -Dtext.test="abab" process-demo}
\begin{verbatim}
ngram=2    train=|ababababab|     test=|abab|
log2 p(test|train)=-2.419
\end{verbatim}
%
Note that -2.419 is greater than -3.060.  In linear probability terms,
that's $2^{-2.419} = 0.225$, whereas $2^{-3.060} = 0.120$.  In other
words, the model, after training, says that \stringmention{abab} is
almost twice as likely as \stringmention{ababab}.

For process language models, which don't model boundaries, substrings
always have higher (log) probability.  This will not necessarily be
the case for the boundary language models we consider in the next section.

A language model will assign probabilities to any texts of any length.
The characters don't need to have been seen in the training data.  For
instance, consider our first example above, where the characters
\charmention{e}, \charmention{n}, and \charmention{l} were new to the
training data.  Characters that were not seen in the training data tend
to have fairly low probabilities.  For instance, consider
%
\commandlinefollow{ant -Dngram=2 -Dtext.train="ababababab" -Dtext.test="xyxy" process-demo}
\begin{verbatim}
ngram=2    train=|ababababab|    test=|xyxy|
log2 p(test|train)=-71.229
\end{verbatim}
%
Again, it's clear why we use the log scale --- $2^-{71}$ is a very low
probability indeed.

The $n$-gram length is the most important of the tuning parameters,
the rest of which we turn to in the next section.  A longer $n$-gram
will provide a tighter model of the training text than shorter
$n$-grams.  The longer the $n$-gram, the higher the probability
assigned to the training text itself as well as to very similar text
(again measuring by $n$-gram frequency).  For instance, consider

\commandlinefollow{ant -Dngram=2 -Dtext.train="abcdef" -Dtext.test="abcdef" process-demo}
\begin{verbatim}
ngram=2    train=|abcdef|    test=|abcdef|
log2 p(test|train)=-11.334
\end{verbatim}

and

\commandlinefollow{ant -Dngram=3 -Dtext.train="abcdef" -Dtext.test="abcdef" process-demo}
\begin{verbatim}
ngram=3    train=|abcdef|    test=|abcdef|
log2 p(test|train)=-10.884
\end{verbatim}

The improvement is relatively small.  If we had repeated the training
text many times or if it had been longer, the effect would be
stronger.  Here, if you actually go up to 4-grams, the probability
estimate gets worse.  The reason for this is the default smoothing
parameters, which we discuss in \refsec{char-lm-smoothing}.

One thing to beware of is that increasing the $n$-gram size tends to
have a dramatic impact on the amount of memory required.  This is
apparent when you consider that for basic English, there are only a
few dozen characters and thus only a few dozen unigrams (1-grams).
But there are thousands of bigrams (2-grams), and millions of
5-grams and 6-grams.  Even with 40 characters, there are over
2.5 million potential 4-grams.  Luckily, text is fairly predictable,
so the set of $n$-grams actually observed tends to grow more
slowly than this worst case may lead you to believe.  


\section{Additive Smoothing}\label{section:char-lm-smoothing}












