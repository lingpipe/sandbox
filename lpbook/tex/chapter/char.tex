\chapter{Numbers and Characters}\label{chap:char}

\firstchar{I}n this chapter, we show first how bytes may be used
to encode characters and second how characters and strings are
represented in Java.

\section{Numbers}

In this section, we explain different numerical bases, including
decimal, octal, hexadecimal and binary.  

\subsection{Digits and Decimal Numbers}

Typically we write numbers in the Arabic form (as opposed to, say, the
Roman form), using decimal notation.  That is, we employ sequences of the
ten digits 0, 1, 2, 3, 4, 5, 6, 7, 8, 9.  

A number such as 23 may be decomposed into a 2 in the ``tens place''
and a 3 in the ``ones place''.  What that means is that $23 = (2 \times
10) + (3 \times 1)$; similarly $4700 = (4 \times 1000) + (7 \times
100)$.  We can write these equivalently as $23 = 2 \times 10^1 + 3 \times 10^0$
and $4700 = (4 \times 10^3) + (7 \times 10^2)$.  Because of the base
of the exponent, decimal notation is also called ``base 10''.

The number 0 is special.  It's the additive identity, meaning
that for any number $x$, $0 + x = x + 0 = x$.  

We also conventionally use negative numbers and negation.  For
instance, -22 is read as ``negative 22''.  We have to add 22 to it to
get back to zero.  That is, negation is the additive inverse, so that
$x + (-x) = (-x) + x = 0$.

The number 1 is also special.  1 is the multiplicative identity,
so that $1 \times x = x \times 1 = x$.  Division is multiplicative
inverse, so that for all numbers $x$ other than 0, $\frac{x}{x} = 1$.

We also use fractions, written after the decimal place.  Fractions are
defined using negative exponents.  For instance $.2 = 2 \times 10^{-1}
= \frac{2}{10^1}$, and $.85 = .8 \times 10^{-1} + .5 \times 10^{-2}$.

For really large or really small numbers, we use scientific notation,
which decomposes a value into a number times an exponent of 10.  For
instance, we write 4700 as $4.7 \times 10^3$ and 0.0047 as $4.7 \times
10^{-3}$.  In computer languages, 4700 and 0.0047 are typically
written as \code{4.7E+3} and \code{4.7E-3}.  Java's floating point
numbers may be input and output in scientific notation.

\subsection{Bits and Binary Numbers}

Rather than the decimal system, computers work in the binary system,
where there are only two values.  In binary arithmetic, bits play the
role that digits play in the decimal system.  A \techdef{bit} can have
the value 0 or 1.  

A number in \techdefs{binary notation}{binary} consists of a sequence
of bits (0s and 1s).  Bits in binary binary numbers play the same role
as digits in decimal numbers; the only difference is that the base is
2 rather than 10.  For instance, in binary, $101 = (1 \times 2^3) + (0
\times 2^2) + (1 \times 2^0)$, which is 7 in decimal notation.  Fractions
can be handled the same way as in decimal numbers.  Scientific notation
is not generally used for binary numbers.

\subsection{Octal Notation}

Two other schemes for representing numbers are common in computer
languages, octal and hexadecimal.  As may be gathered from its name,
\techdef{octal}{octal} is base 8, conventionally written using the
digits 0--7.  Numbers are read in the usual way, so that octal 43 is
expanded as $(4 \times 8^1) + (3 \times 8^0)$, or 35 in decimal
notation.

In Java (and many other computer languages), octal
notation\index{octal!notation} is very confusing.  Prefixing a numeric
literal with a \code{0} (that's a zero, not a capital o) leads to it
being interpreted as octal.  For instance, the Java literal \code{043}
is interpreted as the decimal 35.

\subsection{Hexadecimal Notation}

\techdefs{Hexadecimal}{hexadecimal} is base 16, 
and thus we need some additional symbols.  The first 16 numbers in hex
are conventionally written

\displ{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A, B, C, D, E, F.}

In hexadecimal, the value of A is what we'd write as 10 in decimal
notation.  Similarly, C has the value 12 and F the value 15.  We read
off compound numbers in the usual way, so that in hex, $93 = (9 \times
16^1) + (3 \times 16^0)$, or 138 in decimal notation.  Similarly, in
hex, the number $\mbox{\rm B2F} = (11 \times 16^2) + (2 \times 16^1) + (15 \times
16^0)$, or 2863 in decimal notation.

In Java (and other languages), hexadecimal
numbers\index{hexadecimal!notation} are distinguished by prefixing
them with \code{0x}.  For example,
\code{0xB2F} is the hexadecimal equivalent of the decimal 2863.

\subsection{Bytes}

The basic unit of organization in contemporary computing systems is
the byte.  By this we mean it's the smallest chunk of data that may be
accessed programatically, though in point of fact, hardware often
groups bytes together into larger groups that it operates on all at
once.  For instance, 32-bit architectures often operate on a sequence
of 4 bytes at once and 64-bit architectures on 8 bytes at once.  The
term ``word'' is ambiguously applied to a sequence of two bytes, or to
the size of the sequence of bytes at which a particular piece of
hardware operates.

A \techdef{byte}{byte} is a sequence of 8 bits, and is sometimes
called an octet for that reason.  Thus there are 256 distinct bytes,
ranging from \code{00000000} to \code{11111111}.  The bits are read
from the high (left) end to the low (right end).

In Java, the \code{byte} primitive type\index{Java!primitive!byte} is
signed.  Any number between \code{00000000} and \code{01111111} is
interpreted as a binary number with decimal value between 0 and 127
(inclusive).  If the high order bit is 1, the value is interpreted as
negative.  The negative value is value of the least significant 7 bits
minus 128.  For instance,
\code{10000011} is interpreted as $3 - 128 = -125$, because
\code{00000011} (the result of setting the high bit to \code{0}) is interpreted as 3.

The unsigned value of a byte \code{b} is returned by \code{(b~<~0 ?\
(b~+~256) :\ b)}.  

The primitive data type for computers is a sequence of bytes.  For
instance, the contents of a file is a sequence of bytes, and so
is the response from a web server to an HTTP request for a web page.
Most importantly for using LingPipe, sequences of characters are
represented by sequences of bytes.

\subsection{Code Example: Integral Number Bases}

There is a simple program in
%
\displ{\pathin{src/chars/src/com/lingpipe/book/chars/ByteTable.java}}
%
for displaying bytes, their corresponding unsigned value, and the
conversion of the unsigned value to octal, hexadecimal and binary
notations.  The work is done by the loop
%
\codeblock{ByteTable.1}
%
This code may be run from Ant by first changing into this chapter's
directory and then invoking the \code{byte-table} target,
%
\commandlinefollow{cd \rootdir/src/chars}%
\commandline{ant byte-table}
%
The output, after trimming the filler generated by Ant, looks like
%
\begin{verbatim}
BASES
 10    -10    8  16         2

  0      0    0   0         0
  1      1    1   1         1
  2      2    2   2        10
...
  9      9   11   9      1001
 10     10   12   a      1010
 11     11   13   b      1011
...
127    127  177  7f   1111111
128   -128  200  80  10000000
129   -127  201  81  10000001
...
254     -2  376  fe  11111110
255     -1  377  ff  11111111
\end{verbatim}

\subsection{Other Primitive Numbers}

In addition to bytes, Java provides three other primitive integer
types.  Each has a fixed width in bytes.  Values of type \code{short}
occupy 2 bytes, \code{int} 4 bytes, and \code{long} 8 bytes.  All of
them use the same signed notation as \code{byte}, only with more bits.

\subsection{Floating Point}\index{arithmetic!floating point}

Java has two floating point types, \code{float} and \code{double}.  A
\code{float} is represented with 4 bytes and said to be single
precision, whereas a \code{double} is represented with 8 bytes and
said to be double precision.

In addition to finite values, Java follows the IEEE~754 floating point
standard in providing three additional values.  There is positive
infinity, conventionally $\infty$ in mathematical notation, and
referenced for floating point values by the static constant
\code{Float.POSITIVE\_INFINITY} in Java.  There is also negative
infinity, $-\infty$, referenced by the constant
\code{Float.NEGATIVE\_INFINITY}.  There is also an ``undefined''
value, picked out by the constant \code{Float.NaN}.  There are
corresponding constants in \code{Double} with the same names.

If any value in an expression is \code{NaN}, the result is \code{NaN}.
A \code{NaN} result is also returned if you try to divide 0 by 0,
subtract an infinite number from itself (or equivalently add a positive
and negative infinite number), divide one infinite number
by another, or multiple an infinite number by 0.

Both \code{n/Double.POSITIVE\_INFINITY} and
\code{n/Double.NEGATIVE\_INFINITY} evaluate to 0 if \code{n} is finite
and non-negative.  Conversely, \code{n/0} evaluates to $\infty$ if
\code{n} is positive and $-\infty$ if \code{n} is negative.
The result of multiplying two infinite number is $\infty$ if they are
both positive or both negative, and $-\infty$ otherwise.  If
\code{Double.POSITIVE\_INFINITY} is added to itself, the result is
itself, and the same for \code{Double.NEGATIVE\_INFINITY}.  If one is
added to the other, the result is \code{NaN}.  The negation of an
infinite number is the infinite number of opposite sign.  

Monotonic transcendental operations like exponentiation and logarithms
also play nicely with infinite numbers.  In particular, the log of a
negative number is \code{NaN}, the log of 0 is negative infinity, and
the log of positive infinity is positive infinity.  The exponent of
positive infinity is positive infinity and the exponent of negative
infinity is zero.

\subsection{Number Objects}

For each of the primitive number types, there is a corrsponding class
for representing the object.  Specifically, the number classes for
integers are \code{Byte}, \code{Short}, \code{Integer}, and
\code{Long}.  For floating point, there is \code{Float} and
\code{Double}.  These objects are all in the base package
\code{java.lang}, so they do not need to be explicitly imported before
they are used.  Objects are useful because many of the underlying Java
libraries, particular the collection framework in \code{java.util},
operate over objects, rather than over primitives.%
%
\footnote{Open source libraries are available for primitive collections.
The Jakarta Commons Primitives and Carrot Search's
High Performance Primitive Collections are released under the Apache license,
and GNU Trove under the Lessger GNU Public License (LGPL).}

Each of these wrapper classes holds an immutable reference to an
object of the underlying type.  Thus the class is said to ``box'' the
underlying primitive (as in ``put in a box'').  Each object has a
corresponding method to return the underlying object.  For instance,
to get the underlying byte from a \code{Byte} object, use
\code{byteValue()}.

Two numerical objects are equal if and only
if they are of the same type and refer to the same primitive value.
Each of the numerical object classes is both serializable and
comparable, with comparison being defined numerically.


\subsubsection{Constructing Numerical Objects}

Number objects may be created using one-argument constructors.  For
instance, we can construct an object for the integer 42 using
\code{new~Integer(42)}.  

The problem with construction is that a fresh object is allocated for
each call to \code{new}.  Because the underlying references are
immutable, we only need a single instance of each object for each
underlying primitive value.  The current JVM implementations are smart
enough to save and reuse numerical objects rather than creating new
ones.  The preferred way to acquire a reference to a numerical object
is to use the \code{valueOf()} static factory method from the
approrpiate class.  For example, \code{Integer.valueOf(42)} returns an
object that is equal to the result of \code{new~Integer(42)}, but is
not guaranteed to be a fresh instance, and thus may be reference equal
(\code{==}) to other \code{Integer} objects.

\subsubsection{Autoboxing}

Autoboxing automatically provides an object wrapper for a primitive
type when a primitive expression is used where an object is expected.
For instance, it is legal to write
%
\begin{verbatim}
Integer i = 7;
\end{verbatim}
%
Here we require an \code{Integer} object to assign, but the expression
\code{7} returns an \code{int} primitive.  The compiler automatically
boxes the primitive, rendering the above code equivalent to
%
\begin{verbatim}
Integer i = Integer.valueOf(7);
\end{verbatim}

Autoboxing also applies in other contexts, such as loops, where
we may write
%
\begin{verbatim}
int[] ns = new int[] { 1, 2, 3 };
for (Integer n : ns) { ... }
\end{verbatim}
%
Each member of the array visited in the loop body is autoboxed.
Boxing is relatively expensive in time and memory, and should be
avoided where possible.

The Java compiler also carries out auto-unboxing, which allows things like
%
\begin{verbatim}
int n = Integer.valueOf(15);
\end{verbatim}
%
This is equivalent to using the \code{intValue()} method,
%
\begin{verbatim}
Integer nI = Integer.valueOf(15);
int n = nI.intValue();
\end{verbatim}


\subsubsection{Number Base Class}

Conveniently, all of the numerical classes extend the abstract class
\code{Number}.  The class \code{Number} simply defines all of the
get-value methods, \code{byteValue()}, \code{shortValue()}, ...,
\code{doubleValue()}, returning the corresponding primitive type.
Thus we can call \code{doubleValue()} on an \code{Integer} object; the
return value is equivalent to casting the \code{int} returned by
\code{intValue()} to a \code{double}.


\section{Character Encodings}

For processing natural language text, we are primarily concerned with
the representation of characters in natural languages.  A set of
(abstract) characters is known as a character set.  These range from
the relatively limited set of 26 characters (52 if you count upper and
lower case characters) used in English to the tens of thousands of
characters used in Chinese.


\subsection{What is a Character?}

Precise definitions of characters in human writing systems is a
notoriously tricky business.  

\subsubsection{Types of Character Sets}

Characters are used in language to represent sounds at the level of
phonemic segments \eg{Latin}, syllables \eg{Japanese Hirigana and
Katakana scripts, Linear B, and Cherokee}, or words or morphemes, also
known as logograms \eg{Chinese Han characters, Egyptian
hieroglyphics}.  

These distinctions are blurry.  For instance, both Chinese characters
and Egyptian hieroglyphs are typically pronounced as single syllables
and words may require multiple characters.

\subsubsection{Abstraction over Visualization}

One problem with dealing with characters is the issue of visual
representation versus the abstraction.  Character sets abstract away
from the visual representations of characters.  

A sequence of characters is also an abstract concept.  When
visualizing a sequence of characters, different writing systems lay
them out differently on the page.  When visualizing a sequence of
characters, we must make a choice in how to lay them out on a page.
English, for instance, traditionally lays out a sequence of characters
in left to right order horizontally across a page. 
Arabic, on the other hand, lays out a sequence of
characters on a page horizontally from right to left.  In contast,
Japanese was traditionally set from top to bottom with the following
character being under the preceding characters.

English stacks lines of text from top to bottom, whereas traditional
Japanese orders its columns of text from right to left.  

When combining multiple pages, English has pages that ``turn'' from
right to left, whereas Arabic and traditional Japanese conventionally
order their pages in the opposite direction.  Obviously, page order
isn't relevant for digital documents rendered a page at a time, such
as on e-book readers.

Even with conventional layouts, it is not uncommon to see caligraphic
writing or signage laid out in different directions.  It's not
uncommon to see English written from top to bottom or diagonally from
upper left to lower right.


\subsubsection{Compound versus Primitive Characters}

Characters may be simple or compound.  For instance, the character
'\"o' used in German is composed of a plain Latin 'o' character with
an umlaut diacritic on top.  Diacritics are visual representations
added to other characters, and there is a broad range of them in
European languages.  

Hebrew and Arabic are typically written without vowels.  In very
formal writing, vowels may be indicated in these languages with
diacritics on the consonants.  Devanagari, a script used to write
languages including Hindi and Nepali, includes pitch accent marks.

The Hangul script used for Korean is a compound phonemic system often
involving multiple glyphs overlaid to form syllabic units.


\subsection{Coded Sets and Encoding Schemes}

Eventually, we need to represent sequences of characters as sequences
of bytes.  There are two components to a full character encoding.

The first step is to define a coded character set, which assigns each
character in the set a unique non-negative integer code point.  For
instance, the English character 'A' (capital A) might be assigned the
code point 65 and 'B' to code point 66, and so on.  Code points are
conventionally written using hexadecimal notation, so we would
typically use the hexadecmial notation 0x41 instead of the decimal
notation 65 for capital A.

The second step is to define a character encoding scheme that maps
each code point to a sequence of bytes (bytes are often called octets
in this context).

Translating a sequence of characters in the character set to a
sequence of bytes thus consists of first translating to the sequence
of code points, then encoding these code points into a sequence of
bytes.


\subsection{Legacy Character Encodings}

We consider in this section some widely used character encodings, but
because there are literally hundreds of them in use, we can't be
exhaustive.  In the next section we consider Unicode.

\subsubsection{ASCII}

ASCII is a character encoding consisting of a small character set of
128 code points.  Being designed by computer scientists, the numbering
starts at 0, so the code points are in the range 0--127 (inclusive).
Each code point is then encoded as the corresponding single unsigned
byte with the same value.  It includes characters for the standard
keys of an American typewriter, and also some ``characters''
representing abstract typesetting notions such as line feeds and
carriage returns as well as terminal controls like ringing the
``bell''.

The term ``ASCII'' is sometimes used informally to refer to character
data in general, rather than the specific ASCII character encoding.

\subsubsection{Latin1 and the Other Latins}

Given that ASCII only uses 7 bits and bytes are 8 bits, the natural
extension is to add another 128 characters to ASCII.  Latin1
(officially named ISO-8859-1) did just this, adding common accented
European characters to ASCII, as well as punctuation like upside down
exclaimation points and question marks, French quote symbols, section
symbols, a copyright symbol, and so on.  

There are 256 code points in Latin1, and the encoding just uses the
corresponding unsigned byte to encode each of the 256 characters
numbered 0--255.

The Latin1 character set includes the entire ASCII character set.
Conveniently, Latin1 uses the same code points as ASCII for the ASCII
characters (code points 0--127).  This means that every ASCII encoding
is also a Latin1 encoding of exactly the same characters.

Latin1 also introduced ambiguity in coding, containing both an 'a'
character, the '\"{}' umlaut character, and their combination '\"{a}'.
It also includes all of 'a', 'e', and the compound '\ae', as well as
'1', '/', '2' and the compound $\frac{1}{2}$.

With only 256 characters, Latin1 couldn't render all of the characters
in use in Indo-European languages, so a slew of similar standards,
such as Latin2 (ISO-8859-2) for (some) Eastern European languages,
Latin3 (ISO-8859-1) for Turkish, Maltese and Esperanto, up through
Latin16 (ISO-8859-16) for other Eastern European languages, new styles
of Western European languages and Irish Gaelic.  

\subsubsection{Windows 1252}

Microsoft Windows, insisting on doing things its own way, uses a
character set known as Windows-1252, which is almost identical to Latin1,
but uses code points 128--159 as graphical characters, such as curly
quotes, the euro and yen symbols, and so on.

\subsubsection{Big5 and GB(K)}

The Big5 character encoding is used for traditional Chinese scripts
and the GB (more recently GBK) for simplified Chinese script.  Because
there are so many characters in Chinese, both were designed to use a
fixed-width two-byte encoding of each character.  Using two bytes
allows up to $2^{16} = 65,536$ characters, which is enough for most
of the commonly used Chinese characters.

\subsection{Unicode}

Unicode is the de facto standard for character sets of all kinds.  The
latest version is Unicode 5.2, and it contains over 1,000,000 distinct
characters drawn from hundreds of languages (technically, it assumes
all code points are in the range 0x0 to 0x10FFFF (0 to 1,114,111 in
decimal notation).

One reason Unicode has been so widely adopted is that it contains
almost all of the characters in almost all of the widely used
character sets in the world.  It also happens to have done so in a
thoughful and well-defined manner.

Conveniently, Unicode's first 256 code points (0--255) exactly match
those of Latin1, and hence Unicode's first 128 code points (0--127)
exactly match those of ASCII.

Unicode code points are conventionally displayed in text using a
hexadecimal representation of their code point padded to at least four
digits with initial zeroes, prefixed by \code{U+}.  For instance, code
point 65 (hex 0x41), which represents capital A in ASCII, Latin1 and
Unicode, is conventionally written \unicode{0041} for Unicode.

The problem with having over a million characters is that it would
require three bytes to store each character ($2^{24} = 16,777,216$).
This is very wasteful for encoding languages based on Latin characters
like Spanish or English.

\subsubsection{Unicode Transformation Formats}

There are three standard encodings for Unicode code points that are
specified in the Unicode standard, UTF-8, UTF-16, and UTF-32 (``UTF''
stands for ``Unicode transformation format'').%
%
\footnote{There are also non-standard encodings of (subsets of) 
Unicode, like Apache Lucene's and the one in Java's
\code{DataOutput.writeUTF()} method.}
%
The numbers represent the coding size; UTF-8 uses single bytes,
UTF-16 pairs of bytes, and UTF-32 quadruples of bytes.

\subsubsection{UTF-32}

The UTF-32 encoding is fixed-width, meaning each character occupies
the same number of bytes (in this case, 4).  As with ASCII and
Latin1, the code points are encoded directly as bits in base 2.

There are actually two UTF-32 encodings, UTF-32BE and UTF-32LE,
depending on the order of the four bytes making up the 32 bit blocks.
In the big-endian (BE) scheme, bytes are ordered from left to right
from most significant to least significant digits (like for bits in a
byte).  

In the little-endian (LE) scheme, they are ordered in the
opposite direction.  For instance, in UTF-32BE, \unicode{0041}
(capital A), is encoded as 0x00000041, indicating the four byte
sequence 0x00, 0x00, 0x00, 0x41.  In UTF32-LE, it's encoded as
0x41000000, corresponding to the byte sequence 0x41, 0x00, 0x00, 0x00.

There is also an unmarked encoding scheme, UTF-32, which tries to
infer which order the bytes come in.  Due to restrictions on the
coding scheme, this is usually possible.  In the simplest case,
0x41000000 is only legal in little endian, because it'd be out of
range in big-endian notation.  

Different computer processors use different endian-ness
internally.  For example, Intel and AMD x86 architecture is little
endian, Motorola's PowerPC is big endian.  Some hardware, such
as Sun Sparc, Intel Itanium and the ARM, is called bi-endian, because
endianness may be switched.

\subsubsection{UTF-16}

The UTF-16 encoding is variable length, meaning that different code
points use different numbers of bytes.  For UTF-16, each character is
represented using either two bytes (16 bits) or four bytes (32 bits).

Because there are two bytes involved, their order must be defined.  As
with UTF-32, UTF-16BE is the big-endian order and UTF-16LE the
little-endian order.

In UTF-16, code points below \unicode{10000} are represented using a
single byte in the natural base-2 encodings.  For instance, our old
friend \unicode{0041} (capital A) is represented in big endian as
the sequence of bytes 0x00, 0x41.  

Code points at or above \unicode{10000} are represented using four
bytes arranged in two pairs.  Calculating the two pairs of bytes
involves bit-twiddling.  For instance, given a code point
\code{codepoint}, the following code
calculates the four bytes.
%
\footnote{This code example and the following one for decoding were
adapted from the example code supplied by the Unicode Consortium in
their FAQ at 
\urldisplay{http://unicode.org/faq/utf\_bom.html}
}
%
\begin{verbatim}
static final int LEAD_OFFSET = 0xD800 - (0x10000 >> 10);
static final int SURROGATE_OFFSET = 0x10000 - (0xD800 << 10) - 0xDC00;

int lead = LEAD_OFFSET + (codepoint >> 10);
int trail = 0xDC00 + (codepoint & 0x3FF);

int byte1 = lead >>> 8;
int byte2 = lead & 0xFF;
int byte3 = trail >>> 8;
int byte4 = trail & 0xFF;
\end{verbatim}
%
Going the other way around, given the four bytes, we
can calculate the code point with the following code.
%
\begin{verbatim}
int lead = byte1 << 8 | byte2;
int trail = byte3 << 8 | byte4;

int codepoint = (lead << 10) + trail + SURROGATE_OFFSET;
\end{verbatim}

Another way to view this transformation is through the following
table.%
%
\footnote{Adapted from {\it Unicode Standard Version 5.2}, Table 3-5.}
%
\begin{center}
\begin{tabular}{|r|r|}
\hline
\tblhead{Code Point Bits} & \tblhead{UTF-16 Bytes} 
\\ \hline
\code{xxxxxxxx xxxxxxxx} & \code{xxxxxxxx xxxxxxxx}
\\ \hline
\code{000uuuuuxxxxxxxxxxxxxxxx} & \code{110110ww wwxxxxxx 110111xx xxxxxxxx}
\\ \hline
\end{tabular}
\end{center}
%
Here \code{wwww} = \code{uuuuu} - 1 (interpreted as numbers then
recoded as bits).  The first line indicates is that if the value fits
in 16 bits, those 16 bits are used as is.  Note that this
representation is only possible because not all code points between 0
and $2^{16}-1$ are legal.  The second line indicates that if we have a
code point above \unicode{10000}, we strip off the high-order five
bits uuuuu and subtract one from it to get four bits wwww; again, this
is only possible because of the size restriction on uuuuu.  Then,
distribute these bits in order across the four bytes shown on the
right.  The constants 110110 and 110111 are used to indicate what are
known as surrogate pairs.

Any 16-bit sequence starting with 110110 is the leading half of a
surrogate pair, and any sequence starting with 110111 is the trailing
half of a surrogate pair.  Any other sequence of initial bits means a
16-bit encoding of the code point.  Thus it is possible to determine
by looking at a pair of bytes whether we have a whole character, the
first half of a character represented as a surrogate pair, or the
second half of a character represented as a pair.

\subsubsection{UTF-8}

UTF-8 works in much the same way as UTF-16 (see the previous section),
only using single bytes as the minimum encoding scheme.  We
use the table-based visualization of the encoding scheme.%
%
\footnote{Adapted from {\it Unicode Standard Version 5.2},  Table 3-6.}
%
\begin{center}
\begin{tabular}{|r|r|}
\hline
\tblhead{Code Point Bits} & \tblhead{UTF-8 Bytes}
\\ \hline
\code{0xxxxxxxx} & \code{0xxxxxxx}
\\ \hline
\code{00000yyyyyxxxxxx} & \code{110yyyyy 10xxxxxx}
\\ \hline
\code{zzzzyyyyyyxxxxxx} & \code{1110zzzz 10yyyyyy 10xxxxxx}
\\ \hline
\code{000uuuuuzzzzyyyyyyxxxxxx} & \code{11110uuu 10uuzzzz 10yyyyyy 10xxxxxx}
\\ \hline
\end{tabular}
\end{center}
%
Thus 7-bit ASCII values, which have values from \unicode{0000} to
\unicode{007F}, are encoded directly in a single byte.  Values from
\unicode{0080} to \unicode{07FF} are represented with two bytes, 
values between \unicode{0800} and \unicode{FFFF} with three bytes, 
and values between \unicode{10000} and \unicode{10FFFF} with four bytes.


\subsubsection{Non-Overlap Principle}

The UTF-8 and UTF-16 encoding schemes obey what the Unicode Consortium
calls the ``non-overlap principle.''  Technically, the leading,
continuing and trailing code units (bytes in UTF-8, pairs of bytes in
UTF-16) overlap.  Take UTF-8 for example.  Bytes starting with
\code{0} are singletons, encoding a single character in the range 
\unicode{0000} to \unicode{007F}.  Bytes starting with \code{110}
are the leading byte in a two-byte sequence representing a character
in the range \unicode{0080} to \unicode{07FF}.  Similarly, bytes
starting with \code{1110} represent the leading byte in a three-byte
sequence, and \code{11110} the leading byte in a four-byte sequence.
Any bytes starting with \code{10} are continuation bytes.

What this means in practice is that it is possible to reconstruct
characters locally, without going back to the beginning of a file.
At most, if a byte starts with \code{10} you have to look back
up to three bytes to find the first byte in the sequence.

Furthermore, the corruption of a byte is localized so that the rest of
the stream doesn't get corrupted if one byte is corrupted.

Another advantage is that the sequence of bytes encoding one character
is never a subsequence of the bytes making up another character.  This
makes applications like search more robust.


\subsubsection{Byte Order Marks}

In the encoding schemes which are not explicitly marked as being
little or big endian, namely UTF-32, UTF-16 and UTF-8, it is also
legal, according to the Unicode standard, to prefix the sequence of
bytes with a byte-order mark (BOM).  It is not legal to have
byte-order marks preceding the explicitly marked encodings, UTF-32BE,
UTF-32LE, UTF-16BE, UTF-16LE.

For UTF-32, the byte-order mark is a sequence of four bytes indicating
whether the following encoding is little endian or big endian.  the
sequence 0x00, 0x00, 0xFE, 0xFF indicates a big-endian encoding and
the reverse, 0xFF, 0xFE, 0x00, 0x00 indicates little endian.

For UTF-16, two bytes are used, 0xFE, 0xFF for big endian, and the
reverse, 0xFF, 0xFE for little endian.

Although superfluous in UTF-8, the three byte sequence 0xEF, 0xBB,
0xBF is a ``byte order mark'', though there is no byte order to mark.  
As a result, all three UTF schemes may be distinguished by inspecting
their initial bytes.

Any text processor dealing with Unicode needs to handle the byte order
marks.  Some text editing packages automatically insert byte-order
marks for UTF encodings and some don't.

\subsubsection{Character Types and Categories}

The Unicode specification defines a number of character types and
general categories of character.  These are useful in Java
applications because characters may be examined programatically for
their class membership and the classes may be used in regular
expressions.

For example, category ``Me'' is the general category for enclosing
marks, and ``Pf'' the general category for final quote punctuation,
whereas ``Pd'' is the general category for dash-like punctuation, and
``Sc'' the category of currency symbols.  Unicode supplies a notion of
case, with the category ``Lu'' used for uppercase characters and
``Ll'' for lowercase.

Unicode marks the directionality in which characters are typically
wirtten using character types like ``R'' for right-to-left
and ``L'' for left-to-right.

\subsubsection{Canonical Forms}

Consider again the problem of compound characters.  For example,
consider the three characters: \unicode{00E0},
\unicodedesc{latin small letter a with grave}, rendered
\charmention{\`a}, \unicode{0061}, \unicodedesc{latin small letter a}, 
rendered as \charmention{a}, and \unicode{0300},
\unicodedesc{combining grave accent}, which is typically rendered
by combining it as an accent over the previous character.  For most
purposes, the combination \unicode{00E0} means the same thing in text
as \unicode{0061} followed by \unicode{0300}.

In order to aid text processing efforts, such as search, Unicode
defines a notion of canonical decomposition.  The \techdefs{canonical
decomposition}{unicode!canonical decomposition} of \unicode{00E0} is
as the sequence \unicode{0061},
\unicode{0300}.  In general, a full canonical decomposition continues
to decompose characters into their components until a sequence of
non-decomposible characters is produced.  Furthermore, if there is
more than one nonspacing mark (like \unicode{0300}), then they are
sorted into a canonical order.

From the notion of canonical decomposition, two seqeunces are said to
be \techdefs{canonical equivalents}{unicode!canonical equivalent} if their
full canonical decompositions are equal.


\section{Encodings in Java}

Java has native support for many encodings, as well as the ability
to plug in additional encodings as the need arises.

\subsection{Supported Encodings in Java}

Each Java installation supports a range of character encoding, with
one character encoding serving as the default encoding.  Every
compliant Java installation supports all of the UTF encodings of
Unicode.  Other encodings may be added at the installation level, or
implemented programatically within a JVM instance.

The supported encodings determine what characters can be used to write
Java programs, as well as which character encoding objects are
available to convert sequences of bytes to sequences of characters.

The class \code{SupportedEncodings} contains code to display the
default encoding and the complete range of other encodings supported.
The first part of the main pulls out the default encoding using
a static method of \code{java.nio.charset.Charset},
%
\codeblock{SupportedEncodings.1}
%
The code to pull out the full set of encodings along with their
aliases is
%
\codeblock{SupportedEncodings.2}

The result of running the program is
%
\commandlinefollow{ant available-charsets}
\begin{verbatim}
Default Encoding=windows-1252

Big5
     csBig5
...
x-windows-iso2022jp
    windows-iso2022jp
\end{verbatim}
%
The program first writes out the default encoding.  This is
\code{windows-1252} on my Windows install.  It's typically
\code{ISO-8859-1} on Linux installs in the United States.
After that, the program writes all of the encodings, with the official
name followed by a list of aliases that are also recognized.  We
ellided most of the output, because there are dozens of encodings
supported.

\subsection{Encoding Java Programs}

Java programs themselves may be written in any supported
character encoding.

If you use characters other than ASCII characters in your Java
programs, you should provide the \code{javac} command with a
specification of which character set you used.  If it's not specified,
the platform default is used, which is typically Windows-1252 on
Windows systems and Latin1 on other systems, but can be modified as
part of the install.  

The command line to write your programs in Chinese using Big5 encoding
is
%
\begin{verbatim}
javac -encoding Big5 ...
\end{verbatim}
%
For Ant, you want to use the \code{encoding} attribute on the
\code{javac} task, as in
%
\begin{verbatim}
<javac encoding="Big5" ...
\end{verbatim}


\subsection{Unicode Charactes in Java Programs}

Even if a Java program is encoded in a small character set encoding
like ASCII, it has the ability to express arbitrary Unicode characters
by using escapes.  In a Java program, the sequence
\code{{\bk}u}\varcode{xxxx}  behaves like the character
\unicode{\varcode{xxxx}}, where \varcode{xxxx} is a hexadecimal
encoded value padded to four characters with leading zeroes.

For instance, instead of writing

\begin{verbatim}
int n = 1;
\end{verbatim}
%
we could write
%
\begin{verbatim}
\u0069\u006E\u0074\u0020\u006E\u0020\u003D\u0020\u0031\u003B
\end{verbatim}
%
and it would behave exactly the same way, because
\charmention{i} is \unicode{0069}, \charmention{n} is
\unicode{006E}, \charmention{t} is \unicode{0074},
the space character is \unicode{0020}, and so on, up
through\charmention{;}, which is \unicode{003B}.

As popular as Unicode is, it's not widely enough supported in
components like text editors and not widely enough understood among
engineers.  Writing your programs in any character encoding other than
ASCII is thus highly error prone.  Our recommendation, which we follow
in LingPipe, is to write your Java programs in ASCII, using the full
\code{{\bk}u}\varcode{xxxx} form of non-ASCII characters.


\section{Java Character Data Types}

Java contains primitive data types for characters and numerical
values.  For each primitive type, there is a corresponding
class.  For the primitive \code{char} data type, the corresponding
class is \code{Character}.

\subsection{Primitive Character Type}

Java's primitive \code{char} data type is essentially a 16-bit (two
byte) unsigned integer. Thus a \code{char} can hold values between 0
and $2^16-1 = 65535$.  Furthermore, arithmetic over \code{char} types
works like an unsigned 16-bit representation, including casting a
\code{char} to a numerical primitive type \ie{\code{byte}, \code{short}, 
\code{int}, or \code{long}}.

\subsubsection{Character Literals}

In typical uses, instances of \code{char} will be used to model text
characters.  For this reason, there is a special syntax for character
literals.  Any character wrapped in single quotes is treated as
a character in a program.  For instance, we can assign a character
to a variable with 
%
\begin{verbatim}
char c = 'a'; 
\end{verbatim}
%
Given the availability of arbitrary Unicode characters in Java
programs using the syntax \code{{\bk}u}\varcode{xxxx}, it's easy
to assign arbitrary Unicode characters.  For instance, 
to assign the variable \code{c} to the character \charmention{\`a},
\unicode{00E0}, use
%
\begin{verbatim}
char c = '\u00E0'; 
\end{verbatim}
%
Equivalently, because values of type \code{char} act like unsigned
short integers, it's possible to directly assign integer values
to them, as in
%
\begin{verbatim}
char c = \0x00E0;
\end{verbatim}
%
The expression \code{{\bk}0x00E0} is an integer literal in hexadecimal
notation.  In general, the compiler will infer which kind of integer
is intended.  

Because the Java \code{char} data type is only 16 bits, it is not
allowed to assign Unicode code points above \unicode{FFFF} to a
\code{char} variable.  For instance, \code{{\bk}u10000} is illegal.



\subsubsection{Character Arithmetic}

A character expression may be assigned to an integer result, as in
%
\begin{verbatim}
int n = 'a';
\end{verbatim}
%
After this statement is executed, the value of the variable \code{n}
will be 97.  This is often helpful for debugging, because it allows
the code points for characters to be printed or otherwise examined.

It's also possible, though not recommended, to do arithmetic with Java
character types.  For instance, the expression \code{'a'+1} is equal
to the expression \code{'b'}.

Character data types behave differently in the context of string
concatenation than integers.  The expressions \code{'a' + "bc"} and
\code{{\bk}u0061 + "bc"} are identical because the compiler treats
\code{{\bk}u0061} and \code{'a'} identically, because the Unicode
code point for \charmention{a} is \unicode{0061}.  Both expressions
evaluate to a string equal to \code{"abc"}.  

The expression \code{0x0061 + "bc"} evaluates to \code{"97bc"},
because \code{0x0061} is taken as a hexadecimal integer literal, which
when concatenated to a string, first gets converted to its
string-based decimal representation, \code{"97"}.  There are static methods in
\code{java.lang.Integer} to convert integers to string-based
representations.  The method \code{Integer.toString(int)} may be used
to convert an integer to a string-based decimal notation;
\code{Integer.toHexString(int)} does the same for hex.

\subsubsection{Characters Interpreted as UTF-16}

Java made the questionable decision to use 2-byte representations for
characters.  This is both too wide and too narrow.  It's wasteful for
representing text in Western European languages, where most characters
have single-byte representations.  It's also too narrow, in that
any code point above \unicode{FFFF} requires two characters to
represent in Java; to represent code points would require an
integer (primitive \code{int} type, which is 4 bytes).
%
\footnote{The designers' choice is more understandable given that there were no
code points that used more than a single pair of bytes for UTF-16 when
Java was designed, though Unicode all along advertised that it would
continue to add characters and was not imposing a 16-bit upper limit.}

When \code{char} primitives are used in strings, they are interpreted
as UTF-16.  For any code point that uses only two bytes in UTF-16,
this is just the unsigned integer representation of that code point.
This is why UTF-16 seemed so natural for representing characters in
the original Java language design.  Unfortunately, for code points
requiring four bytes in UTF-16, Java requires two characters, so now
we have all the waste of 2-bytes per character and all the
disadvantages of having Unicode code points that require multiple
characters.


\subsection{The Character Class}

Just like for the numerical types, there is a class,
\code{Character}, used to box an underlying primitive
of type \code{char} with an immutable reference.  As for numbers, the
preferred way to acquire an instance of \code{Character} is with the
static factory method \code{Character.valueOf(char)}.  Autoboxing and
unboxing works the same way as for numerical values.  

Like the numerical classes, the \code{Character} class is also
serializable and comparable, with comparison being carried out based
on interpreting the underlying \code{char} as an unsigned integer.
There are also utilities in Java to sort sequences of \code{char}
values (such as strings) in a locale-specific way, because not every
or dialect of a language uses the same lexicographic sorting.

Equality and hash codes are also defined similarly, so that two
character objects are equal if and only if they reference the same
underlying character.

\subsubsection{Static Utilities}

The \code{Character} class supports a wide range of Unicode-sensitive
static utility constants and methods in addition to the factory method
\code{valueOf()}.

For each of the Unicode categories, there is a constant, represented
using a byte, because the class predates enums.  For instance, the
general category ``Pe'' in the Unicode specification is represented by
the constant \code{END\_PUNCTUATION} and the general category of
mathematical symbols, ``Sm'' in Unicode, is represented by the
constant\code{MATH\_SYMBOL}.

For many of these constants, there are corresponding methods.  For
instance, the method \code{getDirectionality(char)} returns the
directionality of a character as a byte value, which can then be tested
against the possible values represented as static constants.  There
are also useful methods for determining the category of a characer,
such as \code{isLetter(char)} and \code{isWhitespace(char)}.

Because multiple Java \code{char} instances might be needed to
represent a code point, there are also methods operating on code
points directly using integer (primitive \code{int}) arguments.  For
instance, \code{isLetter(int)} is the same as \code{isLetter(char)}
but generalized to arbitrary code points.  The method
\code{charCount(int)} returns the number of \code{char} values
required to represent the code point (so the value will be 1 or 2,
reflecting 2 or 4 bytes in UTF-16).  The methods
\code{isLowSurrogate(char)} and \code{isHighSurrogate(char)} determine
if the \code{char} represents the first or second half of the UTF-16
representation of a code point above \unicode{FFFF}.  

The method \code{charPointAt(char[],int)} determines the code point
that starts at the specified index int he array of \code{char}.  The
method \code{charPointCount(char[],int,int)} returns the number of
code points encoded by the specified \code{char} array slice.
