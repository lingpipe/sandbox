\chapter{Character Language Models}\label{chap:char-lm}

A \techdef{language model} is a model that assigns probabilities to
strings.  Traditionally, these strings are either sequences of bytes,
sequences of characters, or sequences of tokens.  LingPipe provides
language model implementations for sequences of characters and
sequences of tokens.  In this chapter, we introduce the basic
interfaces and character language models.  In \refchap{token-lm},
we introduce token language models.

Although language models may be applied to arbitrary sequences, we are
going to focus on character sequences that make up natural language
text, This text may come from anywhere, including e-mail or instant
messages, blogs, newspapers, fiction novels, emergency room discharge
summaries, job applicant resume's, etc.

The basic operation of language models in LingPipe is to provide them
training data consisting of a sequence of texts.  After they are
trained, language models may be used to estimate the probability of
texts that were not in the training data.  

\section{Applications of Language Models}\label{section:char-lm-apps}

LingPipe uses character language models as the basis of several other
modules, including classification, taggers, chunkers, spelling
correction.  There are further applications for token language models
discussed in \refchap{token-lm}.  In all cases, these applications use
one or more language models to estimate different kinds of text.  For
instance, in language-model based classification, there is a language
model for each category, and texts are classified into the category
which assigns them the highest probability.  In chunkers, separate
language models might be constructed for person names, company names,
place names, and for text that's not part of a name; these are then
combined to analyze new text for names of entities.  For spelling
correction, a single language model characterizes the correct
spellings and is used to weigh alternative corrections against each
other.


One of the major applications of language models is in speech
recognition, where they play the same role as the language model in
spelling correction.  

Another application of language models is compression.  Using a
technique such as arithmetic coding, language models provide
state-of-the-art compression.  Although LingPipe does not implement an
arithmetic coder for compression, we will discuss the connection to
compression below when we discuss empirical cross-entropy.

Yet another application of language models is in characterizing
languages or sublanguages.  For instance, we can compare the entropy
of news stories in the {\it New York Times} with biomedical abstracts
in MEDLINE, the surprising result being that MEDLINE is more
predictable at the text level than the {\it Times}).  We can also
compare the entropy of Chinese and English, measuring the average
information content of a Chinese character (of which there are tens of
thousands) with the information content of an English character (of
which there are dozens).


\section{The Basics of $N$-Gram Language Models}

LingPipe's character- and token-based language models are based on
\techdef{$n$-grams}.  An $n$-gram is nothing more than a sequence of
$n$ symbols.  In this chapter, we consider character-based $n$-grams;
in \refchap{token-lm}, we consider tokens as symbols.

The basis of an $n$-gram model is storing counts for $n$-grams seen in
a training corpus of text.  These are then used to infer the
probability of a new sequence of symbols.  They key technical feature
of $n$-gram models is that they model the probability of a character
in a sequence based only on the previous $n-1$ characters.  Models
such as these with finite memories are called Markov chains.  

The chain rule allows us to model the probability of a sequence of
characters based on a model of a single character following a sequence
of characters before it.  For instance, with the character sequence
\stringmention{tooth} using 3-grams (trigrams), we decompose the
total probability of a sequence into the product of a sequence of
character probabilities, with
%
\begin{equation}
p(\stringmention{tooth})
= p(\stringmention{t})
\times
p(\stringmention{o}|\stringmention{t})
\times
p(\stringmention{o}|\stringmention{to})
\times
p(\stringmention{t}|\stringmention{oo})
\times
p(\stringmention{h}|\stringmention{ot}).
\end{equation}
%
To prevent the possibilty of underflow from multiplying long sequences
of numbers less than 1, we almost always work on the log scale.  This
also has the pleasant property of converting multiplication to
addition, yielding
%
\begin{equation}
\log p(\stringmention{tooth})
= \log p(\stringmention{t})
+
\log p(\stringmention{o}|\stringmention{t})
+ 
\log p(\stringmention{o}|\stringmention{to})
+
\log p(\stringmention{t}|\stringmention{oo})
+ 
\log p(\stringmention{h}|\stringmention{ot}).\label{eq:char-lm-log-scale}
\end{equation}


\section{Character-Level Language Models and Unicode}

LingPipe's character-based language models are models of sequences of
Java \code{char} values, which as you should recall from
\refsec{char-primitive}, represent UTF-16 byte pairs.  When LingPipe's
language models were coded, every Unicode code point could be coded in
a single UTF-16 byte pair.  As we discussed, this is enough to cover
Unicode's basic multilingual plane (BMP), which includes most
characters for most languages.%
%
\footnote{As of Unicode 4.1, there are characters with code points
  outside of the BMP.  To code such characters requires two 16-bit
  \code{char} values, known as a surrogate pair.  For the purposes of
  LingPipe's language models, characters are 16 bit UTF-16 values and
  a code point beyond the BMP is considered two characters.  What this
  will mean in practice is that LingPipe's language models will assign
  non-zero probability to sequences of \code{char} values that do not
  correspond to legal UTF-16 encodings, and thus don't correspond to
  Unicode strings.  This is internally consistent and does not affect
  most of the applications of language models.  The exception is
  untokenized spelling correction, where we have to be careful not to
  suggest corrections that are illegal sequences of \code{char}
  values.  In other cases, we just lose some modeling efficiency (in
  the statistical sense of efficiency, not the run-time sense).}




\section{Language Model Interfaces}\label{section:char-lm-interfaces}

There is a rich set of interfaces for language models and related
support classes in the LingPipe package \code{com.aliasi.lm}.  The
basic interfaces are in the \code{LanguageModel} interface, most
being defined as nested interfaces.

\subsection{Predictive: \code{LanguageModel}}

The top-level language model interface, \code{LanguageModel}, defines
methods for estimating probabilities of sequence of characters.  It
defines two methods for estimating the log (base 2) probability of a
sequence of characters, one based on arrays and one based on the
character sequence interface.
%
\begin{verbatim}
double log2Estimate(char[] cs, int start, int end);
double log2Estimate(CharSequence cs);
\end{verbatim}
%
The first method uses an indexed character array to define the slice
of characters from \code{cs[start]} to \code{cs[end-1]}.  For most
methods, the character slice method is more efficient and the
character-sequence method defined by first converting the sequence to
an array of characters.

The reason estimates are returned as logarithms is that we would
otherwise run into underflow.  If each character has an average
probabilty of 0.25 (roughly what they have in open-domain English text
like a newspaper), the probability of a sequence of 600 tokens has a
$2^{-1200}$ probabilty, which is too small to fit into a Java
double-precision floating point value (the minimum value for which is
$2^{-1074}$).

\subsection{Trainable: \code{LanguageModel.Dynamic}}

Language models that may be trained by supplying example text
implement the interface \code{LanguageModel.Dynamic}.  This interface
extends the corpus interface \code{ObjectHandler<CharSequence>},
which specifies the method
%
\begin{verbatim}
void handle(CharSequence cs);
\end{verbatim}
%
The character sequence is considered as training data and used to fit
the model parameters.  See \refsec{corpus-handlers} for more
information on \code{ObjectHandler} and how it fits into corpus
patterns.

There are four other methods named \code{train()}, the first two
of which simply allow training with character sequences.
%
\begin{verbatim}
void train(CharSequence cs);
void train(CharSequence cs, int count);
\end{verbatim}
%
The first method is a legacy method duplicating the functionality of
\code{handle()}.  The second method takes an integer count value which
determines the number of times the character sequence is used for
training.  Although implemented more efficiently, a training call
with a count is equivalent to calling the basic training method the
count number of times.

The last two methods just repeat the character sequence methods
with character slices.


\subsection{Conditional Probabilitiess: \code{LanguageModel.Conditional}}

Conditional language models predict the probability of a character
given the previous characters.  The interface
\code{LanguageModel.Conditional} specifies the method
%
\begin{verbatim}
double log2ConditionalEstimate(CharSequence);
\end{verbatim}
%
This method returns the probabilty of the last character in
the sequence given the initial characters in the sequence.  For
instance, \code{log2ConditionalEstimate("abcd")} returns the
probability of the character \charmention{d} following the
sequence of characters stringmention{abc}.  There is also
a method working on character slices.


\subsection{Bounded: \code{LanguageModel.Sequence}}

The interface \code{LanguageModel.Sequence} is just a marker
interface, meaning that it does not specify any new methods other than
that inherited from its superintervace, \code{LanguageModel}.

Sequence-based language model implementations are normalized so
that the sum of the probability of all strings of all lengths is one.%
%
\footnote{Because it's just a marker interface, the requirement is
  conventional rather than expressed in Java's type language.  This is
  similar to interface requirements on collections in the
  \code{java.util} core library.}
%
In symbols, this amounts to
%
\begin{equation}
\sum_{n \geq 0} \hspace*{8pt} \sum_{\text{\code{cs.length()} == n}} \hspace*{3pt} 2^{\text{\code{log2Prob(cs)}}} = 1.
\end{equation}

\subsection{Unbounded: \code{LanguageModel.Process}}

The other type of language model is a process language model, declared
by implementing the marker interface \code{LanguageModel.Process}.  A
process language model treats a sequence of characters as being
generated as a kind of random process that generates a character at a
time without a distinct beginning or end.  

Because they don't model the beginning or end of a string, process
language models are normalized by length so that the sum of the
probabilities of all strings of a given length is one.  In symbols,
for every \code{n}, we have
%
\begin{equation}
\sum_{\text{\code{cs.length()} == n}} \hspace*{3pt} 2^{\text{\code{log2Prob(cs)}}} = 1.
\end{equation}
%


\section{Process Character Language Models}

As we saw in the last section, language models have a particularly
simple interface.  We just give them text to train and then they
can model new text. 

The simplest language model implementation supplied by LingPipe is
\code{NGramProcessLM}, in package \code{com.aliasi.lm}.  This class
implements several interfaces, including several of the basic language
model interfaces, \code{LanguageModel}, the unbounded interface,
\code{LanguageModel.Process}, the trainable interface
\code{LanguageModel.Dynamic}, and conditional prediction interface
\code{LanguageModel.Conditional} (see \refsec{char-lm-interfaces} for
descriptions of these interfaces).

In addition to the LM interfaces, the \code{NGramProcessLM} class also
implements \code{ObjectHandler<CharSequence>} (see
\refsec{corpus-handlers} for a description of the handler interface).
This allows language models to be integrated into the general
parser/handler/corpus interfaces (see \refchap{corpus} for more
details).

Furthermore, process character LMs are both serializable (see
\refsec{io-object-data-io}) and compilable (see
\refsec{io-compilable}).  As we will see later in this section,
compilation produces a very efficient, but no longer dynamic
version of the language model, whereas serialization will allow
it to be written out and read back in.


\subsection{Process Character Language Model Demo}\label{section:char-lm-process-demo}

Perhaps the simplest way to get a feel for how language models work is
to code one up.  We provide an example in the class
\code{ProcessLmDemo} in this chapter's package,
\code{com.lingpipe.book.charlm}.  

\subsubsection{Code Walkthrough}

As with all of our demos, it's set up as a simple \code{main()} method
that reads its arguments from the command line.  Specifically, we need
an n-gram length (\code{ngram}), a text on which to train
(\code{textTrain}) and a text on which to test (\code{textTest}),
which are supplied as the first three arguments.  The training and
evaluation are then almost trivial.
%
\codeblock{ProcessLmDemo.1}
%
We start by creating a new instance of an \code{NGramProcessLM}
supplying the n-gram length to the constructor.  We then take the
training text string and pass it to the language model's
\code{handle(CharSequence)} method, which adds the text to the
training data.  Finally, we compute the log (base 2) of the
probaiblity of the test text using the language model's
\code{log2Estimate(CharSequence)} method.  

\subsubsection{Running the Demo}

The Ant target \code{process-demo} runs the command, using
the property \code{ngram} for $n$-gram length, the property
\code{text.train} for the training text, and the property
\code{text.test} for the test text.  It is thus run as
follows.
%
\commandlinefollow{ant -Dngram=5 -Dtext.train="abracadabra" -Dtext.test="candelabra" process-demo}
\begin{verbatim}
ngram=5    train=|abracadabra|    test=|candelabra|
log2 p(test|train)=-69.693
\end{verbatim}
%
The command-line arguments are first printed (with vertical bars to
denote the boundaries of the strings) and then the log probability of
the test sequence given the training data is shown.

One way of thinking of language models is as measuring a kind
of text similarity.  The more similar the test text is to the
training text, the higher its probability.  For instance, consider

If the text is very predictable, the log2 probability will be lower
(larger here means smaller absolute value, because logarithms of
values in $[0,1]$ are always negative).  For instance, consider
the following case.
%
\commandlinefollow{ant -Dngram=2 -Dtext.train="ababababab" -Dtext.test="ababab" process-demo}
\begin{verbatim}
ngram=2    train=|ababababab|    test=|ababab|
log2 p(test|train)=-3.060
\end{verbatim}

In general, shorter texts are more predictable.  For instance,
consider the same training text as above with a test text
that's a subsequence of the test text above:
%
\commandlinefollow{ant -Dngram=2 -Dtext.train="ababababab" -Dtext.test="abab" process-demo}
\begin{verbatim}
ngram=2    train=|ababababab|     test=|abab|
log2 p(test|train)=-2.419
\end{verbatim}
%
Note that -2.419 is greater than -3.060.  In linear probability terms,
that's $2^{-2.419} = 0.225$, whereas $2^{-3.060} = 0.120$.  In other
words, the model, after training, says that \stringmention{abab} is
almost twice as likely as \stringmention{ababab}.

For process language models, which don't model boundaries, substrings
always have higher (log) probability.  This will not necessarily be
the case for the boundary language models we consider in the next section.

A language model will assign probabilities to any texts of any length.
The characters don't need to have been seen in the training data.  For
instance, consider our first example above, where the characters
\charmention{e}, \charmention{n}, and \charmention{l} were new to the
training data.  Characters that were not seen in the training data tend
to have fairly low probabilities.  For instance, consider
%
\commandlinefollow{ant -Dngram=2 -Dtext.train="ababababab" -Dtext.test="xyxy" process-demo}
\begin{verbatim}
ngram=2    train=|ababababab|    test=|xyxy|
log2 p(test|train)=-71.229
\end{verbatim}
%
Again, it's clear why we use the log scale --- $2^-{71}$ is a very low
probability indeed.

The $n$-gram length is the most important of the tuning parameters,
the rest of which we turn to in the next section.  A longer $n$-gram
will provide a tighter model of the training text than shorter
$n$-grams.  The longer the $n$-gram, the higher the probability
assigned to the training text itself as well as to very similar text
(again measuring by $n$-gram frequency).  For instance, consider

\commandlinefollow{ant -Dngram=2 -Dtext.train="abcdef" -Dtext.test="abcdef" process-demo}
\begin{verbatim}
ngram=2    train=|abcdef|    test=|abcdef|
log2 p(test|train)=-11.334
\end{verbatim}

and

\commandlinefollow{ant -Dngram=3 -Dtext.train="abcdef" -Dtext.test="abcdef" process-demo}
\begin{verbatim}
ngram=3    train=|abcdef|    test=|abcdef|
log2 p(test|train)=-10.884
\end{verbatim}

The improvement is relatively small.  If we had repeated the training
text many times or if it had been longer, the effect would be
stronger.  Here, if you actually go up to 4-grams, the probability
estimate gets worse.  The reason for this is the default smoothing
parameters, which we discuss in \refsec{char-lm-smoothing}.

One thing to beware of is that increasing the $n$-gram size tends to
have a dramatic impact on the amount of memory required.  This is
apparent when you consider that for basic English, there are only a
few dozen characters and thus only a few dozen unigrams (1-grams).
But there are thousands of bigrams (2-grams), and millions of
5-grams and 6-grams.  Even with 40 characters, there are over
2.5 million potential 4-grams.  Luckily, text is fairly predictable,
so the set of $n$-grams actually observed tends to grow more
slowly than this worst case may lead you to believe.  


\section{Sequence Character Language Models}

In this section, we describe the second kind of character language
model available in LingPipe, the sequence models.  Sequence models are
applied when there are either boundary or length effects.  They
provide a properly normalized probability model to compare the
probabilities of strings of different lengths; the process models only
allow comparison of probabilities of strings of the same length.  The
sequence models in LingPipe are built on top of process language
models and use exactly the same interfaces for training and evaluation.

Sequence character language models are the basis for generating words
in LingPipe's hidden Markov model taggers and chunkers.  They are also
used for smoothing token generation in token-based language models.  A
slight variant of sequence character LMs is also used to model
sequences for spell checking.  Like process language models, sequence
models may also be used as the basis of classifiers.  The right
language model to use depends on the data.  If there are boundary
effects and the sequences are relatively short, such as words or
sentences as opposed to full documents, use sequence language models.

\subsection{Boundary and Length Effects}

The process character language models, which we saw in the last
section, treat the stream of characters as an ongoing process with no
start and no end.  One consequence of the process model is that the
initial and final characters in a language model are not treated any
differently than those in the middle. 

Suppose we wanted to build a language model for words, or for
sentences.  Note that a subsequence of a word is not necessarily a
word; for instance, consider \stringmention{saril}, which is a
substring of \stringmention{necessarily}, is not an English word.
Being like a word is graded, though, with \stringmention{saril}
being more word like than the substring \stringmention{bseq} of 
\stringmention{subsequence}.

Although it seems obvious, it is important statistically that each
word or sentence is of finite length (though there is, in principle,
no bound on this length other than the ability of our interlocuters to
understand us and the amount of time we have).  Words in most
languages are not uniform from start to end.  For instance, in
English, there are suffixes like \stringmention{ing} and
\stringmention{s} that appear far more often at the ends of words than
at the beginning of words.  

Another effect arises at the syllable level in English from
pronounciation.  Syllables tend to follow the \techdef{sonority
  hierarchy}, meaning that sonority tends to rise toward the middle of
the syllable and fall at the end.  For instance, vowels like
\stringmention{o} are the most sonorous, then nasals like
\stringmention{n} and \stringmention{m}, then consonants like
\charmention{r}, then stops like \charmention{t}.  For instance, the
one syllable sequences \stringmention{tram} and \stringmention{mart}
are words, and both have sonorities that increase toward the voewl and
then fall.  In contrast, \stringmention{rtam} is not pronounceable in
English, with the usual reason given that it violates the syllable
sonority principle, because \charmention{r} is more sonorous than
\charmention{t}.

In English sentences, we tend to find the first word capitalized more
often than would be expected if the generation of characters was
completely uniform.  And the sentence terminating punctuation
characters, like the period and question mark show up more frequently
at the end of sentences than elsewhere.  Perhaps more importantly,
regular characters tend not to show up at the ends of sentences in
well-edited text.

\subsection{Coding Sequences as Processes}

A standard trick used in both computer languages and mathematical
models for converting a process-type model into a bounded
sequence-type model is to add distinguished end-of-string characters.
For instance, this is the technique used by the C programming language
to encode strings.  The byte zero (\code{0x00}) is used to encode the
end of a string.  Thus (ASCII) strings in C are stored in arrays one
byte longer than the string itself.  Similarly, lines in many
file-oriented formats are delineated by end-of-line characters, such
as the carriage return (\unicode{000D}).  In order to present multiple
sequences, characters such as the comma are used in comma-separated
values (CSV) file formats used for spreadsheets.  These commas are
not part of the strings themselves, but act as separators.

To encode sequence models as processes, we add a distinguished
end-of-string character.  When we generate the end-of-string
character, we stop generating characters.  This leads to a bound on
the process.  If the probabilty of generating the next character
includes the possiblity of generating the end-of-string character,
then the result is a bounded model that's properly normalized over all
strings.  That is, the sum of the probabilities of all strings sums to
1, so we have a proper model of strings.  The process model generates
sequences in which the sum of the probabilities of strings of a given
length was 1.  The upshot of this is that there's no way to use the
process language models to compare string probabilities to each other
when the strings are different lengths.

The use of a distinguished end-of-string character also handles
boundary effects.  Because we have to generate the end-of-string given
the suffix of the word, we can model the fact that strings are more
likely to end following some character sequences than others.  For
instance, if we use \charmention{\$} as our end-of-string marker, then
we might find \stringmention{runs\$} to be more likely than
\stringmention{ru\$} because of the probability of generating
\charmention{\$} given \stringmention{runs} can be much higher than
the probabilty of generating \charmention{\$} given
\stringmention{ru\$}; in symbols, we can have
%
\begin{equation}
p(\charmention{\$}|\stringmention{runs}) >> p(\charmention{\$}|\stringmention{ru}).
\end{equation}

We also use a distinguished begin-of-string character which
we use to condition the probabilities for the string's prefix.
Suppose we use \charmention{{\^{}}} as our begin-of-string character.
What we do is reduce the probability of generating a string
like \stringmention{runs} to that of generating 
\stringmention{runs\$} given \charmention{\^{}}, thus setting
%
\begin{equation}
p_{\mbox{\footnotesize seq}}(\stringmention{runs})
= p_{\mbox{\footnotesize pr}}(\stringmention{runs\$}|\stringmention{\^{}}).
\end{equation}
%
This defines the probability of \stringmention{runs} in the
sequence model, $p_{\mbox{\footnotesize seq}}()$, in terms of
a conditional probability calculation in a process model,
$p_{\mbox{\footnotesize pr}}()$.

\subsection{Sequence Character Language Model Demo}

The demo code is in the class file \path{SequenceLmDemo} in this
chapter's package, \code{com.lingpipe.book.charlm}.  The code
is almost identical to the process LM demo introduced in
\refsec{char-lm-process-demo}.

\subsubsection{Code Walkthrough}

There are three command line arguments, populating the \code{ngram}
integer, \code{csvTrain} string and \code{textTest}.  Here, the
training data is assumed to be a sequence of texts separated by
commas.  The work is done in constructing a trainable instance
of the boundary language model, training on each text, then estimating
the probability of the test text.
%
\codeblock{SequenceLmDemo.1}
%
As usual, we do not show the print statements or the boilerplate
for the \code{main()} method or arguments.

\subsubsection{Running the Demo}

The Ant target \code{seq-demo} runs the demo, with arguments
specified as properties \code{ngram} for $n$-gram length, 
\code{csv.train} for the comma-separated training texts,
and \code{text.test} for the test text.  For example,
%
\commandlinefollow{ant -Dngram=4 -Dcsv.train="runs,jumps,eating,sleeping" -Dtext.test="jumps" seq-demo}
\begin{verbatim}
ngram=4   train=|runs|jumps|eating|sleeping|   test=|jumps|
log2 p(test|train)=-9.877
\end{verbatim}
%
Thus the log (base 2) probability of seeing the string
\stringmention{jump} in the model trained on the four specified words
is roughly -10, which translates to $2^{-10}$, or about 1/1000 on the
linear scale.  The reason the probability is so low is due to the
smoothing in the model (see \refsec{char-lm-smoothing}).

In contrast to the process model, in the sequence model, prefixes
are not always more probable than longer strings.  For
instance, consider \charmention{jump} instead of \code{jumps}.
%
\commandlinefollow{ant -Dngram=4 -Dcsv.train="runs,jumps,eating,sleeping" -Dtext.test="jump" seq-demo}
\begin{verbatim}
ngram=4   train=|runs|jumps|eating|sleeping|   test=|jump|
log2 p(test|train)=-13.037
\end{verbatim}
%
Under this model (4-grams with default parameters) with the specified
four training instances, the probability of \stringmention{jumps} is
about 8 times as likely as the probability of \stringmention{jump}
($2^{-13}$ is approximately 1/8000).

One of the most useful parts of the sequence model is how it
generalizes endings and beginnings.  For instance,
\stringmention{running} is estimated to be fairly probable (log
probabilty -21.5).  Even an unseen word such as
\stringmention{blasting} (log probability -46.4) is modeled as more
likely than substrings like \stringmention{blast} (log probability
-46.6), because the model has learned that words are likely to end in
\stringmention{ing}. 


\subsubsection{Configuring the Boundary Character}

The boundary character in the bounded language models may be specified
by the client.  The default is the reserved Unicode character
\unicode{FFFF}.  This must be set in the constructor, as it remains
immutable during training and/or compilation.



\section{Tuning Language Model Smoothing}\label{section:char-lm-smoothing}

Language models provide two parameters for tuning in addition to the
length of $n$-gram.  One of these controls the degree to which
lower-order $n$-grams are blended with higher-order $n$-grams for
smoothing.  The second controls the number of characters in the
0-order model.  We describe both in this section.

Operationally, the tuning parameters may be set at any time before
compilation.  These will dynamically change the behavior of the
dynamic language models.

\subsection{The Problem with Maximum Likelihood}

Maximum likelihood estimation would provide weights for characters
that are based only on the empirically observed counts in a training
set.  The problem with this is that we are likely to run into
sequences of characters in new test data that were not in the training
data.  We would like to assign these unseen sequences non-zero
probabilities so that the documents we observe in testing are assigned
non-zero probabilities.

For example, suppose we're trying to model the probability of the
letter \charmention{r} following the sequence \stringmention{the new
  cha} with a 7-gram model.  In a pure maximum likelihood 7-gram, the
probability of seeing the letter \charmention{r} will be given by
dividing the number of times \stringmention{ew char} occurred in the
training corpus by the number of times \stringmention{ew cha} showed
up followed by any character.  For instance, there might be two
instances of \stringmention{ew chai}, perhaps as a part of
\stringmention{new chair} (furniture) or \stringmention{new chai}
(beverage) and one of \stringmention{ew chas}, perhaps as part of
\stringmention{grew chastened}, in the corpus.  In this case, the
probability of the next letter being \charmention{i} is estimated at
2/3 and the probability of the next letter being \charmention{s} at
1/3.  Note that there is no probability left over for the possibility
of \charmention{r} being next.  Thus our model says the string we
observe during testing is impossible.  

\subsection{Witten-Bell Smoothing}

To get around this problem, we use a a technique known as
\techdef{interpolative smoothing} to ensure that every sequence of
characters is assigned a non-zero probability.  

LingPipe uses an approached to smoothing developed by Ian Witten and Timothy Bell.%
%
\footnote{Witten, Ian H.\ and Timothy C.~Bell. 1991. The zero-frequency
  problem: estimating the probabilities of novel events in adaptive
  text compression. {\it IEEE Transactions on Information Theory} {\bf 37}(4).}
%
The idea behind Witten-Bell smoothing for $n$-grams involves blending
order $n$ predictions with order $n-1$ predictions, all the way down
to a basic uniform distribution at order 0.  Thus even if we haven't
ever seen \stringmention{ew char} in the corpus, we will still assign
it a non-zero probability.  That probability will be a weighted
average of the 7-gram estimate of \charmention{r} given
\stringmention{ew cha}, the 6-gram estimate of \charmention{r} given
\stringmention{w cha}, the 5-gram estimate of \charmention{r} given
\stringmention{ cha} (with an initial space), the 4-gram estimate of
\charmention{r} given \stringmention{cha}, the 3-gram estimate of
\charmention{r} given \stringmention{ha}, the 2-gram estimate of
\charmention{r} given \stringmention{a}, the 1-gram estimate of
\charmention{r} given \stringmention{} (empty context).  

\subsection{Uniform Base Distribution}

This recursion bottoms out at the uniform estimate, which we consider
a 0-gram estimate for the sake of consistency.  In the uniform
estimate, each character is given equal probabilty.  If there are 256
possible characters, as in Latin 1, then each has a 1/256 probability
estimate in the uniform base case.  

Because we have a uniform base case, LingPipe's character language
models require the number of possible characters to be specified in
the constructor.  The default is \code{Character.MAX\_VALUE}, which is
$2^{16}-1$, which is a slight overestimate of the number of Unicode
UTF-16 values (see \refsec{char-primitive}).  The number of characters
can be set lower if we know we are dealing with ASCII or Latin 1 or
another restricted subset of Unicode characters.

\subsection{Weighting the $n$-Grams}

We have so far said that we take a weighted average of the predictions
of an $n$-gram, $(n-1)$-gram, down to a uniform model.  What we haven't
said is how the weights for each model are determined.  We'll provide
precise mathematical definitions in \refsec{char-lm-math}.  For now, we
will provide a more descriptive explanation.

The first thing to note is that the weighting is context dependent in
that it depends on the characters in the context from which we are
predicting the next character.  That is, if we are predicting the next
character from context \stringmention{ew char} we will likely have
different weightings than if we predict from context \stringmention{t
  of th}.

The weight assigned to the order $n$-gram model in the interpolated
model is based on two factors determined from the context for the
prediction and one parameter in the constructor.  For deciding how
heavily to weight an $n$-gram context in prediction, we take into
account how many times that context has been seen in the training
data.  The more we have seen it in the training data, the more heavily
it is weighted.  The second factor for weighting is how many different
outcomes we've seen given that context.  If we have a context with
many different outcomes, we will weight it less for prediction.  

In addition to these contextual factors, there is also a parameter
given to the constructor (with variable named \code{lambdaFactor})
that determines how much to weight higher-order contexts relative to
lower-order contexts.  The higher the value of this factor, the more
agressively we smooth.  In other words, with high values of the lambda
factor, the more heavily the lower-order $n$-grams are weighted.


\section{Underlying Sequence Counter}

The boundary language models are based on the sequence models.  The
sequence model probabilities are based on keeping a count of character
subsequences.  For both boundary and sequence models, the object that
manages the counts is accessible through the method
\code{substringCounter()}.  This returns an instance of
\code{TrieCharSeqCounter}.  The name reflects the data structure used,
a \techdef{trie}.  

With the sequence counter, raw counts of arbitrary substrings in
the training data may be queried and updated.  It is unlikely that
a client of the language model class will need to directly modify
the underlying counter by any operation other than pruning (see
\refsec{char-lm-pruning}.  

All of the counts are based on \code{long} values.  The sequence
counter efficiently stores values for low count sequences using
shorter integer representations.%
%
\footnote{Carpenter, Bob.  2005.  Scaling high-order character
language models to gigabytes. {\it ACL Software Workshop}.}



\section{Pruning Counts}\label{section:char-lm-pruning}

One problem with the naive application of $n$-gram character
language models is that the size of the model increases with
the amount of training data.  Although the model size increases
sublinearly due to repeated subsequences, it can grow almost
arbitrarily large with long $n$-grams.

One way to keep the memory constrained is to prune the underlying
counts from time to time.  Pruning works by removing all strings whose
count falls below some minimum value.  If there are millions of such
contexts, removing the ones with low count often has very little
effect on the overall probabilities assigned by the model.  One reason
for this is that the low counts tend to be associated with long
sequences that are low frequency.  These tend to have shorter
sequences which are almost as representative, but slightly higher
frequency.  Another reason is that the low frequency counts are for
low frequency training events which tend to occur less often in new
data than high frequency training events (if not, there's something
wrong with the training data that's making it unlike the test data).


\section{Compling and Serializing Character LMs}

Character language models can be compiled in the ordinary way.
They may also be serialized, but not using the standard Java
serialization.

\subsection{Compilation}

Like many of LingPipe's statistical models, the character language
model implementations come in two flavors.  First, there are the two
classes we've discussed already, \code{NGramProcessLM} and
\code{NGramBoundaryLM}, which implement handlers for character
sequences for training.  These two classes are both compilable (see
\refsec{io-compilable} for a general overview of the \code{Compilable}
interface in LingPipe).  Compiling an instance of one of these classes
produces instances of \code{CompiledNGramProcessLM} and
\code{CompiledNGramBoundaryLM}.  Although based on shared abstract
superclasses, these abstract bases are not public because there is a
sufficiently rich set of interfaces (see \refsec{char-lm-interfaces}
for the complete list).

Compiled character language models are much much faster than the
dynamic language models that allow training.  The reason for this is
threefold.  First, all of the smoothing is done at compile time, so
only a single $n$-gram need be evaluated, the one with the longest
matching context.  Second, a suffix tree data structure is used to
make it efficient to find the longest suffix for sequences of
evaluations.  Third, all of the logarithms are precomputed, so only
addition is required at run time (see \refeq{char-lm-log-scale}).  

Compiled language models only support the estimation interfaces,
though they support both the joint (sequence of characters) and
conditional (one character given previous characters) forms of
estimation.  The only other method they support is
\code{observedCharacters()}, which returns an array containing all of
the characters that have been seen (including the boundary character
for sequence models).

The classes that depend on character language models use the dynamic
forms of language models for training and the compiled form in compiled
models.

\subsection{Serialization-like Behavior}

Language models do not support the Java \code{Serializable}
interface.


\section{Thread Safety}

As with most of LingPipe's classes, language models are thread safe
under read-write synchronization.  Specifically, any number of
operations that read from language models, including all of the
estimation and sequence counter operations may be run concurrently
with each other. Write operations on the other hand, like training,
pruning, and setting configuration parameters like the interpolation
ratio, must operate exclusively of all other read or write operations.


\section{The Mathematical Model}\label{section:char-lm-math}

The mathematics of character language models are straightforward.
We'll begin with the process language model and move onto the
bounded language model.

\subsection{Strings}

First we'll establish some notation for talking about strings
mathematically.  Suppose we have an alphabet $\Sigma$ of characters.%
%
\footnote{In practice, $\Sigma$ will be a subset of Unicode code
  points.}
%
Let $\Sigma^n$ be the set of all sequences of such characters of
length $n$, which are called strings.  We let $\Sigma^* = \bigcup_{n
  \in \nats} \Sigma^n$ be the set of all strings.  We'll let
$\emptystring$ be the \techdef{empty string}, which is defined as the
unique sequence of length 0.


\subsubsection{The Chain Rule}

The basic idea of $n$-gram language models is
to use the chain rule to factor the probabiltiy of a string into
the product of the probabilities of each of its characters given
the characters that came before it.  In symbols, suppose we have
a string $\sigma = s_1,\ldots,s_k$ of length $k$.  The
chain rule tells us we can compute its probability as
%
\begin{equation}
p(s_1,\ldots,s_k) = p(s_1) \times p(s_2|s_1) \times \cdots \times p(s_k|s_1,\ldots,s_{k-1}).
\end{equation}
%

\subsection{Markov Chains}

The point of using a limited-context-length $n$-gram is that you only
look back at most $(n-1)$ characters.  This gives us
%
\begin{equation}
p(s_1,\ldots,s_k) 
= p(s_1) \times p(s_2|s_1) \times \cdots \times p(s_k|s_{k-n+1},\ldots,s_{k-1}).
\end{equation}
%
For instance, if we have a 2-gram model and string \stringmention{abra},
our model is
%
\begin{equation}
p(\stringmention{abra}) 
= p(\stringmention{a}) 
\times p(\stringmention{b}|\stringmention{a}) 
\times p(\stringmention{r}|\stringmention{b}) 
\times p(\stringmention{a}|\stringmention{r}).
\end{equation}
%
Assuming that our history is bounded in this way to a finite history
leads to what is known as a \techdef{Markov model}, with a sequence of
characters generated from it called a \techdef{Markov chain}.  In a
Markov model of character string generation, each character is
generated based on finitely many previous characters.  Note that being
Markovian doesn't say how the probabilty of the next character is
estimated, but only that it depends on at most a bounded finite
history.

\subsection{Training Data Sequence Counts}

Now suppose we have seen training data in the form of a set of $M$
strings $\sigma_1,\ldots,\sigma_M$.  Given such a training set, let
$\cnt{s}$ be the number of times the string $s$ appeared as a
substring of one of the training strings $\sigma_m$.  For instance, if
the training strings are \stringmention{abe} and
\stringmention{abracadabra}, the substring \stringmention{ab} shows up
3 times, once in the first string and twice in the second.

Let $\extcnt{\sigma}$ be the count of all extensions by one character
of the string $\sigma$, that is
%
\begin{equation}
\extcnt{\sigma} = \sum_{s' \in \Sigma} \cnt{\sigma \cdot s}.
\end{equation}
%
For instance, in the training data, the count of \stringmention{ra} is
2, but the extension count of \stringmention{ra} is only 1, because
the second training string ends with \stringmention{ra} which is not
extended.


\subsection{Maximum Likelihood Estimates}

We'll use the notation $\pmle(s_n|s_1,\ldots,s_{n-1})$ for
the maximum likelihood estimate of the conditional probabilty of a
seeing the character $s_n$ given the previous $(n-1)$ characters
$s_1,\ldots,s_{n-1}$.  The maximum likelihood estimate model is
derived in the usual way, by counting in the training data.
%
\begin{equation}
\pmle(s_n|s_1,\ldots,s_{n-1})
= \frac{\cnt{s_1,\ldots,s_{n-1},s_{n}}}
       {\extcnt{s_1,\ldots,s_{n-1}}}.
\end{equation}
%
That is, we count up all the times we've seen the character in
question following the string and divide by the number of times
the string was extended by any character.

These maximum likelihood estimates will almost certainly assign zero
probability to many characters in even relatively modest length
$n$-grams. 

\subsection{Witten-Bell Smoothing}

We follow Witten and Bell's approach to smoothing these maximum
likelihood estimates through interpolation.  The basic definition is
recursive, interpolating the maximum likelihood estimate at length $n$
with the smoothed estimate of length $(n-1)$.  In symbols, we let
$\lambda_{s_1,\ldots,s_{n-1}} \in [0,1]$ be the weight of the maximum
likelihood estimate for predicting the next character given that the
previous characters were $s_1,\ldots,s_{n-1}$.  The recursive case
of the definition is then given by
%
\begin{equation}
p(s_n|s_1,\ldots,s_{n-1})
= \lambda_{s_1,\ldots,s_{n-1}} \times \pmle(s_n|s_1,\ldots,s_{n-1})
+
(1 - \lambda_{s_1,\ldots,s_{n-1}}) \times p(s_n|s_1,\ldots,s_{n-2}).
\end{equation}
%
On each application of this rule, the context is shortened by one
character.

The recursion bottoms out by interpolating with the uniform
distribution $\punif(s_n)$, where if $N$ is the total number
of characters, then 
%
\begin{equation}
\punif(s_n) = 1/N.
\end{equation}
%
The exact form of the base case interpolates with the 1-gram
model.
%
\begin{equation}
p(s_n) = \lambda_{\emptystring} \times \pmle(s_n)
+ (1 - \lambda_{\emptystring}) \times \punif(s_n).
\end{equation}


\subsection{Process Language Models}

A process language model assigns probabilities to strings
in such a way that the probabilities of all strings of a given
length sums to 1.%
%
\footnote{The terminology, which is not standard in the
  language-modeling literature, derives from the fact that generating
  the sequence of characters forms what is known as a \techdef{random
    process.}}
%
For process language models, if we let $p(s)$ be the probability of a
string $s$, then
%
\begin{equation}
\sum_{s \in \Sigma^n} p(s) = 1.
\end{equation}
%


\subsection{Sequence Language Models}

Unlike the process language models, for which the probabilities of all
strings of a given length sum to 1, a bounded language model is a
proper model of arbitrary character sequences.  Thus a bounded
language model, with probability $p(s)$, is such that
%
\begin{equation}
\sum_{s \in \Sigma^n} p(s) = 1.\label{eq:lm-seq-norm}
\end{equation}
%
Thus we think of sequence language models as generating finite
sequences of characters then stopping; a process language model
goes on generating characters indefinitely.

We follow fairly standard (though often undocumented) procedure of
definining bounded language models in terms of process language
models.  Suppose we are definining sequences over the character set
$\Sigma$.  We will choose a new character, $b$, called the boundary
character, such that $b \not\in \Sigma$.  We then define a process
language model over the alphabet $\Sigma \cup \setext{b}$ and use it
to define our sequence language model distribution.

We define the probability of a string $s = s_1,\ldots,s_M \in \Sigma^*$
as
%
\begin{equation}
p_B(s_1,\ldots,s_M) = p(s_1,\ldots,s_M,b|b),\label{eq:lm-seq-def}
\end{equation}
%
where $p_B$ is the sequence language model probability distribution.
In words, we start with the boundary character $b$ as context, then
generate our string $s = s_1,\ldots,s_M$, then generate a final
boundary character $b$.  Note that we chose $s$ from the unaugmented
set of strings  $\Sigma^*$.  

Because we now have a termination condition for our character
generating process, it is fairly straightforward to verify that the
process generates a probability distribution normalized over all
strings, as shown in \refeq{lm-seq-norm}.  We start with the boundary
character, though don't generate it, which is why it's in the context
in \refeq{lm-seq-def}.  We then continue generating, at each point
deciding whether to generate the boundary character $b$ and stop or to
generate a non-boundary character and keep going.  Because the sum of
each next character in the augmented alphabet $\Sigma \cup \setext{b}$
sums to 1.0, the normalization in \refeq{lm-seq-norm} holds.











