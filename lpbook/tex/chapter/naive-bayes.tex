\chapter{Naive Bayes Classifiers}\label{chap:naive-bayes}

So-called naive Bayes classifiers are neither naive nor, under their
usual realization, Bayesian.  In this chapter, we will focus on
LingPipe's implementation of the traditional naive Bayes classifier,
only returning the other naive Bayes-like classifier after we have
covered language models and the general language-model classifiers.

After covering the basic modeling assumptions of naive Bayes, we
provide a simple example to help you get started.  The rest of the
chapter considers the details of the API, how to tune and evaluate a
naive Bayes model, and how to use it in a semi-supervised setting with
small amounts of training



\section{Introduction to Naive Bayes}

The theory of naive Bayes classifiers is based on several fairly
restrictive assumptions about the classification problem.  This
section lays out the basics.

\subsection{Texts as Bags of Words}

Although naive Bayes may be applied to arbitrary multivariate count
data, LingPipe implements text classifiers, where the objects being
classified are implementations of Java's \code{CharSequence} interface
\eg{\code{String} or \code{StringBuilder}}.

For LingPipe's naive Bayes implementations, texts are represented as
so-called bags of words.%
%
\footnote{In general, these ``words'' will be arbitrary tokens, but
  ``bag of words'' is the usual terminology.  In statistical parlance,
  a bag corresponds to a multinomial outcome.}
%
LingPipe uses a tokenizer factory to convert a character sequence into
a sequence of tokens.  The order of these tokens doesn't matter for
naive Bayes or other classifiers that operate over bags of words.  As
we'll see later, the way in which a sequence of characters is
tokenized into sequences of strings plays a large role in
tuning a naive Bayes classifier effectively.

A bag of words is like a set of words in that the order doesn't
matter, but is unlike a set in that the count does matter.  For
instance, using a whitespace-based tokenizer, the strings
\stringmention{hee hee haw} and \stringmention{hee haw hee} produce
the same bag of words, namely \stringmention{hee} appears twice and
\stringmention{haw} once.  These strings produce a different bag of
words than \stringmention{hee haw}, which only has a count of one for
\stringmention{hee}.


\subsection{Exhaustivity and Exclusivity and Text Genre}

Naive Bayes classifiers require two or more categories into which
input texts are categorized.  These categories must be both exhaustive
and mutually exclusive.

For example, a news site such as Bing's or Google's, might classify a
news story as to whether it belongs in the category U.S., World,
Entertainment, Sci/Tech, Business, Politics, Sports, or Health.  As
another example, a research consortium might want to classify MEDLINE
citations mentioning mice as to whether they mention the effects of
any specific gene or not (the former class being useful for those
researching the genetic landscape of mice).  As a third example, a
marketing firm interested in a particular brand might classify a blog
post into four categories: positive toward brand, negative toward
brand, neutral toward brand, doesn't mention brand.  As a fourth
example, we could classify a patients discharge summaries (long texts
written by care givers) as to whether it indicates the patient is a
smoker or not.

Note that in each case, the texts under consideration were a
particular kind, such as newswire stories, MEDLINE citations about
mice, general blog posts, or patient discharge summaries.  We can
refer to the set of documents of this type as a genre or the domain of
the classifier.

The requirement of exhaustiveness is relative to texts that are drawn
from the genre under consideration.  We don't try to classify sports
stories as to whether they are about genomics or not or have a
positive sentiment toward the food quality.  

Often, this genre boundary can be moved by reconceptualizing the
classifier and training it on broader or narrower data types.  For
instance, the second example was restricted to MEDLINE citations about
mice, and doesn't consider full-length research articles or scientific
news stories, or even MEDLINE citations not about mice.  The third
example, in contrast, classifies all blog entries, but has a category
``doesn't mention brand'' to deal with posts not about the brand in
question.

In practice, classifiers may be applied to texts drawn from a
different genre from which they were trained.  For instance, we could
take blog sentiment classifiers and try to apply them to hotel
reviews.  Or we could apply MEDLINE citation classifiers to the full
texts of research articles.  In these cases, accuracy is almost always
worse on out-of-domain texts than in-domain texts.  For instance, we
could apply our blog sentiment classifier to product reviews in
magazines, but we would not expect it to work as well in that context
as for the kinds of blogs over which it was trained.  Similarly, we
could apply the mouse genetics classifier to full-length journal
articles, but it would likely not perform as well as for the citations
over which it was trained.

\subsection{Training Data: Natural versus Handmade}

To train naive Bayes classifiers and other supervised classifiers, we
require training data in the form of labeled instances.  In the case
of text classifiers, these consist of a sequence of texts paired with
their unique categories.  

For naive Bayes, and indeed for most statistical classifiers, the
training data should be drawn at random from the same distribution as
the test data will be drawn from.  Naive Bayes uses information about
which categories are most prevalent as well as what words are likely
to show up in which category.

For instance, we would create training data for news story section
headings by gathering news articles of the kind we'd like to classify
and assigning them to categories by hand. 

Sometimes we can find data already labeled for us.  For instance, we
could scrape a news aggregation or set of newspaper sites, recording
the category under which the article was listed (and perhaps
converting it back to the set of categories we care about).  These
might have originally arisen by hand labeling in the newspaper site,
but are most likely automatically generated by a news aggregator.
Similarly, we could examine the Medical Subject Heading (MeSH) tags
applied to MEDLINE citations by its curators to see if they were
marked as being about genomics and about mice.  

Sometimes we can gather data from less directly labeled sources.  For
instance, we can find positive and negative restaurant reviews by
examining how many stars a user assigned to them.  Or find blog posts
about food by creating a set of tags and searching for blogs with
those tags.  Or even by doing a web search with some terms chosen
for each category.

Our training data is almost always noisy, even if labeled by task
specialists by hand.  Naive Bayes is particularly robust to noisy
training data.  This produces a quality-quantity tradeoff when
creating the data.  High-quality data labeling is very labor
intensive, and its often good enough to get



\subsection{Generative Story}

Naive Bayes classifiers are based on a probabilistic model of a corpus
of texts with category-specific content.  Models like naive Bayes are
called ``generative'' in the machine learning literature because they
are richly specified enough to generate whole corpora.  In contrast,
classifiers like logistic regression are not generative in the sense
of being able to generate a corpus from the model.

The way in which naive Bayes represents a corpus, each document is
provided with a single category among a set of possible categories.
We suppose there is a fixed set of $K > 1$ categories and that each
document belongs to exactly one category.  To generate a document, we
first generate its category based on a probability distribution
telling us the prevalence of documents of each category in the
collection.

Then, given the category of a document, we generate the words in the
document according to a category-specific distribution over words.
The words are generated independently from one another given the
document's category.  The conditional independence of words given the
document category is almost always violated by natural language texts.
that is why the naive Bayes model is often erroneously called
``naive'' (the error is in labeling the model itself naive rather
than its application in a given setting).

Note that we do not model the selection of the number of topics $K$,
the number of documents $N$, or the number of words $M_n$ in the
$n$-th document; these are given as constants.  

Given a top-level distribution over categories along with a
distribution over words for each category, it is straightforward to
generate a corpus of documents.  But we can go one better and also
generate the category prevalence distribution and the
category-specific distributions over words.  In order to do so, we
need a distribution over these distributions, which is known as a
prior in Bayesian statistics.

The standard assumption is that the distribution over categories is
drawn from a symmetric Dirichlet prior and the distributions over
words for each category are all drawn from a symmetric Dirichlet
prior.  In practice, this amounts to additive smoothing for our
parameter estimates for the category distributions.  In natural
language processing, the most common priors are Laplace priors, which
adds 1, Jeffreys priors, which add 0.5, and uniform priors, which add
0.  The uniform priors lead to traditional maximum likelihood
estimates, which are dangerous in practice for reasons we explain
below.


\section{Getting Started with Naive Bayes}\label{section:nb-getting-started}

LingPipe has two naive Bayes implementations.  In this section, we
focus on the traditional implementation of naive Bayes, which is found
in the LingPipe class \code{TradNaiveBayes} in the package
\code{com.aliasi.classify}.  

\subsection{Laugh Classification: His or Hers?}

Our example involves classifying laughs based on whether they were
produced by a man (his) or his wife (hers).  The manly laugh consists
of more ``haw'' and less ``hee.''  The training data we will use has
three examples of laughs from him and her.  His laughs are
\stringmention{haw}, \stringmention{haw hee haw}, and
\stringmention{haw haw}.  Her laughs in the training data are
\stringmention{haw hee}, \stringmention{hee hee hee haw}, and
\stringmention{haw}.  Note that the single word \stringmention{haw}
shows up as a laugh for her and for him.  Naive Bayes and all of our
other classifiers can handle this kind of ``inconsistent'' training
data, which is not actually inconsistent under a probabilistic model.
It's a matter of who's most likely to utter what, not that they
can't utter the same laughs.

\subsection{Setting up the Classifier}

The basic functionality of the naive Bayes classifier class can be
gleaned from a simple demo program which shows how the model is
trained and how it is run.  We provide such an example in the class
\code{TradNbDemo}, which consists of a single \code{main()} method.
The method starts by assigning the input arguments, in this case
a single argument representing the text to be classified.
%
\codeblock{TradNbDemo.1}

The next step is to set up the classifier itself, the
minimal constructor for which requires a tokenizer factory
and set of categories represented as strings.
%
\codeblock{TradNbDemo.2}
%
The regular expression \code{{\bk}P\{Z\}+} (see
\refsec{regex-unicode-classes}) produces a tokenizer factory that
defines tokens to be maximal sequences of characters which Unicode
considers not whitespace.  We have used the \code{asSet()} method in
LingPipe's \code{CollectionUtils} class to define a set of strings
consisting of the categories his and hers.  The classifier is
constructed using the categories and tokenizer factory.


\subsection{Providing Training Data}

At this point, we are ready to train the classifier using training
data.  For the demo, we just hard code the training data.  Each
training datum consists of an instance of the class
\code{Classified<CharSequence>}.  These training data are passed to
the classifier one at a time using its \code{handle()} method (see
\refsec{corpus-handlers} for a general overview of LingPipe handlers).
Because it implements \code{handle(Classified<CharSequence>)}, the
naive Bayes classifier class is able to implement the interface
\code{ObjectHandler<Classified<CharSequence>{}>}, which is convenient if
we want to supply the classifier as a callback to either a parser or a
corpus.

The order in which training data is provided to a naive
Bayes classifier is irrelevant, so we will provide all the
training data from the lady's laugh before the gentleman's.
%
\codeblock{TradNbDemo.3}
%
We start by creating a classification, \code{hersCl}, using the
category name literal \code{"hers"}.  A base LingPipe classification
extends the \code{Classification} class in the
\code{com.aliasi.classify} package.  These are used as the results of
classification.  Classifications are immutable, so they may be reused
for training, and thus we only have one.

After creating the classification, we create a list of training texts
using Java's built-in utility \code{asList()} from the \code{Arrays}
utility class.  Note that we used a list rather than a set because we
can train on the same item more than once.  For instance, the woman in
question may have hundreds of laughs in a training set with lots of
duplication.  Training on the same text again adds new information
about the proportion of laughs of different kinds. 

The final statement is a for-each loop, which iterates over the texts,
wraps them in an instance of \code{Classified<CharSequence>}, and
sends them to the classifier via its
\code{handle(Classified<CharSequence>)} method.  This is where the
actual learning takes place.  Wehn the handle method is called, the
classifier tokenizes the text and keeps track of the counts of each
token it sees for each category, as well as the number of instances
of each category.  

We do the same thing for training the classifier for his laughs,
so there's no need to show that code.

\subsection{Performing Classification}

Once we've trained our classifier with examples of his laughs and
her laughs, we are ready to classify a new instance.  This is
done with a single call to the classifiers \code{classify(CharSequence)}
method.
%
\codeblock{TradNbDemo.4}
%
Note that the result is an instance of \code{JointClassification},
which is the richest classification result defined in LingPipe.  The
joint classification provides a ranking of results, and for each
provides the conditional probability of the category given the text as
well as the log (base 2) of the joint probability of the category and
the text.  These are pulled off by iterating over the ranks and
then pulling out the rank-specific values.  After that, we print them
in code that is not shown.

\subsection{Running the Demo}

The code is set up to be run from Ant using the target \code{nb-demo},
reading the value of property \code{text} for the text to be classified.
for instance, if we want to classify the laugh \stringmention{hee hee},
we would call it as follows.
%
\commandlinefollow{ant -Dtext="hee hee" nb-demo}
\begin{verbatim}
Input=|hee hee|
Rank= 0  cat=hers  p(c|txt)=0.87  log2 p(c,txt)= -2.66
Rank= 1  cat= his  p(c|txt)=0.13  log2 p(c,txt)= -5.44
\end{verbatim}
%
Our classifier estimates an 87\% chance that \stringmention{hee hee} was
her laugh and not his, and thus it is the top-ranked answer (note that
we count from 0, as usual).  We will not worry about the joint
probabilities for now.  

\subsection{Unknown Tokens}

The naive Bayes model as set up in LingPipe's
\code{TradNaiveBayesClassifier} class follows the standard practice
of ignoring all tokens that were not seen in the training data.  So
if we set the text to \stringmention{hee hee foo}, we get exactly
the same output, because the token \stringmention{foo} is simply
ignored.

This may be viewed as a defect of the generative model, because it
doesn't generate the entire text.  We have the same problem if the
tokenizer reduces, say by case normalizing or stemming or stoplisting
--- we lose the connection to the original text.  Another way of
thinking of naive Bayes is as classifying bags of tokens drawn
from a known set. 


\section{Independence, Overdispersion and Probability Attenuation}

Although naive Bayes returns probability estimates for categories
given texts, these probability estimates are typically very poorly
calibrated for all but the shortest texts.  The underlying problem is
the independence assumption underlying the naive Bayes model, which
is not satisfied with natural language text.  

A simple example should help.  Suppose we have newswire articles about
sports.  For instance, as I write this, the lead story on the {\it New
  York Times} sports page is about the player Derek Jeter's contract
negotiations with the New York Yankees baseball team.  This article
mentions the token \stringmention{Jeter} almost 20 times, or about
once every 100 words or so.  If the independence assumptions
underlying naive Bayes were correct, the odds against this happening
would be astronomical.  Yet this article is no different than many
other articles about sports, or about any other topic for that matter,
that focus on a small group of individuals.

In the naive Bayes model, the probability of seeing the word
\stringmention{Jeter} in a document is conditionally independent of
the other words in the document.  The words in a document are
not strictly independent of each other, but they are independent
of each other given the category of the document.  In other words,
the naive Bayes model assumes that in documents about sports,
the word \stringmention{Jeter} occurs at a constant rate.  In
reality, the term \stringmention{Jeter} occurs much more frequently
about baseball, particularly ones about the Yankees.

The failure of the independence assumption for naive Bayes manifests
itself in the form of inaccurate probability assignments.  Luckily,
naive Bayes classifiers work better for first-best classification than
one might expect given the violation of the assumptions on which the
model is based.  What happens is that the failed independence
assumption mainly disturbs the probabiilty assignments to different
categories given the texts, not the rankings of these categories.

Using our example from the previous section of the his and hers
laughter classifier, we can easily demonstrate the effect.  The
easiest way to see this is duplicating the input.  For instance,
consider classifying \stringmention{hee hee haw}, using
%
\commandlinefollow{ant -Dtext="hee hee haw" nb-demo}
\begin{verbatim}
Rank= 0  cat=hers  p(c|txt)=0.79
Rank= 1  cat= his  p(c|txt)=0.21
\end{verbatim}
%
If we simply double the input to \stringmention{hee hee haw hee hee haw},
note how the probability estimates become more extreme.
%
\commandlinefollow{ant -Dtext="hee hee haw hee hee haw" nb-demo}
\begin{verbatim}
Rank= 0  cat=hers  p(c|txt)=0.94
Rank= 1  cat= his  p(c|txt)=0.06
\end{verbatim}
%
Just by duplicating the text, our estimate of the laugh being hers
jumps from 0.79 to 0.94.  The same thing happens when Derek Jeter's
called out twenty times in a news story.

Because naive Bayes is using statistical inference, it is reasonable
for the category probability estimates to become more certain when
more data is observed.  The problem is just that certainty grows
exponentially with more data in naive Bayes, which is a bit too fast.
As a result, naive Bayes typically grossly overestimates or
underestimates probabilities of categories for documents.  And
the effect is almost always greater for longer documents.

\section{Tokens, Counts and Sufficient Statistics}

Document length per se is not itself a factory in naive Bayes models.
It only comes into play indirectly by adding more tokens.  In general,
there are only two pieces of information that the naive Bayes classifier
uses for training:
%
\begin{enumerate}
\item The bag of tokens for each category derived from combining all
  training examples, and
\item The number of training examples per category.
\end{enumerate}
%
As long as we hold the number of examples per category constant, we
can rearrange the positions of tokens in documents.  For instance,
we could replace his examples
%
\codeblock{FragmentsNb.1}
%
with
%
\codeblock{FragmentsNb.2}
%
of even
%
\codeblock{FragmentsNb.3}
%
with absolutely no effect the resulting classifier's behavior.  Neither
the order of tokens or their arrangement into documents is considered.

The latter example shows that the empty string is a perfectly good
training example; although it has no tokens (under most sensible
tokenizers anyway), it does provide an example of him laughing and ups
the overall probability of the laugher being him rather than her.
That is, the last sequence of three training example is not equivalent to
using a single example with the same tokens,
%
\codeblock{FragmentsNb.4}



\section{Unbalanced Category Probabilities}

In the simple example of laugh classification, we used the same number
of training examples for his laughs and her laughs.  The quantity
under consideration is the number of times \code{handle()} was
called, that is the total number of texts used to train each category,
not the number of tokens.

The naive Bayes classifier uses the information gained from the number
of training instances for each category.  If we know that she is more
likely to laugh than him, we can use that information to make better
predictions.  

In the example above, with balanced training sizes, if we provide no
input, the classifier is left with only prevalence to go by, and
returns a 50\% chance for him or her, because that was the balance of
laughs in the training set.  The following invocation uses a text
consisting of a single space (no tokens).%
%
\footnote{The program accepts the empty string, but Ant's notion of
  command-line arguments don't; empty string values for the \code{arg}
  element for the \code{java} task are just ignored, as in
  \code{<arg value=""/>}.}
%
\commandlinefollow{ant -Dtext=" " nb-demo}
\begin{verbatim}
Rank= 0  cat=hers  p(c|txt)=0.50
Rank= 1  cat= his  p(c|txt)=0.50
\end{verbatim}
%

Now let's consider what happens when we modify our earlier demo
to provide two training examples for her and three for him.  The
resulting code is in class \code{CatSkew}, which is identical
to our earlier example except for the data,
%
\codeblock{TradNbSkewedTrain.1}
%
\codeblock{TradNbSkewedTrain.2}
%
Note that we have used exactly the same tokens as the first
time to train each category.  


Now if we input a text with no tokens, we get a different estimate.
%
\commandlinefollow{ant -Dtext=" " cat-skew}
\begin{verbatim}
Rank= 0  cat= his  p(c|txt)=0.58
Rank= 1  cat=hers  p(c|txt)=0.42
\end{verbatim}
%
You might be wondering why the resulting estimate is only 58\% likely
to be his laugh when 60\% of the training examples were his.  The
reason has to do with smoothing, to which we turn in the next section.


\section{Maximum Likelihood Estimation and Smoothing}

At root, a naive Bayes classifier estimates two kinds of things.  First,
it estimates the probability of each category independently of any
tokens.  As we saw in the last section, this is carried out based on
the number of training examples presented for each category.

The second kind of estimation is carried out on a per-category basis.
For each category, the naive Bayes model provides an estimate of the
probability of seeing each token (in the training set) in that
category.

\subsection{Maximum Likelihood by Frequency}

These estimates are carried out by counting.  In the simplest case,
for category prevalence, if there are two categories, $A$ and $B$,
with three training instances for category $A$ and seven for category
$B$, then a simple estimate, called the maximum likelihood estimate,
can be derived based on relative frequency in the training data.  In
this case, the estimated probability of category $A$ is 0.3 and that
of category $B$ is 0.7.

The frequency-based estimate is called ``maximum likelihood'' because
it assigns the probability that provides the highest probability
estimate for the data that's seen.  If we have three instance of
category $A$ and seven of category $B$, the maximum likelihood
estimate is 0.3 for $A$ and 0.7 for category $B$.  The same thing
holds, namely that maximum likelihood probability estimates are
proportional to frequency, in the situation where there are more than
two categories.

The problem with simple frequency-based estimates is that they are not
very robust for large numbers of words with limited training data,
which is just what we find in language.  For instance, if we had
training for him and her that looked as follows,
%
\codeblock{FragmentsNb.5}
%
then the probability assigned to her uttering \stringmention{har} or
him uttering \stringmention{tee} would be zero.  This leads to a
particularly troubling situation when classifying an utterance such as
\stringmention{har tee har}, which contains both strings.  Such a
string would be impossible in the model, because he is assigned zero
probability of uttering \stringmention{tee} and she's assigned zero
probability of uttering \stringmention{har}.  This becomes a huge problem
when we're dealing with vocabularies of thousands or millions of 
possible tokens, many of which are seen only a handful of times in
a few categories.  

Maximum likelihood estimation is supported by LingPipe's traditional
naive Bayes implementation.  Just set the prior counts to zero; see
the next section for more detail on prior counts.

\subsection{Prior Counts for Smoothed Estimates}

To get around the zero-probability estimates arising from maximum
likelihood, the usual approach is to smooth these estimates.  The
simplest way to do this is to start all of the counts at a (typically
small) positive initial value.  This leads to what is known as {\it
  additive smoothing} estimates and the amount added is
called the {\it prior count}.%
%
\footnote{In statistical terms, additive smoothing produces the
  maximum a posteriori (MAP) parameter estimate given a symmetric
  Dirichlet prior with parameter value $\alpha$ equal to the prior
  count plus 1.}
%

There is a second constructor for \code{TradNaiveBayesClassifier} that
allows the prior counts to be specified, as well as a length normalization,
which we currently set to \code{Double.NaN} in order to turn off that
feature (see \refsec{naive-bayes-length-norm} for more information on
length normalization).  

An example of the use of this constructor is provided in the
class \code{AdditiveSmooth} in this chapter's package.  It consists
of a static main method, which begins as follows.
%
\codeblock{AdditiveSmooth.1}
%
Here we have defined the set of categories to be \stringmention{hot}
and \stringmention{cold}, used a whitespace-breaking tokenizer factory,
and set the category prior count to 1.0 and the token prior count to 0.5.
This will have the effect of adding 0.5 to the count of all tokens and
1.0 to the count of all categories.

Next, consider training three hot examples and two cold examples.
%
\codeblock{AdditiveSmooth.2}
%
There is a total of 7 different tokens in these five training items,
\stringmention{super}, \stringmention{steamy}, \stringmention{out},
\stringmention{boiling}, \stringmention{today},
\stringmention{freezing}, and \stringmention{icy}.  Only the token
\stringmention{out} appears in both a hot training example and a cold one.
All other tokens appear once, except \stringmention{steamy},
which appears twice in the hot category.

After building the model, the code iterates over the categories and
tokens printing probabilities.  First, the category probabilities
are computed; for generality, the code uses the method \code{categorySet()}
on the classifier to retrieve the categories.
%
\codeblock{AdditiveSmooth.3}
%
The method \code{probCat()} retrieves the probability of a category.

The next code block just iterates over the tokens using the
traditional naive Bayes classifier method \code{knownTokenSet()},
which contains all the tokens in the training data.  This is
an unmodifiable view of the actual token set.  Testing whether
a token is known may be carried out directly using the method
\code{isKnownToken(String)}.

Within the loop, it again iterates over the categories.  This time,
the traditional naive Bayes classifier method \code{probToken()}, with
arguments for a token and category, calculates the estimated
probability that any given token in a document of the specified
category is the specified token.
%
\codeblock{AdditiveSmooth.4}


\subsubsection{Running the Demo}

There is an ant target \code{additive-smooth} which calls the 
command.

\commandlinefollow{ant additive-smooth}
\begin{verbatim}
p(cold)=0.429    p(hot)=0.571

p(   super|cold)=0.077    p(   super| hot)=0.158
p(     icy|cold)=0.231    p(     icy| hot)=0.053
p( boiling|cold)=0.077    p( boiling| hot)=0.158
p(  steamy|cold)=0.077    p(  steamy| hot)=0.263
p(   today|cold)=0.077    p(   today| hot)=0.158
p(freezing|cold)=0.231    p(freezing| hot)=0.053
p(     out|cold)=0.231    p(     out| hot)=0.158
\end{verbatim}
%
The first pair of columns indicates the estimated probability of each
token given the category, hot or cold, and the bottom part indicates
the overall probability of the categories hot and cold before seeing
any data.  

Let's look at some particular estimates, focusing on the hot category.
Given this training data, a token in a statement expressing hot
weather is 5\% or so likely to be \stringmention{icy} and 26\% likely
to be \stringmention{steamy}.  The token \stringmention{icy} was
not in the training data for the hot category, whereas the token
\stringmention{steamy} appeared twice.  

With additively smoothed estimates, the probability assigned to a
given token is proportional to that token's count in the training data
plus its prior count.  Looking at our examples, these are 2.5 for
\stringmention{steamy} (count of 2 plus 0.5 prior count), 0.5 for
\stringmention{icy} and \stringmention{freezing}, and 1.5 for the
other four tokens.  Thus the total effective count is $1 \times 2.5 +
4 \times 1.5 + 2 \times 0.5 = 9.5$, and the probability for
\stringmention{steamy} is $2.5/9.5 \approx 0.263$.  Given this
estimation method, the sum of the token probabilities is guaranteed to
be 1.

The probabilities for categories are calculated in the same way.  That
is, the overall count of hot instances was 3, whereas the count of
cold training instances was 2.  The prior count for categories was
1.0, so the effective count for hot is 4 and for cold is 3, so the
estimated probability of a hot category is $4/7 \approx 0.571$.  As
with the tokens, the category probabilities will sum to 1, which is as
it should be with the interpretation of the categories as exclusive
and exhaustive.



\section{Item-Weighted Training}

Having seen in the last section how prior counts and training examples
determine estimates for two kinds of probabilities.  First, the probability
of a category, and second, the probability of a token in a message of
a given category.  In each case, we added one to the count of categories
or tokens for each relevant item in the training set.

There are two situations where it is helpful to weight the training
examples non-uniformly.  The first situation arises when some examples
are more important than others.  These can be given higher weights.
The second situation arises when there is uncertainty in the category
assignments.  For instance, we might have a string and be only 90\%
sure it's about hot weather.  In this case, we can train the hot
catgory with a count of 0.9 and the cold category with a count of 0.1
for the same training example.  This latter form of training is
particularly useful for semi-supervised learning, which we consider
below.  For now, we will just describe the mechanisms for carrying it
out and see what actually happens.

We provide an example in the class \code{ItemWeighting} in this
chapter's package.  For the most part, it is the same as the
\code{AdditiveSmooth} class in the way it sets up the classifier and
prints out the probability estimates.  The difference is that it weights
training examples rather than using the default weight of 1.0.
%
\codeblock{ItemWeighting.1}
%
We've used a smaller example set to focus on the effect of weighting.

There is an ant target \code{item-weight} that prints out the
resulting estimates, which are as follows.

\commandlinefollow{ant item-weight}
\begin{verbatim}
p(cold)=0.536    p(hot)=0.464

p( boiling|cold)=0.098    p( boiling| hot)=0.333
p(  warmly|cold)=0.255    p(  warmly| hot)=0.112
p(   dress|cold)=0.255    p(   dress| hot)=0.112
p(    mild|cold)=0.196    p(    mild| hot)=0.112
p(     out|cold)=0.196    p(     out| hot)=0.333
\end{verbatim}
%
There are two cold training instances trained with weights of 0.8 and
0.5.  Now rather than adding 1 to the category count for each
instance, we add its weight, getting a total observed weight of 1.3.
With a prior category count of 1.0, the efective count for the cold
category is 2.3.  There is one training instance for the hot category,
trained with a weight of 0.99, so the effective hot category count is
1.99, and the resulting probability of the cold category is
$2.3/(2.3+1.99) \approx 0.536$.

The tokens get weighted in the same way.  For instance, dress is
weighted 0.8, and there is a prior count of 0.5 for tokens, so the
effective count of \stringmention{dress} in the cold category is 1.3;
its effective count in the hot category is just the prior count, 0.5.
The token \stringmention{out} is weighted 1.0 in the cold category and
1.49 in the hot category.  The total count for cold tokens corresponds
to all counts and priors, with effective counts of 1.3 for
\stringmention{dress}, 1.3 for \stringmention{warmly}, 1.0 for
\stringmention{mild} and 1.0 for \stringmention{cold}.  There is also
a count of 0.5 for \stringmention{boiling}, even though it was never
used in a cold example.  Thus the probability estimate for
\stringmention{warmly} is $1.3/(1.3 + 1.3 + 1.0 + 1.0 + 0.5) \approx
0.255$.

\subsection{Training with Conditional Classifications}

Because it can train with weighted examples, the traditional naive
Bayes classifier supports a method to train using a conditional
classification.  The method \code{trainConditional()} takes four
arguments, a \code{CharSequence} for the input text, a
\code{ConditionalClassification} for the result, as well as a count
multiplier and minimum category virtual count, the effect of which we
describe below. This result is the same as if weighted training had
been carried out for every category with weight equal to the
conditional probability of the category in the classification times
the count multiplier.  If the virtual count consisting of the
probability times the count multiplier is less than the minimum
virtual count, the category is not trained.


\section{Document Length Normalization}\label{section:naive-bayes-length-norm}

It is common in applications of naive Bayes classification to mitigate
the attenuating effect of document length and correlation among the
tokens by treating each document as if it were the same length.  This
can result in better probabilistic predictions by reducing the
tendency of conditional probability estimates for categories to be
zero or one.  Length normalization does not affect the ranking of
categories for a single document, and thus returns the same first-best
guess as to category for a document.  Even though it doesn't change
rankings, length normalization does rescale the probabilty estimates
of categories given text.  This can lead to better calibration of
probabilities across documents, by making the probability estimates
less dependent on document length and token correlation.

Document length normalization works by multiplying the weight of each
token in the document by the actual document length divided by the
length norm.  Thus if the document is ten tokens long and the length
normalization is four tokens, each token of the ten in the input
counts as if it were only 0.4 tokens, for a total length of four
tokens.  If the length norm is four tokens and the input is two tokens
long, each token counts as if it were two tokens.  If the numbers are
round, the effects will be the same as removing that many tokens.  For
instance, if the input is \stringmention{hee hee haw haw haw haw} and
the length normalization is 3, the result is the same as presenting
\stringmention{hee haw haw} to an un-length-normalized naive Bayes
instance.

As we will see below in an example, length normalization applies
before unknown tokens are discarded.  Thus the behavior before and
after length normalization is different if there are unknown tokens
present.  The decision to implement things this way is rather
arbitrary; the model would be coherent under the decision to throw
away unknown tokens before rather than after computing length.  As
implemented in LingPipe, unknown tokens increase uncertainty, but
do not change the relative ranking of documents.  

Because of the multiplicative nature of probability, overall
scores are geometric averages, so that the reweighting takes place
in the exponent, not as a simple multiplicative factor.


\subsection{Demo: Length Normalization}

\subsubsection{Setting Length Normalization}

The length normalization value, which is the effective length used for
all documents, may be set in the constructor or using the method
\code{setLengthNorm(double)}.  The current value of the length norm is
oreturned by \code{lengthNorm()}.  Length norms cannot be negative,
zero, or infinite.  Setting the length norm to \code{Double.NaN} turns
off length normalization.

The class \code{LengthNorm} in this chapter's package trains exactly
the same model using exactly the same data as in our first example in
\refsec{nb-getting-started}.  To repeat, his laughs are trained on
three examples, \stringmention{haw}, \stringmention{haw hee haw}, and
\stringmention{haw haw}, and hers are trained on three instances,
\stringmention{haw hee}, \stringmention{hee hee hee haw}, and
\stringmention{haw}.  

The length normalization is read in as the second command-line
argument after the text to classify, and parsed into the \code{double}
variable \code{lengthNorm}.  The length norm for the classifier is
then set with the \code{setLengthNorm()} method.  
%
\codeblock{LengthNorm.1}
%
Note that we set the value before running classification.  It
doesn't matter when the length norm is set before classification.
We could have also set it in the constructor.

\subsubsection{Running Examples}

The Ant target \code{length-norm} runs the demo, with the text being
supplied through property \code{text} and the length norm through
property \code{length.norm}.  For example, we can evaluate a length
norm of 3.0 tokens as follows (with the rank 1 results elided):
%
\commandlinefollow{ant -Dtext="hee hee haw" -Dlength.norm=3.0 length-norm}
\begin{verbatim}
Input=|hee hee haw|   Length Norm=   3.00
Rank= 0  cat=hers  p(c|txt)=0.79  log2 p(c,txt)= -3.85
\end{verbatim}
%
Unlike the situation without length normalization, the number of
times the input is repeated no longer matters.  If we repeat it twice,
we get the same result.
%
\commandlinefollow{ant -Dtext="hee hee haw hee hee haw" -Dlength.norm=3.0 length-norm}
\begin{verbatim}
Input=|hee hee haw hee hee haw|   Length Norm=   3.00
Rank= 0  cat=hers  p(c|txt)=0.79  log2 p(c,txt)= -3.85
\end{verbatim}

We can also see that with length normalization, unknown tokens
are included in the length normalization.  For example, consider
the following, which adds two unknown tokens to the input.
%
\commandlinefollow{ant -Dtext="hee hee haw foo bar" -Dlength.norm=3.0 length-norm}
\begin{verbatim}
Input=|hee hee haw foo bar|   Length Norm=   3.00
Rank= 0  cat=hers  p(c|txt)=0.69  log2 p(c,txt)= -2.71
\end{verbatim}
%
With length normalization, the presence of unknown tokens drive
the probability estimates closer to 0.5 because they participate
in the length calculation but are not discriminative.  

Providing a length-norm value of \code{Double.NaN} turns off
length normalization.  For the input of \stringmention{hee hee haw},
this provides the same result as setting length normalization to
3.0.
%
\commandlinefollow{ant -Dtext="hee hee haw" -Dlength.norm=NaN length-norm}
\begin{verbatim}
Input=|hee hee haw|   Length Norm=    NaN
Rank= 0  cat=hers  p(c|txt)=0.79  log2 p(c,txt)= -3.85
\end{verbatim}
%
But now if we duplicate the document, we see the attenuating
effect of length again.
%
\commandlinefollow{ant -Dtext="hee hee haw hee hee haw" -Dlength.norm=NaN length-norm}
\begin{verbatim}
Input=|hee hee haw hee hee haw|   Length Norm=    NaN
Rank= 0  cat=hers  p(c|txt)=0.94  log2 p(c,txt)= -6.71
\end{verbatim}



\section{Serialization and Compilation}

The traditional naive Bayes classifier class implements both Java's
\code{Serializable} interface and LingPipe's \code{Compilable}
interface.  See \refsec{io-object-data-io} for object reading and
writing, including serializability, and \refsec{io-compilable} for a
general description of LingPipe's compilation interface.

Serializing a traditional naive Bayes classifier and reading it
back in results in an instance of \code{TradNaiveBayesClassifier} in
exactly the same state as the serialized classifier.  In particular,
a deserialized traditional naive Bayes classifier may be trained
with further examples.  

For more efficiency, a traditional naive Bayes classifier may be
compiled.  Among other optimizations, all the logarithms required
to prevent underflows and convert multiplications to additions
are precomputed.  

Compiling a traditional naive Bayes classifier and reading it back in
results in an implementation of \code{JointClassifier<CharSequence>}
if there are more than two categories.  If there are only two
categories, the class read back in will only implement the interface
\code{ConditionalClassifier<CharSequence>}.  The reason for this is
that the binary (two-category) case is heavily optimized for computing
the conditional probabilities and adding in the data to compute the
joint probabilities would double the size and halve the speed of
binary compiled classifiers.

In order to serialize a traditional naive Bayes classifier, its
tokenizer factory must be serializable.  For compilation, the
tokenizer factory must be either compilable or serializable.  If
the tokenizer factory implements LingPipe's \code{Compilable} interface
it'll be compiled, otherwise it'll be serialized.%
%
\footnote{This is carried out with LingPipe's static utility method
  \code{compileOrSerialize()}, which may be found in the
  \code{AbstractExternalizable} class in the \code{com.aliasi.util}
  package.}

\subsection{Serialization and Compilation Demo}

The demo class \code{SerializeCompile} in this chapter's package
illustrates both serialization and compilation.  Because
deserialization in java may throw class-not-found and I/O exceptions,
the main method starts off declaring that.
%
\codeblock{SerializeCompile.1}

The first two command-line arguments are for the text to be classified
and a (temporary) file name into which the classifier is serialized
(or compiled).

After constructing and training with the same code as in the first
example in \refsec{nb-getting-started}, compiling a naive Bayes
classifier to a file and reading it back in may be accomplished as
follows.
%
\codeblock{SerializeCompile.2}
%
The static utility method \code{compileTo()} from Java's class
\code{AbstractExternalizable} class (in the \code{util} package) is
used to do the writing.  This could also be done through LingPipe's
\code{Compilable} interface directly using the traditional naive Bayes
class's method \code{compileTo(ObjectOut)}.  We deserialized using
another utility method, \code{readObject()}, which reads and returns
serialized objects from files (there are also utility methods to read
compiled or serailized models from the class path as resources).

Usually, one program would write the compiled file and another
program, perhaps on another machine or at a different site, would read
it.  Here, we have put the two operations together for reference.
Note that the unchecked cast warning on reading back in is suppressed;
an error may still result at runtime from the cast if the file
supplied to read back in has an object of a different type or isn't a
serialized object at all. Note that when read back in, it is
assigned to a \code{JointClassifier<CharSequence>}; attempting to
cast to a \code{TradNaiveBayesClassifier} would fail, as the compiled
version is not an instance of that class.

We do the same thing for serialization, using a different utility method
to serialize, but the same \code{readObject()} method to deserialize. 
%
\codeblock{SerializeCompile.3}
%
Here, we are able to cast the deserialized object back to the original
class, \code{TradNaiveBayesClassifier}.  

Because deserialization results in a traditional naive Bayes
classifier, we may provide more training data.  Repeating the
serialization and deserialization with a different variable,
we can go on to train.
%
\codeblock{SerializeCompile.4}



\subsubsection{Running the Demo}

The Ant target \code{serialize-compile} runs the class, classifying
a text supplied as property \code{text} and using a file derived
from the value of the property \code{file}.  Running it shows
the result of classifying with the original, compiled, deserialized,
and then deserialized and further trained classifier.

\commandlinefollow{ant -Dtext="hee haw hee hee" serialize-compile}
\begin{verbatim}
text=hee haw hee hee

Results for: Original
Rank= 0  cat=hers  p(c|txt)=0.91

Results for: Compiled
Rank= 0  cat=hers  p(c|txt)=0.91

Results for: Serialized
Rank= 0  cat=hers  p(c|txt)=0.91

Results for: Serialized, Additional Training
Rank= 0  cat=hers  p(c|txt)=0.97
\end{verbatim}

Note that the conditional probability assigned to her laughing are the
same for the original, compiled, and serialized model.  The estimate
is different for the model with additional training data, as we
would expect.  We have not reported log joint probabilities because
they are not produced by the compiled model.


\section{Training and Testing with a Corpus}

In this section, we show how to use a \code{Corpus} implementation to
train and test a classifier.  In our demo of LingPipe's \code{Corpus}
abstract base class in \refsec{corpus-20-newsgroups-corpus}, we showed
how to create a corpus from the 20 Newsgroups data set (the data
and download location are described in \refsec{corpora-20-newsgroups}).

We will re-use our corpus code in this demo, though we have copied the
implementation so as to not make the imports in the demos too
confusing.  Thus you'll find the implementation in
\code{TwentyNewsgroupsCorpus} in this chapter's demo package
(\code{com.lingpipe.book.naivebayes}).

\subsection{Training Naive Bayes with a Corpus}

Training a classifier like naive Bayes with a corpus is particularly
straightforward.  As usual, the demo code lives in the \code{main()}
method and starts by marshalling the command-line arguments, here just
the name of the tar gzipped file in which the corpus was saved as
the \code{File} variable \code{corpusTgzFile}.  We
then just use the constructor to create the corpus. 
%
\codeblock{Nb20NewsCorpus.1}
%
Given the corpus, we apply the static method \code{getCatSet()} to
extract the categories from the corpus.  The set is then used to generate
an array of categories, which is then sorted.

The static utility category extraction method is defined using an
anonymous class used as a callback, as follows.
%
\codeblock{Nb20NewsCorpus.0}
%
The set is declared to be final so that it can be used within the
anonymous handler's method.  The corpus method \code{visitCorpus()} is
then applied to a newly defined object handler instance; in general,
the \code{visitCorpus()} method sends all of the classified objects in
the corpus to the specified handler.  In this case, our handler is
anonymous and handles a classified object by adding its
classification's first best category assignment to the category set.
The generic \code{<T>} is being used for the type of object being
classified, which doesn't matter here, but must be kept consistent
across the corpus and the inner class handling the callbacks.

The next block of code creates a fully parameterized traditional
naive Bayes classifier.
%
\codeblock{Nb20NewsCorpus.2}
%
LingPipe's built-in Indo-Euroepan tokenizer factory singleton is used
for tokenization (see \refchap{tokenization} for more on tokenization
in general and \refsec{tok-indo-euro} for more information on
LingPipe's Indo-European tokenizer factory).  Further note that the
category prior count is set to 1.0 and the token count prior to 0.1,
and the length normalization is set to 10.0; these were explained
earlier in this chapter.

After setting up the classifier, the corpus's \code{visitTrain()}
method is used to send all of the training instances to the
classifier instance.  

After visiting the training data, the classifier's ready to use.
Instead of using it directly, we compile it to a more efficient form.
This is done with the utility method \code{compile()} in LingPipe's
\code{AbstractExternalizable} class (see \refsec{io-compilable} for
more information on LingPipe's general compilation interface).  The
required cast may throw I/O exceptions and the implicit
deserialization may throw a class-not-found exception, so those are
declared on the top-level \code{main()} method.  Note that we have
assigned the compiled object to a more general type,
\code{ConditionalClassifier}.  That's because the compiled form of the
traditional naive Bayes classifiers are not instances of
\code{TradNaiveBayesClassifier}.

\subsection{Evaluation with a Corpus}

Because our corpus was built with a training and test section
according to the way the 20 Newsgroup corpus is distributed, we can
also use the corpus for testing.  The first step is to set up the
evaluator.  
%
\codeblock{Nb20NewsCorpus.4}
%
We use an evaluator for conditional classifications, which
we construct with the compiled classifier, the array of categories, and
a flag indicating whether or not to store inputs, which we set to
\code{false} here.  If you want to inspect the true positives, false
positives, etc., you need to set this flag to \code{true}.  

After setting up the classifier evaluator, we call the corpus
method \code{visitTest()} to supply the test cases to the 
evaluator's \code{handle()} method.  

Rather than just printing the evaluator, which provides a very verbose
dump, we have focused on the most common evaluation metrics for
all-versus-all and one-versus-all evaluations.  See
\refchap{classifier-evaluation} for a complete description of
available classifier evaluations.

\subsubsection{All-Versus-All Results}

The first thing we do is extract the results for accuracy,
and macro-averaged precision, recall, F-measure, and kappa
for the overall system
%
\codeblock{Nb20NewsCorpus.5}
%
These are then printed using code not displayed here.  See
\refsec{classifier-eval-macro-avg} for more information on
interpreting macro averaged results; they're essentially averages
of the one-versus-all results, to which we turn next.


\subsubsection{One-Versus-All Results}

In one versus all evaluations, we treat the twenty newsgroup
categories as being decomposed into twenty one-versus-all evaluations.
In these cases, we treat each problem as a binary classification problem.
See \refsec{classifier-eval-one-versus-all} for more information on
one-versus-all evaluations.  In code, we just loop over the
categories, extracting the one-versus-all evaluations as
precision-recall evaluation objects.
%
\codeblock{Nb20NewsCorpus.6}
%
From these, we pull out the true and false positives and negative
counts, overall accuracy, along with precision, recall (aka sensitivity),
specificity (aka rejection recall), and F measure.  These are then
printed in code not shown.

Further, for each category, we print out the non-zero cells of
the confusion matrix itself.  These indicate counts for the
number of times a document of a given category was classified
as something else, and vice-versa, though we only show the first
case.
%
\codeblock{Nb20NewsCorpus.7}
%
The confusion matrix as an integer array was created outside of the
loop as part of the last code block using the \code{matrix()} method
from the confusion matrix class.  The entries \code{matrix[k1][k2]}
indicate the number of times a document with true category \code{k1}
was misclassified as being of category \code{k2}.

\subsection{Running the Demo}

There's an Ant target \code{20-news-corpus} that runs the demo.
It reads the name of the tar gzipped data file from
the property \code{20-news-file}, which we specify here
on the command line.
%
\commandline{ant -D20-news-file=../../data-dist/20news-bydate.tar.gz 20-news-corpus}
%
The first thing it prints out after it finishes is the confusion
matrix accuracies for all-versus-all:
%
\begin{verbatim}
All Versus All
  correct/total = 5638 / 7532
  accuracy=0.749
  Macro Avg prec=0.785  rec=0.728   F=0.716
  Kappa=0.735
\end{verbatim}
%
After this, it prints the one-versus-all results.  For instance,
consider the fourth category, \stringmention{talk.politics.mideast},
for which we see the following results.
%
\begin{verbatim}
Category[17]=talk.politics.mideast versus All
  TP=313 TN=7137 FP=19 FN=63
  Accuracy=0.989
  Prec=0.943  Rec(Sens)=0.832  Spec=0.997  F=0.884
    * => alt.atheism : 6
    * => comp.graphics : 1
    * => comp.windows.x : 5
    * => misc.forsale : 1
    * => rec.autos : 1
    * => rec.motorcycles : 2
    * => rec.sport.baseball : 5
    * => rec.sport.hockey : 2
    * => sci.electronics : 3
    * => sci.space : 1
    * => soc.religion.christian : 34
    * => talk.politics.guns : 2
    * => talk.politics.mideast : 313
    alt.atheism => * : 5
    comp.graphics => * : 1
    sci.med => * : 2
    talk.politics.guns => * : 2
    talk.politics.misc => * : 5
    talk.religion.misc => * : 4
\end{verbatim}
%
After dumping the number true and false positive and negatives, it
prints the overall accuracy, which is 98.9\% in this case.  The other
one-versus-all evaluations have similarly high accuracies in the high
90\% range.  But overall accuracy was only around 75\%.  The reason
for this is that any negative is counted as correct, with overall
accuracy being the number of true positives plus true negatives
divided by the total cases.  The true negatives dominate these
numbers, and aren't sensitive to getting the correct category as long
as it's not the category being evaluated one versus all.  This is
reflected in relatively high precision and specificity values (94.3\%
and 99.7\% here), but lower recall values (83.2\% here).

The remaining lines indicate the actual confusions, with an asterisk
replacing the category being evaluated.  For instance, six instances
of documents which were in the \stringmention{talk.politics.mideast}
category were mistakenly classified as being of category
\stringmention{alt.atheism}.  Similarly, there were five cases of
documents whose true category was \stringmention{sci.med} that were mistakenly
classified as \stringmention{talk.politics.mideast}.









