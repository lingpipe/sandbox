\chapter{Character Data and the Web}\label{chap:web}

In this chapter we examine the intersection of Java character encoding 
and web technologies by showing the processing steps involved 
in sending character data via HTTP between Java programs.

The example programs for this chapter are in the code directory:
%
\displ{\filepath{src/chars/src/com/lingpipe/book/http}}
%

\section{The HTTP Protocol}\label{section:http}

How is character data sent via HTTP?
This is a misleading question.
Character data isn't sent via HTTP; \emph{bytes} are sent via HTTP.
It is up to the client and server programs to map these bytes to characters.
As we stated in \refsec{char-probs},
data sent and received across a socket is a stream of bytes.
Without knowing the character set and encoding scheme used to generate
these bytes, we cannot reliably interpret the data.

The HTTP protocol is used to send data between web servers and clients.
Web browers are but one kind of client.
Data can be sent between many web services.
Data interchange formats developed for data feeds include XML formats such as
RSS, ATOM, as well as JSON, the Javascript Object Notation.

An HTTP message consists of a header and an optional message body.
HTTP/1.1 is version of protocol in common use today.

The header consists of a series of fields.
A field is a key-value pair in clear text format separated by a colon character
and terminated by a carriage-return line-feed (CR-LF).
A field may have multiple values, in which case the values are separated by spaces,
therefore a value may not contain spaces.
The key must be ASCII text.
Non-ASCII values are allowed provided they are encoded.%
%
\footnote{See Internet Engineering Task Force (IETF) RFC 5987 for details.}
%
The header is terminated by a blank field, that is, two consecutive CR-LF sequences.
The message body, if any, follows the header.

The header fields of interest are the \code{Content-Length} and \code{Content-Type} fields.
The \code{Content-Length} field specifies the number of bytes in the body.
The \code{Content-Type} header consists of a MIME type optionally followed by 
a semi-colon followed by parameters of the form \code{attribute=value}.
The following example is taken from the HTTP/1.1 specification.
\begin{verbatim}
Content-Type: text/html; charset=ISO-8859-4
\end{verbatim}
This example specifies that message body is HTML encoded using
ISO-8859-4, the Latin-4 encoding set.
The HTTP/1.1 protocol specifies that if the \code{Content-Type} is missing from the header,
the default MIME type is text/plain and the default
character encoding is ISO-8859-1, Latin-1 (see \refsec{latin1}).

The \code{java.net} package contains classes used to implement networking applications.
We start by using \code{java.net.URL} and \code{java.net.URLConnection} objects
to send and recieve HTTP requests and responses via the example class \code{EchoHttpHeader}.
%
\displ{\filepath{src/regex/src/com/lingpipe/book/webchars/EchoHttpHeader.java}}
%
\codeblock{EchoHttpHeader.1}
%
First we create a URL object from the first command-line argument.
If the argument doesn't start with a known protocal such as
\code{http}, \code{ftp}, \code{file}, \code{jar}, or if it is null,
the constructor will throw a \code{MalformedURLException}.
Next we create an \code{URLConnection} by calling the \code{openConnection()}
method on the \code{URL} object.
Despite its name, this method doesn't connect to the specified website, it just
creates a URLConnection object, to which setup parameters and and general 
request properties can be modified and added.
When the \code{connect()} method is invoked, the request is sent and the remote object
becomes available.  
%
\codeblock{EchoHttpHeader.2}
%
The method \code{getHeaderFields} parses the HTTP header into a map of
field names to values.
The first line of the HTTP header is the Status-Line which gives the
HTTP-version, status-code, and  reason-phrase, e.g.
\code{HTTP/1.1 200 OK}.
When we run this program with the wikipedia home page as the URL,
it returns the following headers
%
\commandlinefollow{ant -Durl="http://www.wikipedia.org" http-headers}
\begin{verbatim} 
field name: null	value: HTTP/1.0 200 OK 
field name: Age	value: 30
field name: Content-Length	value: 46388
field name: Last-Modified	value: Fri, 21 Dec ...
field name: X-Cache-Lookup	values : HIT from ...
field name: Connection	value: keep-alive
field name: Server	value: Apache
field name: X-Cache	values : HIT from cp1017.eqia...
field name: Cache-Control	value: s-maxage=3600, ...
field name: X-Content-Type-Options	value: nosniff
field name: Date	value: Tue, 15 Jan 2013 21:13:43 GMT
field name: Vary	value: Accept-Encoding
field name: Content-Type	value: text/html; charset=utf-8
\end{verbatim}
%
This page is the universal wikipedia page which consists of links to all
the language specific-wikipedias.

Next we request the Japanese wikipedia hompage
%
\commandlinefollow{ant -Durl="http://ja.wikipedia.org" http-headers}
\begin{verbatim}
field name: null	value: HTTP/1.0 200 OK 
field name: Age	value: 139
field name: Content-Language	value: ja
field name: Content-Length	value: 86186
field name: Last-Modified	value: Tue, 15 Jan 2013 ...
field name: Connection	value: keep-alive
field name: X-Cache-Lookup	values : MISS from cp1003.eqia...
field name: X-Cache	values : MISS from cp1003.eqiad.wmnet, ...
field name: Server	value: Apache
field name: X-Content-Type-Options	value: nosniff
field name: Cache-Control	value: private, s-maxage=0, ...
field name: Date	value: Tue, 15 Jan 2013 21:13:05 GMT
field name: Vary	value: Accept-Encoding,Cookie
field name: Content-Type	value: text/html; charset=UTF-8
\end{verbatim}
%
The field \code{Content-Language} is specified for the Japanese home page
but not for the universal Wikipedia page.  The contents of these pages are different
and this is reflected in the different values for the \code{Content-Length} field.
Both pages have the same Content-Type value: \code{text/html; charset=UTF-8}.
Using a Unicode charset means that almost all widely used characters in most of the
worlds languages can be used.
On my browser, most of the wikipedia languages display in thier proper font, excepting
Dhivehi, Gutisk, and a few others.

We use the header information to process the message body.
The example class \code{EchoHttpBody} retrieve the bytes from the message body and
uses the \code{charset} parameter in the \code{Content-Type} field to convert them
to characters.  
%
\displ{\filepath{src/regex/src/com/lingpipe/book/webchars/EchoHttpBody.java}}
%
This program reads in the URL from the command line and creates a \code{URLConnection},
as in the previous example.  Next it looks for the \code{Content-Type} field and
parses out the charset name.
%
\codeblock{EchoHttpBody.2}
%
The convenience method \code{URLConnection.getContentType()} returns the entire contents
of the \code{Content-Type} field.
We use a simple regex to extract the value of the charset parameter.
If no header field is present or the charset parameter is not specified
then the encoding defaults to Latin-1.
%
\codeblock{EchoHttpBody.3}
%
Next we read in the bytes from the body and convert them to a string
using the specified charset to correctly map bytes to characters.
%
\codeblock{EchoHttpBody.4}
%
In order to write this string to the terminal we need to use a \code{java.io.Writer}
that has the same character encoding as that of the string.
We construct one using Java's \code{Standard.out}, which is a \code{PrintStream}
that writes to the standard output stream.
The resulting \code{PrintWriter} will print the characters correctly,
provided that Ant and the terminal display are correctly configured (see \refsec{ant-console-config-utf8}).

To demonstrate, we use this program to fetch the French Wikipedia home page.
\commandlinefollow{ant -Durl="http://fr.wikipedia.org" http-body}
\begin{verbatim}  
URL: http://fr.wikipedia.org
Content-Type: text/html; charset=UTF-8
charset: UTF-8
HTTP Response body:
<!DOCTYPE html>
<html lang="fr" dir="ltr" class="client-nojs">
<head>
<title>Wikipédia, l'encyclopédie libre</title>
<meta charset="UTF-8" />
<meta name="generator" content="MediaWiki 1.21wmf6" />
\end{verbatim}
The first four lines of output are diagnostics printed by the \code{EchoHttpBody} program itself.
The remaining lines are the beginning of the French Wikipedia homepage,
which is an HTML document.


\section{MIME Types and the  \code{charset} Parameter}

MIME types are specified as type/subtype pairs optionally, followed by a parameter. 
MIME type \code{text/*} allows the \code{charset} parameter and includes type/subtypes:
\begin{verbatim} 
text/plain 
text/html
text/xml
\end{verbatim}
The MIME-type standard (see \url{http://www.ietf.org/rfc/rfc2046.txt}) specifies
that the default \code{charset} parameter for type \code{text} is ASCII (see \refsec{ascii}).

The type \code{text/xml} is a legacy type; for XML data the type 
\code{application/xml} is preferred. 
Data in a defined dialect of XML is specified accordingly.
For example, RSS feed data should be of type \code{application/rss+xml}.
Examples of MIME-types for XML and dialects of XML include:
\begin{verbatim}  
application/xml
application/rss+xml 
application/xhtml+xml 
\end{verbatim}  
The \code{application/xml} and \code{+xml} MIME types can take an
optional charset parameter.  If none is present it is up to the application
to infer or guess the character encoding used based on rules laid out
in the XML 1.0 Recommendation.  See \url{http://www.w3.org/TR/REC-xml/#sec-guessing}.

\subsection{MIME Type \code{application/x-www-form-urlencoded}}\label{section:urlencoding}

URL encoding, also called percent encoding, is used to encode both form data and URIs which
contain reserved or non-ascii characters.
The set of \emph{reserved characters} consists of the punctuation marks
\mbox{\code{\! *  ' ( ) ; : @ \& = \$ , / ? \# {[} {]} }}
all of which have special meaning in a URL.
The set of \emph{unreserved characters} consists of the ASCII characters A--Z, a--z, 0--9,
and the punctuation marks dash, underscore, period, and tilde (\textasciitilde{}).
The reserved characters and all other characters must be converted to UTF-8
and each byte of the UTF-8 representation
is encoded separately as a percent sign followed by the hex value of that byte
(which ranges from {\bk}x00 to {\bk}xFF).
Because the percent character itself is meaningful, if the percent character occurs in the
data then it too is URL encoded as \code{\%25}.

For example, the French wikipedia contains an index page named \mbox{Sp{\'e}cial:Toutes\_les\_pages}.
The link in the HTML to this page is encoded as
\begin{verbatim}
http://fr.wikipedia.org/wiki/Sp%C3%A9cial:Toutes_les_pages
\end{verbatim}
Were this link to be encoded again, the result would be
\begin{verbatim}
http://fr.wikipedia.org/wiki/Sp%25C3%25A9cial:Toutes_les_pages
\end{verbatim}


\section{XML}

In this section we focus on processing text data in XML documents.
There are many good books XML processing.
We particularly recommend \emph{Java and XML} by McLaughlin et.al.
published by O'Reilly.

\subsection{XML Encoding Declaration}

The character encoding used for the XML document is of primary importance.
It is specified in the opening XML declaration, e.g.
\begin{verbatim} 
<?xml version="1.0" encoding="UTF-8" ?>
\end{verbatim} 
The encoding declaration is optional.  If omitted, the default is UTF-8.

As we saw in \refsec{unicode}, not all Unicode characters are legal XML characters.
ASCII control characters are not legal Unicode, excepting tab, carriage return, and linefeed.
XML also disallows some Unicode characters in the surrogate code blocks.
XML documents which have been created from legacy data encoded in Latin-1 (ISO-8859-1)
or Windows-1252 may sometimes violate this constraint and will result in the parser
throwing some kind of exception, especially if the encoding declaration is missing
from the opening XML declaration.


\subsection{ XML}

An XML document consists of a single top-level element which contains
nested XML elements.  

\subsubsection{DOM and SAX APIs for XML}

The DOM (Document Object Model) approach to XML maps the XML document into a tree structure
and then navigates that tree.  
Once fully constructed, any part of the tree can be accessed and the structure of 
the tree itself can be modified.
Building the DOM tree for large documents can require considerable system resources.

SAX, the Simple API for Java, is an event-based sequential access
parser API.  It is a streaming interface.  Applications using SAX
receive event notifications about the XML document being processed as each
element and attribute are encountered in sequential order starting at the
top of the document, and ending with the closing of the ROOT
element. The application must implement handlers to deal with the different events.
SAX parsers can process XML in
linear time without placing too many demands upon system memory.

Here is the ``Hello World'' XML document
\begin{verbatim}
<?xml version="1.0"?>
<!DOCTYPE foo SYSTEM "foo.dtd">
<foo><bar>Hello, world!</bar></foo>
\end{verbatim}

An event-based parser treats this as a series of linear events
that are reported to the calling application as
\begin{verbatim}
start document
start element: foo
start element: bar
characters: "Hello, world!"
end element: bar
end element: foo
end document
\end{verbatim}

XML elements may contain text interleaved with other elements,
as in this schematic example of a paragraph in XHTML
\begin{verbatim}
<p> tok1 <b> tok2 </b> tok3 <b> tok4 tok5 </b> tok6 </p>
\end{verbatim}

The events reported in parsing this paragraph are
\begin{verbatim}
start element: p
characters: " tok1 " 
start element: b 
characters: " tok2 "  
end element: b 
characters: " tok3 "   
start element: b
characters: " tok4 tok5 " 
end element: b 
characters: " tok6 "  
end element: p
\end{verbatim}


\subsubsection{InputSource}








\subsection{SAX example}


event-based parsing using callbacks.
parsers keep track of markup, data
events are when markup is encountered

Applications implement handlers 

\subsection{Escapes for XML reserved characters}


\section{HTML}

Processing HTML is challenging because although standards
have been developed for HTML, they are often not met.
Therefore applications which mine HTML documents for text data
must be able to handle ill-formed HTML.
When we need to extract text data from an HTML document,
we use the NekoHTML parser.

http://nekohtml.sourceforge.net/

\subsection{Escapes for HTML reserved characters}

\section{JSON}

MIME-type application/json

simple json

\subsection{Escapes for JSON reserved characters}

