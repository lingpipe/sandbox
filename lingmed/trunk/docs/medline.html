<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html
     PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
     "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<title>LingMed: LingPipe for Entrez-Pubmed</title>
<meta http-equiv="Content-type"
      content="application/xhtml+xml; charset=utf-8"/>
<meta http-equiv="Content-Language"
      content="en"/>
<link href="lp-site.css"
      type="text/css"
      rel="stylesheet"
      title="lp-site"
      media="screen,projection,tv"/>
<link href="lp-site-print.css"
      title="lp-site"
      type="text/css"
      rel="stylesheet"
      media="print,handheld,tty,aural,braille,embossed"/>
</head>

<body>
<div id="header">
<h1 id="product">LingMed</h1><h1 id="pagetitle">MEDLINE Processing</h1>
<a id="logo" href="http://alias-i.com/">
<img src="http://alias-i.com/lingpipe/web/img/logo-small.gif" alt="alias-i logo"/>
</a>
</div><!-- head -->

<div id="navig">
<!-- set class="current" for current link -->
<ul>
<li><a href="../read-me.html">lingmed</a>
<ul>
<li><a href="getting_started.html">getting started</a></li>
<li><a href="dao.html">search and indexing</a></li>
<li><a class="current" href="medline.html">medline processing</a>
<ul>
<li><a href="#downloading">downloading medline</a></li>
<li><a href="#indexing">indexing medline</a></li>
<li><a href="#codec">choosing a codec</a></li>
<li><a href="#search">searching medline</a></li>
</ul>
</li>

<li><a href="lingblast.html">lingblast</a></li>
<li><a href="genelinkage.html">putting it together: genelinkage</a></li>
</ul>
</li>

<li><a href="http://alias-i.com/lingpipe/index.html">lingpipe</a></li>
</ul>
</div><!-- navig -->

<div id="content" class="content">
<a name="overview"><h2>Overview</h2></a>

<p>
Building a local searchable repository over MEDLINE
is more complicated than for the other Entrez databases 
because of the number of files which make up
the MEDLINE distribution, and the release schedule.
A new version of the index is released once a year,
and update files are released on a daily basis throughout the year.
The yearly release of the index is called the baseline distribution.
The baseline distribution of MEDLINE for 2009 contains 593 GZipped
files containing over 16 million citations. 
Updates are released daily, excepting weekends and holidays.
Update files include new citations, revisions of existing citations,
and deletion requests for existing records.
</p>

<p>


<p>
To build a MEDLINE index from scrach these tasks 
should be carried out in the following order:
</p>

<ul>
<li>Download the MEDLINE baseline distribution
</li>
<li>Index the MEDLINE baseline distribution
</li>
<li>Download the MEDLINE updates file(s)
</li>
<li>Index the MEDLINE updates file(s)
</li>
</ul>

<p>
The <a href="getting_started.html">getting started</a> section covers
how to run the LingMed programs to do this using Apache Ant.
In this section we examine the implementation details of these programs.
</p>

<a name="downloading"><h2>Downloading MEDLINE</h2></a>

<p>
The challenge in downloading the baseline distribution is 
getting hundreds of data files totalling many gigabytes worth of
data via FTP robustly and efficiently.
The challenge in downloading the updates is getting new
updates files in a timely fashion, without generating too
much traffic on the NLM servers.
</p>
<p>
The program 
<a href="../src/com/aliasi/lingmed/medline/DownloadMedline.java">
<code>com.aliasi.lingmend.medline.DownloadMedline</code></a>
handles downloading of both the baseline and updates files.
Once started, it runs perpetually, sleeping for a specified
interval between download attempts.
On each attempt it compares the set of files in the local repository 
against the files on the NLM server, and only
downloads files from NLM that are missing from the local repository.
In this way the program can handle transient network outages &mdash; 
if it cannot conntect to the NLM FTP server during one attempt, 
it simply goes back to sleep and tries again later.
We use Apache's 
<a href="http://logging.apache.org/log4j/1.2/index.html">log4j</a> 
logging facility to provide regular feedback on download progress, 
and to detect and diagnose 
the multiple points of failure which are possible
in network operations.
On startup the program logs its ftp and directory information,
and then it logs each file download attempt.
</p>

<p>
The NLM repository contains pairs of files: a gzipped data file 
and a corresponding md5 checksum file, 
e.g. {medline2009n0001.xml.gz, medline2009n0001.xml.gz.md5}.
A successful file download is one where both the data file 
and the checksum file download without error, 
and when the checksum is recomputed locally 
the computed checksum matches the checksum in the checksum file.
<code>DownloadMedline</code> first downloads the checksum file, and verifies its format.
Next it downloads the data file, recomputes the checksum,
and compares the expected checksum to the actual checksum.
If they match, then the downloader moves the data file to the
target download directory, else it removes both files.
The checksum files are downloaded to a subdirectory of the target download directory
named &quot;<code>md5</code>&quot;.
</p>

<p>
<code>DownloadMedline</code> extends the 
<a href="http://alias-i.com/lingpipe/docs/api/com/aliasi/util/AbstractCommand.html">
<code>com.aliasi.util.AbstractCommand</code></a> utility class,
which allows us to pass in local configuration information as
a set of command-line name-value pairs.
The required arguments to this command are:
</p>
<p>
<ul>
<li>
<b>domain</b>
&mdash; Domain name from which to download the citations.
</li>
<li>
<b>path</b>
&mdash; Path on the domain from which to download the citations.
</li>
<li>
<b>user</b>
&mdash; User name assigned by NLM. 
</li>
<li>
<b>password</b>
&mdash; Password assigned by NLM. 
</li>
<li>
<b>repositoryPath</b>
&mdash; Name of NLM directory where the distribution files are found.
</li>
<li>
<b>targetDir</b>
&mdash; Name of directory where distribution files are downloaded to. 
If downloading baseline files 
target should be the local baseline directory, 
and if downloading updates files
target should be the local updates directory.
</li>
</ul>
</p>
<p>
The following arguments are optional:
</p>
<p>
<ul>
<li>
<b>maxTries</b>
&mdash; Maximum number of download attempts per session. 
</li>
<li>
<b>sleep</b>
&mdash; Number of minutes to sleep between download sessions. 
</li>
</ul>
</p>

<a name="indexing"><h2>Indexing MEDLINE</h2></a>

<p>
Each baseline XML file consists of a top-level
<code>MedlineCitationSet</code> element which contains 
one or more <code>MedlineCitation</code> elements.
These individual citations are unique across the baseline distribution set,
that is, they have a unique <code>PMID</code> (PubMed identifier) element
which is a one to eight digit accession number without leading zeros.  
To process the baseline distribution we create a new Lucene index,
and process all baseline files.
Lucene processing is straightforward:  we convert each citation
into a Lucene 
<a href="http://lucene.apache.org/java/2_4_0/api/org/apache/lucene/document/Document.html">
<code>org.apache.lucene.document.Document</code></a>
and add it to the index.
</p>

<p>
The updates XML files contain a single top-level <code>MedlineCitationSet</code>
element.
This contains zero or more <code>MedlineCitation</code> elements
and possibly a <code>DeleteCitation</code> element which lists
the PubMed identifiers of the citations to be deleted.
Both new citation entries and revisions of existing citations
are encoded as <code>MedlineCitation</code>.
In order to distinguish between the two we must check the index
to see if there is already a citation entry in the index with the same PubMed identifier.
Lucene processing is more complicated than for the baseline files
because now we must delete documents as well as add them to the existing index.
</p>

<h3>The <code>IndexMedline</code> command</h3>

<p>
The 
<a href="../src/com/aliasi/lingmed/medline/IndexMedline.java">
<code>com.aliasi.lingmend.medline.IndexMedline</code></a> 
program parses the distribution files
and adds the citation entries that it finds to a Lucene index.
Command line parameters are used to specify which type of
distribution files are being processed, the directory they are in, 
and the Lucene index to create or update.
When processing the updates files it runs perpetually, 
sleeping for a specified interval between indexing sessions.
The required arguments are:
</p>
<p>
<ul>
<li>
<b>distType</b>
&mdash;  If value is "baseline" then distribution files
are treated as baseline files: all citations entries are added to the index
(no checking for revisions), and deletions are not allowed. 
Otherwise the files will be processed as updates files. 
</li>
<li>
<b>index</b>
&mdash;  Path to the Lucene index file. 
</li>
<li>
<b>distDir</b>
&mdash;  Path to the directory containing the distribution files. 
All files in this directory which end in ".xml" or ".xml.gz" will be processed.
</li>
<li>
<b>codec</b>
&mdash;    The name of the class which is used to transform 
a MedlineCitation object to a Lucene Document. 
This class must implement 
<a href="api/com/aliasi/lingmed/dao/Codec.html">
<code>com.aliasi.lingmed.dao.Codec</code></a>. 
</li>
</ul>
</p>
<p>
The following arguments are optional:
</p>
<p>
<ul>
<li>
<b>sleep</b>
&mdash;Number of minutes to sleep between indexing sessions. 
</li>
</ul>
</p>

<p>
<code>IndexMedline</code> extends the 
<a href="http://alias-i.com/lingpipe/docs/api/com/aliasi/util/AbstractCommand.html">
<code>com.aliasi.util.AbstractCommand</code></a> utility class.
Member variables are created from the command line parameters
in the constructor, and methods from the  class
<a href="../src/com/aliasi/lingmed/utils/FileUtils.java">
<code>com.aliasi.lingmend.utils.FileUtils</code></a> 
are invoked to check the validity of these arguments.
The <b>distType</b> parameter is parsed into a <code>boolean</code> variable <code>sIsBaseline</code>.
The <b>index</b> and <b>distDir</b> parameters are parsed into <code>File</code> variables.
The <b>codec</b> parameter is used to instantiate a <code>MedlineCodec</code> 
(see <a href="#codec">choosing a codec</a>).
</p>

<pre class="code">
public class IndexMedline extends AbstractCommand {
    ... 
    private File mIndex;
    private File mDistDir;
    ... 
    static boolean sIsBaseline;
    ... 
    private final MedlineCodec mCodec;

    // Instantiate IndexMedline object and 
    // initialize instance variables per command line args
    private IndexMedline(String[] args) throws Exception {
        super(args,DEFAULT_PARAMS);

        mIndexName = getExistingArgument(LUCENE_INDEX);
        mDistDirPath = getExistingArgument(DIST_DIR);
        mType = getExistingArgument(DIST_TYPE);
        ...
        if (mType.equalsIgnoreCase("baseline")) sIsBaseline = true;
        if (sIsBaseline) mIndex = FileUtils.checkIndex(mIndexName,true);
        else mIndex = FileUtils.checkIndex(mIndexName,false);
        mDistDir = new File(mDistDirPath);
        FileUtils.ensureDirExists(mDistDir);

        String codecClassName = getExistingArgument(CODEC_PARAM);
        Class clss = Class.forName(codecClassName);
        Constructor cons = clss.getConstructor();
        mCodec = (MedlineCodec) cons.newInstance();
</pre>

<h3>Parsing the distribution files</h3>

<p>
Processing a distribution file requires parsing the XML file
into its constituent entries and adding them to the Lucene index.
The
<a href="http://alias-i.com/lingpipe/docs/api/com/aliasi/medline/package-summary.html"><code>com.aliasi.medline</code></a>
package contains the classes used to parse the MEDLINE distribution files.
Parsing is done by a 
<a href="http://alias-i.com/lingpipe/docs/api/com/aliasi/medline/MedlineParser.html">
<code>MedlineParser</code></a>.
We use a Lucene
<a href="http://lucene.apache.org/java/2_4_0/api/org/apache/lucene/index/IndexWriter.html"><code>org.apache.lucene.index.IndexWriter</code></a>
to build the index.
Once all files have been processed the IndexWriter's
<a href="http://lucene.apache.org/java/2_4_0/api/org/apache/lucene/index/IndexWriter.html#optimize()"><code>optimize</code></a>
method is called to optimize the index file for search.
</p>

<p>
The <code>IndexMedline.run()</code> method instantiates the <code>MedlineParser</code>
and <code>IndexWriter</code> and processes each distribution file:
</p>

<pre class="code">
public void run() {
    ...
        MedlineParser parser = new MedlineParser(true); // true = save raw XML
        IndexWriter indexWriter 
            = new IndexWriter(FSDirectory.getDirectory(mIndex),
                              mCodec.getAnalyzer(),
                              new IndexWriter.MaxFieldLength(IndexWriter.DEFAULT_MAX_FIELD_LENGTH));
        for (File file: files) {
            ...
            MedlineIndexer indexer = new MedlineIndexer(indexWriter,mCodec);
            parser.setHandler(indexer);
            parseFile(parser,file);
            indexer.close();
            ...
        }
        indexWriter.optimize();
        indexWriter.close();
    ...
</pre>

<p>
The <a href="http://alias-i.com/lingpipe/docs/api/com/aliasi/medline/MedlineParser.html">
<code>MedlineParser</code></a> works through callbacks to a 
<a href="http://alias-i.com/lingpipe/docs/api/com/aliasi/medline/MedlineHandler.html"><code>MedlineHandler<code></a>.
The MedlineParser invokes the <code>MedlineHandler</code>&apos;s
<a href="http://alias-i.com/lingpipe/docs/api/com/aliasi/medline/MedlineHandler.htmll#handle(com.aliasi.medline.MedlineCitation)">
<code>handle(MedlineCitation)</code></a> method
on each <code>MedlineCitation</code> element in the <code>MedlineCitationSet</code>,
and for each PubMed identifier listed in the <code>DeleteCitation</code> element
it calls the method 
<a href="http://alias-i.com/lingpipe/docs/api/com/aliasi/medline/MedlineHandler.htmll#delete(java.lang.String)">
<code>delete(String)</code></a>.
</p>

<p>
<code>IndexMedline</code> contains a static class called 
<code>MedlineIndexer</code> which implements <code>MedlineHandler</code>
and which does the work of updating the index.
</p>

<pre class="code">
static class MedlineIndexer implements MedlineHandler {
    final IndexWriter mIndexWriter;
    final MedlineCodec mMedlineCodec;
    final IndexReader mReader;
    final IndexSearcher mSearcher;

    public MedlineIndexer(IndexWriter indexWriter, MedlineCodec codec) 
        throws IOException {
        mIndexWriter = indexWriter;
        mMedlineCodec = codec;
	mReader = IndexReader.open(indexWriter.getDirectory(),true); // read-only
	mSearcher = new IndexSearcher(mReader);
    }
    ...
    public void close() throws IOException { 
        mSearcher.close();
        mReader.close();
        mIndexWriter.commit();
    }
</pre>

<p>
Instantiating a <code>MedlineIndexer</code> automatically opens a fresh 
<code>IndexReader</code> and <code>IndexSearcher</code> on the index.  
The <code>MedlineIndexer.close()</code> method closes these objects and commits updates on the index.
</p>

<p>
When processing updates files we need to check whether or not a citation 
is already in the index.
The <code>MedlineCodec</code> maps the <code>MedlineCitation.pmid()</code> to 
its own field in the Lucene <code>Document</code>, so that we can
uniquely identify documents by PubMed identifier.
To lookup documents by PubMed id  we create a 
<a href="http://lucene.apache.org/java/2_4_0/api/org/apache/lucene/index/Term.html">
<code>org.apache.lucene.index.Term</code></a> 
object:
</p>

<pre class="code">
Term idTerm = new Term(Fields.ID_FIELD,citation.pmid());
</pre>

<p>
If this term is already in the index, then the handler updates the citation,
else it is added as a new document.
Here is the <code>MedlineIndexer.handle()</code> method:
</p>

<pre class="code">
public void handle(MedlineCitation citation) {
    Document doc = mMedlineCodec.toDocument(citation);
    try {
        if (sIsBaseline) {
           mIndexWriter.addDocument(doc);  
        } else {
            Term idTerm = new Term(Fields.ID_FIELD,citation.pmid());
            if (mSearcher.docFreq(idTerm) > 0) {
                mIndexWriter.updateDocument(idTerm,doc);
            } else {
                mIndexWriter.addDocument(doc);  
            }
         }
    } catch (IOException e) {
        mLogger.warn("index access error, term: "+citation.pmid());
    }
}
</pre>


<p>
To delete a citation from the index we use the 
<a href="http://lucene.apache.org/java/2_4_0/api/org/apache/lucene/index/IndexWriter.html#deleteDocuments(org.apache.lucene.index.Term)">
<code>deleteDocuments</code></a> method.
Here is the <code>MedlineIndexer.delete()</code> method:
</p>

<pre class="code">
public void delete(String pmid) {
    ...
    Term idTerm = new Term(Fields.ID_FIELD,pmid);
    mLogger.debug("delete citation: "+pmid);
    try {
        mIndexWriter.deleteDocuments(idTerm);
    } catch (IOException e) {
        mLogger.warn("index access error, term: "+pmid);
    }
}
</pre>

<p>
Indexing the MEDLINE daily updates file is a  simple batch-oriented process. 
The <code>IndexWriter</code> holds a write-lock on the index 
(a file-system based lock, see the 
<a href="http://lucene.apache.org/java/2_4_0/api/org/apache/lucene/store/LockFactory.html"><code>LockFactory</code></a>
javadoc), 
so only one <code>IndexMedline</code> process can ever be updating the index.
</p>

<h3>Storing index state information in index</h3>

<p>
To create a complete and consistent MEDLINE citation index
from the set of distribution files 
we must first process all of the files in the baseline distribution 
before processing the updates, 
and the updates files must be processed in the order in which they are released.
Furthermore, if we process the same baseline file more than once using the
<code>BaselineIndexer</code>, this will create duplicate documents in the index
for the citations in that file.
</p>

<p>
Because the <code>IndexMedline</code> program runs independently of
the <code>DownloadMedline</code> program, we need to find a way of
keeping track of which of the distribution files have already been processed.
We do this by storing the name of the last file processed in
the index itself.
We create a special <code>Document</code> containing 2 fields:
a tag field which uniquely identifies this document, 
and a field which contains the name of the last file processed.
The distribution files from NLM are named corresponding
to the order in which they are released.
By sorting the names of the files on disk
and comparing them to the name of the last processed file
we can easily determine which files have been processed already,
and which files have yet to be indexed.
</p>

<p>
When we are done parsing a distribution file we call the function <code>recordLastUpdate</code>:
</p>

<pre class="code">
private void recordLastUpdate(IndexWriter indexWriter, String fileName)
    throws IOException {
    Term term = new Term(Fields.LAST_FILE_FIELD,
                         Fields.LAST_FILE_VALUE);
    Document doc = new Document(); 
    Field tagField = new Field(Fields.LAST_FILE_FIELD,
                               Fields.LAST_FILE_VALUE,
                               Field.Store.YES,
                               Field.Index.NOT_ANALYZED_NO_NORMS);
    doc.add(tagField);
    Field nameField = new Field(Fields.FILE_NAME_FIELD,
                                fileName,
                                Field.Store.YES,
                                Field.Index.NO);
    doc.add(nameField);
    indexWriter.updateDocument(term,doc);
}
</pre>

<p>
This approach has one drawback:  
the index no longer consists of a set of homogeneous documents.
This is problematic for the <code>DaoSearcher</code> interface,
which extends the
<a href="http://java.sun.com/javase/6/docs/api/java/lang/Iterable.html"><code>Iterable</code></a>
interface, in order to provide a natural mechanism for walking over all documents in the index.
In our implementation, the <code>Iterator</code> returned by the <code>iterator()</code>
function of the <code>DaoSearcher</code> only iterates over those documents which 
can be successfully converted back to objects of the generic type.
For the MEDLINE index the <code>Iterator</code> will return 
all <code>MedlineCitation</code> documents, but not the document
for the last file name.
</p>



<a name="codec"><h2>Choosing a Codec</h2></a>

<p>
The structure and contents of the Lucene documents in the index
depends on the <code>Codec</code> implementation used by the indexer.
The package 
<a href="api/com/aliasi/lingmed/medline/package-summary.html"><code>com.aliasi.lingmed.medline</code></a>
contains a base implementation
<a href="api/com/aliasi/lingmed/medline/MedlineCodec.html"><code>MedlineCodec</code></a>,
which is extended by the class
<a href="api/com/aliasi/lingmed/medline/SearchableMedlineCodec.html"><code>SearchableMedlineCodec</code></a>.
</p>

<p>
Documents created using a <code>MedlineCodec</code> are indexed by PubMed id.
Running the <code>MedlineIndexer</code>
with an instance of <code>MedlineCodec</code> 
creates a compact data store over the entire MEDLINE distribution.
This is useful for applications where lookup is done by PubMed id alone.
Documents created using a <code>SearchableMedlineCodec</code> contain 
a rich set of searchable fields.
The resulting index allows searches over the 
the contents of any elements of a <code>MedlineCitation</code>.
</p>

<p>
Here is the base <code>MedlineCodec</code>&apos;s <code>toDocument(MedlineCitation)</code> method:
</p>

<pre class="code">
public Document toDocument(MedlineCitation citation) {
    Document doc = new Document(); 

    // index pubmed id (as keyword)
    Field idField = new Field(Fields.ID_FIELD,
                              citation.pmid(),
                              Field.Store.YES,
                              Field.Index.NOT_ANALYZED_NO_NORMS);
    doc.add(idField);

    // store raw XML
    Field xmlField = new Field(Fields.XML_FIELD,
                               citation.xmlString(),
                               Field.Store.COMPRESS,
                               Field.Index.NO);
    doc.add(xmlField);
    return doc;
}
</pre>

<p>
The document created contains 2 fields:
the ID field, which is indexed using the entire ID string as a keyword,
and the field containing the raw XML for the MedlineCitation entry,
which is not indexed for search, but which is stored in the index
in a compressed format.
</p>

<p>
A <code>SearchableMedlineCodec</code> 
creates Lucene documents with a rich set of searchable fields.
The <code>toDocument</code> method systematically indexes
each subpart of the <code>MedlineCitation</code> object.
Here is a snippet from <code>SearchableMedlineCodec</code>
showing how the MEDLINE abstract title is indexed:
</p>

<pre class="code">
public static final String TITLE_FIELD = "title";
...

public Document toDocument(MedlineCitation citation) {
    // get doc with its basic fields
    Document doc = super.toDocument(citation);

    Article article = citation.article();
    ...
    String title = article.articleTitleText();
    add(doc,TITLE_FIELD,title);
    ...
</pre>

<p>
As is evident, all work is done by the <code>add</code> method 
(and its helper methods):
</p>

<pre class="code">
static void add(Document doc, String fieldName, String text) {
    if (text == null || text.length() == 0) return;
    boolean appendToExisting = doc.getField(fieldName) != null;
    if (appendToExisting)
        text = " , " + text;
    if (TEXT_FIELD_SET.contains(fieldName)) {
        addTextField(doc,fieldName,text);
        addTextField(doc,fieldName+EXACT_FIELD_SUFFIX,text);
        addTextField(doc,fieldName+NGRAM_FIELD_SUFFIX,text);
    } else if (SIMPLE_FIELD_SET.contains(fieldName)) {
        addTextField(doc,fieldName,text);
    } else {
        addKeywordField(doc,fieldName,text);
    }
}

static void addTextField(Document doc, String fieldName, String text) {
    Field field = new Field(fieldName,text,
                            Field.Store.NO,
                            Field.Index.ANALYZED);
    doc.add(field);
}

...
static final String[] TEXT_FIELDS = new String[] {
    ABSTRACT_FIELD,
    ...
    TITLE_FIELD,
};

...
static final Set<String> TEXT_FIELD_SET 
    = new HashSet<String>(Arrays.<String>asList(TEXT_FIELDS));
</pre>

<p>
Text data is indexed 3 different ways:
using a standard analyzer which tokenizes its inputs,
strips off punctuation, lowercases, and filters out common stop words;
using an analyzer which uses a
<a href="http://www.alias-i.com/lingpipe/docs/api/com/aliasi/tokenizer/IndoEuropeanTokenizerFactory.html">
<code>com.aliasi.tokenizer.IndoEuropeanTokenizer</code></a> to
tokenize its inputs;
and using an analyzer which uses a 
<a href="http://www.alias-i.com/lingpipe/docs/api/com/aliasi/tokenizer/NGramTokenizer.html">
<code>com.aliasi.tokenizer.NGramTokenizer</code></a>
to create 3-gram tokens from its inputs.
</p>

<p>
In the code snippet from the <code>toDocument</code> method above,
the call:
</p>

<pre class="code">
 add(doc,TITLE_FIELD,title);
</pre>

<p>
ultimately adds 3 fields to the document:
&quot;title&quot;: indexed by tokens which have been lower-cased, punctuation and stop words removed;
&quot;titleX&quot;: indexed according to the LingPipe IndoEuropeanTokenizer rules;
&quot;titleN&quot;: indexed by 3-grams.
</p>

<p>
<code>MedlineCodec</code> contains a static class <code>MedlineAnalyzer</code>
which sets up the appropriate tokenizers:
</p>

<pre class="code">
static class MedlineAnalyzer extends LuceneAnalyzer {

    static class StandardTokenizerFactory implements TokenizerFactory {     
        public Tokenizer tokenizer(char[] cs, int start, int length) {
            Tokenizer tokenizer = SIMPLE_TOKENIZER_FACTORY.tokenizer(cs,start,length);
            tokenizer = new LowerCaseFilterTokenizer(tokenizer);
            tokenizer = new EnglishStopListFilterTokenizer(tokenizer);
            tokenizer = new PorterStemmerFilterTokenizer(tokenizer);
            return tokenizer;
        }
    } // acts like Lucene's StandardAnalyzer
    public TokenizerFactory TEXT_TOKENIZER_FACTORY
        = new StandardTokenizerFactory();

    // like Lucene's analysis.SimpleAnalyzer, but with digits, too
    public static final TokenizerFactory SIMPLE_TOKENIZER_FACTORY
        = new RegExTokenizerFactory("\\p{L}+|\\p{Digit}+");

    public static final TokenizerFactory EXACT_TEXT_TOKENIZER_FACTORY 
        = IndoEuropeanTokenizerFactory.FACTORY;

    public static final TokenizerFactory NGRAM_TEXT_TOKENIZER_FACTORY
        = new NGramTokenizerFactory(3,3);
    ...
    private MedlineAnalyzer() {
        for (String field : SearchableMedlineCodec.TEXT_FIELDS) {
            setTokenizer(field,TEXT_TOKENIZER_FACTORY);
            setTokenizer(field + SearchableMedlineCodec.EXACT_FIELD_SUFFIX,
                         EXACT_TEXT_TOKENIZER_FACTORY);
            setTokenizer(field + SearchableMedlineCodec.NGRAM_FIELD_SUFFIX,
                         NGRAM_TEXT_TOKENIZER_FACTORY);
        }
        for (String field : SearchableMedlineCodec.SIMPLE_FIELDS) {
            setTokenizer(field,SIMPLE_TOKENIZER_FACTORY);
        }
    }
}
</pre>

<a name="search"><h2>Searching MEDLINE</h2></a>

<p>
The interface
<a href="api/com/aliasi/lingmed/medline/MedlineSearcher.html"><code>MedlineSearcher</code></a>
defines the search functionality for Lucene MEDLINE citation index.
In the previous section on
<a href="dao.html">Search and Indexing</a> we present
the program <a href="../src/com/aliasi/lingmed/server/TestClient.java"><code>TestClient</code></a>,
which shows how to instantiate a <code>MedlineSearcher</code>
and search the MEDLINE index by PubMed id:
</p>

<pre class="code">
MedlineCodec medlineCodec = new MedlineCodec();
...
MedlineSearcher medlineSearcher = new MedlineSearcherImpl(medlineCodec,medlineRemoteSearcher);
...
MedlineCitation mc = medlineSearcher.getById(id);
</pre>

<p>
Another example of search over the MEDLINE index is the
<a href="api/com/aliasi/lingmed/lingblast/DictionaryBuilder.html">
<code>DictionaryBuilder</code></a> command in the 
<a href="api/com/aliasi/lingmed/lingblast/package-summary.html">
com.aliasi.lingmed.lingblast</a> package.
The DictionaryBuilder command creates an 
exact-match dictionary over a set of gene names and 
gene symbols from EntrezGene.
This dictionary is used to identify putative gene mentions in text.
Some gene names or symbols are the same as extremely frequent common
words, such as "Is".
We check to see how many times the gene name matches against 
words in the title or abstract text of all citations in the MEDLINE index
via the <code>MedlineSearcher</code>&apos;s
<a href="api/com/aliasi/lingmed/medline/MedlineSearcher.html#numExactPhraseMatches(java.lang.String)">
<code>numExactPhraseMatches(String)</code></a> method.
Highly ambiguous names are not added to the dictionary
(unless explicitly allowed).
</p>

<pre class="code">
for (Iterator dictIt=mDictMap.entrySet().iterator(); dictIt.hasNext(); ) {
    Entry&lt;String,Set&lt;String&gt;&gt; entry = (Entry&lt;String,Set&lt;String&gt;&gt;)dictIt.next();
    String alias = entry.getKey();
    ...
    int pubmedHits = mMedlineSearcher.numExactPhraseMatches(alias);
    if (pubmedHits &gt; mMaxPubmedHits &amp;&amp; !mAllowed.contains(alias)) {
        continue;  // don't add this entry to dictionary
    ...
</pre>

<p>
This method is implemented by 
<a href="api/com/aliasi/lingmed/medline/MedlineSearcherImpl"><code>com.aliasi.lingmed.medline.MedlineSearcherImpl</code></a>:
</p>

<pre class="code">
/**      
 * Return count for MedlineCitations which have an exact match for phrase
 * in either the title or abstract field.
 */
public int numExactPhraseMatches(String phrase) throws DaoException {
    String query = "abstractX:(+\"" + phrase + "\") titleX:(+\"" + phrase + "\")";
    return numHits(query);
}
</pre>

<p>
First we create the query string which will be executed against the index.
This query searches for  over two fields:  the exact-match title field and the
exact-match abstract field.
For example, a search for the phrase &quot;Your Favorite Gene&quot;
will be coded up as the following Lucene query:
</p>

<pre class="code">
abstractX:(+"Your Favorite Gene") titleX:(+"Your Favorite Gene")
</pre>

<p>
The parenthesis deliminate the field searches, the plus sign indicates that the field
must contain this item, and the double quotes enclose an entire phrase.
For more information on the Lucene query parser syntax see the Lucene
<a href="http://lucene.apache.org/java/2_4_0/queryparsersyntax.html">
Query Parser Syntax</a> documentation.
</p>

<p>
A natural addition to the <code>MedlineSearcher</code> interface would be the following method:
</p>

<pre class="code">
/**      
 * Find all MedlineCitations which have an exact match for phrase
 * in either the title or abstract field.
*/
public SearchResults<MedlineCitation> getExactPhraseMatches(String phrase) throws DaoException;
</pre>

<p>
Its implementation is left as an exercise to the reader.
</p>

</div><!-- content -->

<div id="foot">
<p>
&#169; 2003&ndash;2006 &nbsp;
<a href="mailto:lingpipe@alias-i.com">alias-i</a>
</p>
</div>

</body>

</html>
