\chapter{Naive Bayes Classifiers}\label{chap:naive-bayes}

So-called naive Bayes classifiers are neither naive nor, under their
usual realization, Bayesian.  In this chapter, we will focus on
LingPipe's implementation of the traditional naive Bayes classifier,
only returning the other naive Bayes-like classifier after we have
covered language models and the general language-model classifiers.

After covering the basic modeling assumptions of naive Bayes, we
provide a simple example to help you get started.  The rest of the
chapter considers the details of the API, how to tune and evaluate a
naive Bayes model, and how to use it in a semi-supervised setting with
small amounts of training



\section{Introduction to Naive Bayes}

The theory of naive Bayes classifiers is based on several fairly
restrictive assumptions about the classification problem.  This
section lays out the basics.

\subsection{Texts as Bags of Words}

Although naive Bayes may be applied to arbitrary multivariate count
data, LingPipe implements text classifiers, where the objects being
classified are implementations of Java's \code{CharSequence} interface
\eg{\code{String} or \code{StringBuilder}}.

For LingPipe's naive Bayes implementations, texts are represented as
so-called bags of words.%
%
\footnote{In general, these ``words'' will be arbitrary tokens, but
  ``bag of words'' is the usual terminology.  In statistical parlance,
  a bag corresponds to a multinomial outcome.}
%
LingPipe uses a tokenizer factory to convert a character sequence into
a sequence of tokens.  The order of these tokens doesn't matter for
naive Bayes or other classifiers that operate over bags of words.  As
we'll see later, the way in which a sequence of characters is
tokenized into sequences of strings plays a large role in
tuning a naive Bayes classifier effectively.

A bag of words is like a set of words in that the order doesn't
matter, but is unlike a set in that the count does matter.  For
instance, using a whitespace-based tokenizer, the strings
\stringmention{hee hee haw} and \stringmention{hee haw hee} produce
the same bag of words, namely \stringmention{hee} appears twice and
\stringmention{haw} once.  These strings produce a different bag of
words than \stringmention{hee haw}, which only has a count of one for
\stringmention{hee}.


\subsection{Exhaustivity and Exclusivity and Text Genre}

Naive Bayes classifiers require two or more categories into which
input texts are categorized.  These categories must be both exhaustive
and mutually exclusive.

For example, a news site such as Bing's or Google's, might classify a
news story as to whether it belongs in the category U.S., World,
Entertainment, Sci/Tech, Business, Politics, Sports, or Health.  As
another example, a research consortium might want to classify MEDLINE
citations mentioning mice as to whether they mention the effects of
any specific gene or not (the former class being useful for those
researching the genetic landscape of mice).  As a third example, a
marketing firm interested in a particular brand might classify a blog
post into four categories: positive toward brand, negative toward
brand, neutral toward brand, doesn't mention brand.  As a fourth
example, we could classify a patients discharge summaries (long texts
written by care givers) as to whether it indicates the patient is a
smoker or not.

Note that in each case, the texts under consideration were a
particular kind, such as newswire stories, MEDLINE citations about
mice, general blog posts, or patient discharge summaries.  We can
refer to the set of documents of this type as a genre or the domain of
the classifier.

The requirement of exhaustiveness is relative to texts that are drawn
from the genre under consideration.  We don't try to classify sports
stories as to whether they are about genomics or not or have a
positive sentiment toward the food quality.  

Often, this genre boundary can be moved by reconceptualizing the
classifier and training it on broader or narrower data types.  For
instance, the second example was restricted to MEDLINE citations about
mice, and doesn't consider full-length research articles or scientific
news stories, or even MEDLINE citations not about mice.  The third
example, in contrast, classifies all blog entries, but has a category
``doesn't mention brand'' to deal with posts not about the brand in
question.

In practice, classifiers may be applied to texts drawn from a
different genre from which they were trained.  For instance, we could
take blog sentiment classifiers and try to apply them to hotel
reviews.  Or we could apply MEDLINE citation classifiers to the full
texts of research articles.  In these cases, accuracy is almost always
worse on out-of-domain texts than in-domain texts.  For instance, we
could apply our blog sentiment classifier to product reviews in
magazines, but we would not expect it to work as well in that context
as for the kinds of blogs over which it was trained.  Similarly, we
could apply the mouse genetics classifier to full-length journal
articles, but it would likely not perform as well as for the citations
over which it was trained.

\subsection{Training Data: Natural versus Handmade}

To train naive Bayes classifiers and other supervised classifiers, we
require training data in the form of labeled instances.  In the case
of text classifiers, these consist of a sequence of texts paired with
their unique categories.  

For naive Bayes, and indeed for most statistical classifiers, the
training data should be drawn at random from the same distribution as
the test data will be drawn from.  Naive Bayes uses information about
which categories are most prevalent as well as what words are likely
to show up in which category.

For instance, we would create training data for news story section
headings by gathering news articles of the kind we'd like to classify
and assigning them to categories by hand. 

Sometimes we can find data already labeled for us.  For instance, we
could scrape a news aggregation or set of newspaper sites, recording
the category under which the article was listed (and perhaps
converting it back to the set of categories we care about).  These
might have originally arisen by hand labeling in the newspaper site,
but are most likely automatically generated by a news aggregator.
Similarly, we could examine the Medical Subject Heading (MeSH) tags
applied to MEDLINE citations by its curators to see if they were
marked as being about genomics and about mice.  

Sometimes we can gather data from less directly labeled sources.  For
instance, we can find positive and negative restaurant reviews by
examining how many stars a user assigned to them.  Or find blog posts
about food by creating a set of tags and searching for blogs with
those tags.  Or even by doing a web search with some terms chosen
for each category.

Our training data is almost always noisy, even if labeled by task
specialists by hand.  Naive Bayes is particularly robust to noisy
training data.  This produces a quality-quantity tradeoff when
creating the data.  High-quality data labeling is very labor
intensive.  Sometimes it is more effective to collect more
lower-quality data.  



\subsection{Generative Story}

Naive Bayes classifiers are based on a probabilistic model of a corpus
of texts with category-specific content.  Models like naive Bayes are
called ``generative'' in the machine learning literature because they
are richly specified enough to generate whole corpora.  In contrast,
classifiers like logistic regression are not generative in the sense
of being able to generate a corpus from the model.

The way in which naive Bayes represents a corpus, each document is
provided with a single category among a set of possible categories.
We suppose there is a fixed set of $K > 1$ categories and that each
document belongs to exactly one category.  To generate a document, we
first generate its category based on a probability distribution
telling us the prevalence of documents of each category in the
collection.

Then, given the category of a document, we generate the words in the
document according to a category-specific distribution over words.
The words are generated independently from one another given the
document's category.  The conditional independence of words given the
document category is almost always violated by natural language texts.
that is why the naive Bayes model is often erroneously called
``naive'' (the error is in labeling the model itself naive rather
than its application in a given setting).

Note that we do not model the selection of the number of topics $K$,
the number of documents $N$, or the number of words $M_n$ in the
$n$-th document; these are given as constants.  

\subsection{Priors and Posteriors}

Given a top-level distribution over categories along with a
distribution over words for each category, it is straightforward to
generate a corpus of documents.  In a Bayesian setting we can go one
step further and also generate the category prevalence distribution
and the category-specific distributions over words.  In order to do
so, we need to go up a level of abstraction and consider a probability
distribution over the parameters of a probabiltiy distribution.  Then,
we start by generating the prevalence distribution and the topic
distributions, then continue to generate the documents.  

Such a distribution over distributions is called a \techdef{prior} in
Bayesian statistics.  The reason such distributions are called priors
has to do with how they apply in inference.  The idea is that any
point in time, we have a distribution over distributions.  Then we
observe data and update our prior distribution with the observed data
to construct a posterior distribution.  The posterior distribution
combines the information in the prior with the information in the
data.  This process can be iterated, with the posterior aftering
seeing a first batch of data used as the prior for subsequent data.
This is, in fact, exactly what the naive Bayes implementation allows.
The implementation starts with a prior distribution over parameters,
then as it sees data, it updates the parameters of this prior
distribution to define the posterior distribution.  

In practice, we use a very simple kind of so-called \techdef{additive
  prior} that is both easy to understand and easy to calculate.
The prior is characterized by prior counts for data.  That is, we
start from a condition that is the same as if we've observed some
data.  For instance, Laplace suggested adding one to all counts.

Our counts for naive Bayes are of two forms.  The first is the
number of documents that have been seen of each category.  These
counts are used to estimate the prevalence of categories.  Our
prior might do something like add 1 to all of the counts.  

The second form of data is the number of times a token has been
observed in a document of a given category.  These counts are
used to estimate the probability of tokens being generated in
a document of a given category.  

\subsection{Maximum Likelihood Estimation}

Suppose we choose to add zero prior counts to all of our data.  The
result is that the parameters will all be set to their empirical
ratios.  Adding zero prior counts is, in this case, equivalent to
saying any distribution over categories or any distribution over the
words in a category is equally likely.  This leads to what is called
maximum likelihood estimates, as we will see later.


\section{Getting Started with Naive Bayes}\label{section:nb-getting-started}

LingPipe has two naive Bayes implementations.  In this section, we
focus on the traditional implementation of naive Bayes, which is found
in the LingPipe class \code{TradNaiveBayes} in the package
\code{com.aliasi.classify}.  

\subsection{Laugh Classification: His or Hers?}

Our example involves classifying laughs based on whether they were
produced by a man (his) or his wife (hers).  The manly laugh consists
of more ``haw'' and less ``hee.''  The training data we will use has
three examples of laughs from him and her.  His laughs are
\stringmention{haw}, \stringmention{haw hee haw}, and
\stringmention{haw haw}.  Her laughs in the training data are
\stringmention{haw hee}, \stringmention{hee hee hee haw}, and
\stringmention{haw}.  Note that the single word \stringmention{haw}
shows up as a laugh for her and for him.  Naive Bayes and all of our
other classifiers can handle this kind of ``inconsistent'' training
data, which is not actually inconsistent under a probabilistic model.
It's a matter of who's most likely to utter what, not that they
can't utter the same laughs.

\subsection{Setting up the Classifier}

The basic functionality of the naive Bayes classifier class can be
gleaned from a simple demo program which shows how the model is
trained and how it is run.  We provide such an example in the class
\code{TradNbDemo}, which consists of a single \code{main()} method.
The method starts by assigning the input arguments, in this case
a single argument representing the text to be classified.
%
\codeblock{TradNbDemo.1}

The next step is to set up the classifier itself, the
minimal constructor for which requires a tokenizer factory
and set of categories represented as strings.
%
\codeblock{TradNbDemo.2}
%
The regular expression \code{{\bk}P\{Z\}+} (see
\refsec{regex-unicode-classes}) produces a tokenizer factory that
defines tokens to be maximal sequences of characters which Unicode
considers not whitespace.  We have used the \code{asSet()} method in
LingPipe's \code{CollectionUtils} class to define a set of strings
consisting of the categories his and hers.  The classifier is
constructed using the categories and tokenizer factory.


\subsection{Providing Training Data}

At this point, we are ready to train the classifier using training
data.  For the demo, we just hard code the training data.  Each
training datum consists of an instance of the class
\code{Classified<CharSequence>}.  These training data are passed to
the classifier one at a time using its \code{handle()} method (see
\refsec{corpus-handlers} for a general overview of LingPipe handlers).
Because it implements \code{handle(Classified<CharSequence>)}, the
naive Bayes classifier class is able to implement the interface
\code{ObjectHandler<Classified<CharSequence>{}>}, which is convenient if
we want to supply the classifier as a callback to either a parser or a
corpus.

The order in which training data is provided to a naive
Bayes classifier is irrelevant, so we will provide all the
training data from the lady's laugh before the gentleman's.
%
\codeblock{TradNbDemo.3}
%
We start by creating a classification, \code{hersCl}, using the
category name literal \code{"hers"}.  A base LingPipe classification
extends the \code{Classification} class in the
\code{com.aliasi.classify} package.  These are used as the results of
classification.  Classifications are immutable, so they may be reused
for training, and thus we only have one.

After creating the classification, we create a list of training texts
using Java's built-in utility \code{asList()} from the \code{Arrays}
utility class.  Note that we used a list rather than a set because we
can train on the same item more than once.  For instance, the woman in
question may have hundreds of laughs in a training set with lots of
duplication.  Training on the same text again adds new information
about the proportion of laughs of different kinds. 

The final statement is a for-each loop, which iterates over the texts,
wraps them in an instance of \code{Classified<CharSequence>}, and
sends them to the classifier via its
\code{handle(Classified<CharSequence>)} method.  This is where the
actual learning takes place.  When the handle method is called, the
classifier tokenizes the text and keeps track of the counts of each
token it sees for each category, as well as the number of instances
of each category.  

We do the same thing for training the classifier for his laughs,
so there's no need to show that code.

\subsection{Performing Classification}

Once we've trained our classifier with examples of his laughs and
her laughs, we are ready to classify a new instance.  This is
done with a single call to the classifiers \code{classify(CharSequence)}
method.
%
\codeblock{TradNbDemo.4}
%
Note that the result is an instance of \code{JointClassification},
which is the richest classification result defined in LingPipe.  The
joint classification provides a ranking of results, and for each
provides the conditional probability of the category given the text as
well as the log (base 2) of the joint probability of the category and
the text.  These are pulled off by iterating over the ranks and
then pulling out the rank-specific values.  After that, we print them
in code that is not shown.

\subsection{Running the Demo}

The code is set up to be run from Ant using the target \code{nb-demo},
reading the value of property \code{text} for the text to be classified.
for instance, if we want to classify the laugh \stringmention{hee hee},
we would call it as follows.
%
\commandlinefollow{ant -Dtext="hee hee" nb-demo}
\begin{verbatim}
Input=|hee hee|
Rank= 0  cat=hers  p(c|txt)=0.87  log2 p(c,txt)= -2.66
Rank= 1  cat= his  p(c|txt)=0.13  log2 p(c,txt)= -5.44
\end{verbatim}
%
Our classifier estimates an 87\% chance that \stringmention{hee hee} was
her laugh and not his, and thus it is the top-ranked answer (note that
we count from 0, as usual).  We will not worry about the joint
probabilities for now.  

\subsection{Unknown Tokens}

The naive Bayes model as set up in LingPipe's
\code{TradNaiveBayesClassifier} class follows the standard practice
of ignoring all tokens that were not seen in the training data.  So
if we set the text to \stringmention{hee hee foo}, we get exactly
the same output, because the token \stringmention{foo} is simply
ignored.

This may be viewed as a defect of the generative model, because it
doesn't generate the entire text.  We have the same problem if the
tokenizer reduces, say by case normalizing or stemming or stoplisting
--- we lose the connection to the original text.  Another way of
thinking of naive Bayes is as classifying bags of tokens drawn
from a known set. 


\section{Independence, Overdispersion and Probability Attenuation}

Although naive Bayes returns probability estimates for categories
given texts, these probability estimates are typically very poorly
calibrated for all but the shortest texts.  The underlying problem is
the independence assumption underlying the naive Bayes model, which
is not satisfied with natural language text.  

A simple example should help.  Suppose we have newswire articles about
sports.  For instance, as I write this, the lead story on the {\it New
  York Times} sports page is about the player Derek Jeter's contract
negotiations with the New York Yankees baseball team.  This article
mentions the token \stringmention{Jeter} almost 20 times, or about
once every 100 words or so.  If the independence assumptions
underlying naive Bayes were correct, the odds against this happening
would be astronomical.  Yet this article is no different than many
other articles about sports, or about any other topic for that matter,
that focus on a small group of individuals.

In the naive Bayes model, the probability of seeing the word
\stringmention{Jeter} in a document is conditionally independent of
the other words in the document.  The words in a document are
not strictly independent of each other, but they are independent
of each other given the category of the document.  In other words,
the naive Bayes model assumes that in documents about sports,
the word \stringmention{Jeter} occurs at a constant rate.  In
reality, the term \stringmention{Jeter} occurs much more frequently
about baseball, particularly ones about the Yankees.

The failure of the independence assumption for naive Bayes manifests
itself in the form of inaccurate probability assignments.  Luckily,
naive Bayes classifiers work better for first-best classification than
one might expect given the violation of the assumptions on which the
model is based.  What happens is that the failed independence
assumption mainly disturbs the probabiilty assignments to different
categories given the texts, not the rankings of these categories.

Using our example from the previous section of the his and hers
laughter classifier, we can easily demonstrate the effect.  The
easiest way to see this is duplicating the input.  For instance,
consider classifying \stringmention{hee hee haw}, using
%
\commandlinefollow{ant -Dtext="hee hee haw" nb-demo}
\begin{verbatim}
Rank= 0  cat=hers  p(c|txt)=0.79
Rank= 1  cat= his  p(c|txt)=0.21
\end{verbatim}
%
If we simply double the input to \stringmention{hee hee haw hee hee haw},
note how the probability estimates become more extreme.
%
\commandlinefollow{ant -Dtext="hee hee haw hee hee haw" nb-demo}
\begin{verbatim}
Rank= 0  cat=hers  p(c|txt)=0.94
Rank= 1  cat= his  p(c|txt)=0.06
\end{verbatim}
%
Just by duplicating the text, our estimate of the laugh being hers
jumps from 0.79 to 0.94.  The same thing happens when Derek Jeter's
called out twenty times in a news story.

Because naive Bayes is using statistical inference, it is reasonable
for the category probability estimates to become more certain when
more data is observed.  The problem is just that certainty grows
exponentially with more data in naive Bayes, which is a bit too fast.
As a result, naive Bayes typically grossly overestimates or
underestimates probabilities of categories for documents.  And
the effect is almost always greater for longer documents.

\section{Tokens, Counts and Sufficient Statistics}

Document length per se is not itself a factor in naive Bayes models.
It only comes into play indirectly by adding more tokens.  In general,
there are only two pieces of information that the naive Bayes
classifier uses for training:
%
\begin{enumerate}
\item The bag of tokens for each category derived from combining all
  training examples, and
\item The number of training examples per category.
\end{enumerate}
%
As long as we hold the number of examples per category constant, we
can rearrange the positions of tokens in documents.  For instance,
we could replace his examples
%
\codeblock{FragmentsNb.1}
%
with
%
\codeblock{FragmentsNb.2}
%
of even
%
\codeblock{FragmentsNb.3}
%
with absolutely no effect the resulting classifier's behavior.  Neither
the order of tokens or their arrangement into documents is considered.

The latter example shows that the empty string is a perfectly good
training example; although it has no tokens (under most sensible
tokenizers anyway), it does provide an example of him laughing and ups
the overall probability of the laugher being him rather than her.
That is, the last sequence of three training example is not equivalent to
using a single example with the same tokens,
%
\codeblock{FragmentsNb.4}



\section{Unbalanced Category Probabilities}

In the simple example of laugh classification, we used the same number
of training examples for his laughs and her laughs.  The quantity
under consideration is the number of times \code{handle()} was
called, that is the total number of texts used to train each category,
not the number of tokens.

The naive Bayes classifier uses the information gained from the number
of training instances for each category.  If we know that she is more
likely to laugh than him, we can use that information to make better
predictions.  

In the example above, with balanced training sizes, if we provide no
input, the classifier is left with only prevalence to go by, and
returns a 50\% chance for him or her, because that was the balance of
laughs in the training set.  The following invocation uses a text
consisting of a single space (no tokens).%
%
\footnote{The program accepts the empty string, but Ant's notion of
  command-line arguments doesn't; empty string values for the \code{arg}
  element for the \code{java} task are just ignored, so that adding
  the line \code{<arg value=""/>} as an argument in the \code{java}
  element has no effect.}
%
\commandlinefollow{ant -Dtext=" " nb-demo}
\begin{verbatim}
Rank= 0  cat=hers  p(c|txt)=0.50
Rank= 1  cat= his  p(c|txt)=0.50
\end{verbatim}
%

Now let's consider what happens when we modify our earlier demo
to provide two training examples for her and three for him.  The
resulting code is in class \code{CatSkew}, which is identical
to our earlier example except for the data,
%
\codeblock{TradNbSkewedTrain.1}
%
\codeblock{TradNbSkewedTrain.2}
%
Note that we have used exactly the same tokens as the first
time to train each category.  


Now if we input a text with no tokens, we get a different estimate.
%
\commandlinefollow{ant -Dtext=" " cat-skew}
\begin{verbatim}
Rank= 0  cat= his  p(c|txt)=0.58
Rank= 1  cat=hers  p(c|txt)=0.42
\end{verbatim}
%
You might be wondering why the resulting estimate is only 58\% likely
to be his laugh when 60\% of the training examples were his.  The
reason has to do with smoothing, to which we turn in the next section.


\section{Maximum Likelihood Estimation and Smoothing}

At root, a naive Bayes classifier estimates two kinds of things.  First,
it estimates the probability of each category independently of any
tokens.  As we saw in the last section, this is carried out based on
the number of training examples presented for each category.

The second kind of estimation is carried out on a per-category basis.
For each category, the naive Bayes model provides an estimate of the
probability of seeing each token (in the training set) in that
category.

\subsection{Maximum Likelihood by Frequency}

These estimates are carried out by counting.  In the simplest case,
for category prevalence, if there are two categories, $A$ and $B$,
with three training instances for category $A$ and seven for category
$B$, then a simple estimate, called the maximum likelihood estimate,
can be derived based on relative frequency in the training data.  In
this case, the estimated probability of category $A$ is 0.3 and that
of category $B$ is 0.7.

The frequency-based estimate is called ``maximum likelihood'' because
it assigns the probability that provides the highest probability
estimate for the data that's seen.  If we have three instance of
category $A$ and seven of category $B$, the maximum likelihood
estimate is 0.3 for $A$ and 0.7 for category $B$.  The same thing
holds, namely that maximum likelihood probability estimates are
proportional to frequency, in the situation where there are more than
two categories.

The problem with simple frequency-based estimates is that they are not
very robust for large numbers of words with limited training data,
which is just what we find in language.  For instance, if we had
training for him and her that looked as follows,
%
\codeblock{FragmentsNb.5}
%
then the probability assigned to her uttering \stringmention{har} or
him uttering \stringmention{tee} would be zero.  This leads to a
particularly troubling situation when classifying an utterance such as
\stringmention{har tee har}, which contains both strings.  Such a
string would be impossible in the model, because he is assigned zero
probability of uttering \stringmention{tee} and she's assigned zero
probability of uttering \stringmention{har}.  This becomes a huge problem
when we're dealing with vocabularies of thousands or millions of 
possible tokens, many of which are seen only a handful of times in
a few categories.  

\subsection{Prior Counts for Smoothed Estimates}

To get around the zero-probability estimates arising from maximum
likelihood, the usual approach is to smooth these estimates.  The
simplest way to do this is to start all of the counts at a (typically
small) positive initial value.  This leads to what is known as {\it
  additive smoothing} estimates and the amount added is
called the {\it prior count}.%
%
\footnote{In statistical terms, additive smoothing produces the
  maximum a posteriori (MAP) parameter estimate given a symmetric
  Dirichlet prior with parameter value $\alpha$ equal to the prior
  count plus 1.}
%

There is a second constructor for \code{TradNaiveBayesClassifier} that
allows the prior counts to be specified, as well as a length normalization,
which we currently set to \code{Double.NaN} in order to turn off that
feature (see \refsec{naive-bayes-length-norm} for more information on
length normalization).  

An example of the use of this constructor is provided in the
class \code{AdditiveSmooth} in this chapter's package.  It consists
of a static main method, which begins as follows.
%
\codeblock{AdditiveSmooth.1}
%
Here we have defined the set of categories to be \stringmention{hot}
and \stringmention{cold}, used a whitespace-breaking tokenizer
factory, and set the category prior count to 1.0 and the token prior
count to 0.5.%
%
\footnote{It may appear odd that we are allowing ``counts'' to take on
  fractional values.  This is unproblematic because all of our
  estimates are derived from ratios.  On the other hands, counts will
  never be allowed to be negative.}
%
This will have the effect of adding 0.5 to the count of all tokens and
1.0 to the count of all categories.

Next, consider training three hot examples and two cold examples.
%
\codeblock{AdditiveSmooth.2}
%
There is a total of 7 different tokens in these five training items,
\stringmention{super}, \stringmention{steamy}, \stringmention{out},
\stringmention{boiling}, \stringmention{today},
\stringmention{freezing}, and \stringmention{icy}.  Only the token
\stringmention{out} appears in both a hot training example and a cold one.
All other tokens appear once, except \stringmention{steamy},
which appears twice in the hot category.

After building the model, the code iterates over the categories and
tokens printing probabilities.  First, the category probabilities
are computed; for generality, the code uses the method \code{categorySet()}
on the classifier to retrieve the categories.
%
\codeblock{AdditiveSmooth.3}
%
The method \code{probCat()} retrieves the probability of a category.

The next code block just iterates over the tokens using the
traditional naive Bayes classifier method \code{knownTokenSet()},
which contains all the tokens in the training data.  This is
an unmodifiable view of the actual token set.  Testing whether
a token is known may be carried out directly using the method
\code{isKnownToken(String)}.

Within the loop, it again iterates over the categories.  This time,
the traditional naive Bayes classifier method \code{probToken()}, with
arguments for a token and category, calculates the estimated
probability that any given token in a document of the specified
category is the specified token.
%
\codeblock{AdditiveSmooth.4}


\subsubsection{Running the Demo}

There is an ant target \code{additive-smooth} which calls the 
command.

\commandlinefollow{ant additive-smooth}
\begin{verbatim}
p(cold)=0.429    p(hot)=0.571

p(   super|cold)=0.077    p(   super| hot)=0.158
p(     icy|cold)=0.231    p(     icy| hot)=0.053
p( boiling|cold)=0.077    p( boiling| hot)=0.158
p(  steamy|cold)=0.077    p(  steamy| hot)=0.263
p(   today|cold)=0.077    p(   today| hot)=0.158
p(freezing|cold)=0.231    p(freezing| hot)=0.053
p(     out|cold)=0.231    p(     out| hot)=0.158
\end{verbatim}
%
At the top, we see the probabilities of the categories
\stringmention{cold} and \stringmention{hot} before seeing any data.
Note that these two values sums to 1.  Below that, columns display the
estimated probability of each token given the category, hot or cold.
For each category, \stringmention{hot} and \stringmention{cold}, 
the sum of the probabilities of all tokens is also 1.  

Each of these lines corresponds to a parameter in the model.  Thus
this model has 16 parameters.  We only need to specify 13 of them,
though.  Given $p(\stringmention{cold})$, we know that
$p(\stringmention{hot}) = 1 - p(\stringmention{cold})$.  Furthermore,
we know that $p(\stringmention{out}|\stringmention{cold})$ is one
minus the sum of the probabilities of the other tokens, and similarly
for $p(\stringmention{out}|\stringmention{hot})$.

Let's look at some particular estimates, focusing on the hot category.
Given this training data, a token in a statement expressing hot
weather is 5\% or so likely to be \stringmention{icy} and 26\% likely
to be \stringmention{steamy}.  The token \stringmention{icy} was
not in the training data for the hot category, whereas the token
\stringmention{steamy} appeared twice.  

With additively smoothed estimates, the probability assigned to a
given token is proportional to that token's count in the training data
plus its prior count.  Looking at our examples, these are 2.5 for
\stringmention{steamy} (count of 2 plus 0.5 prior count), 0.5 for
\stringmention{icy} and \stringmention{freezing}, and 1.5 for the
other four tokens.  Thus the total effective count is $1 \times 2.5 +
4 \times 1.5 + 2 \times 0.5 = 9.5$, and the probability for
\stringmention{steamy} is $2.5/9.5 \approx 0.263$.  Given this
estimation method, the sum of the token probabilities is guaranteed to
be 1.

The probabilities for categories are calculated in the same way.  That
is, the overall count of hot instances was 3, whereas the count of
cold training instances was 2.  The prior count for categories was
1.0, so the effective count for hot is 4 and for cold is 3, so the
estimated probability of a hot category is $4/7 \approx 0.571$.  As
with the tokens, the category probabilities will sum to 1, which is as
it should be with the interpretation of the categories as exclusive
and exhaustive.



\section{Item-Weighted Training}

Having seen in the last section how prior counts and training examples
determine estimates for two kinds of probabilities.  First, the probability
of a category, and second, the probability of a token in a message of
a given category.  In each case, we added one to the count of categories
or tokens for each relevant item in the training set.

There are two situations where it is helpful to weight the training
examples non-uniformly.  The first situation arises when some examples
are more important than others.  These can be given higher weights.
The second situation arises when there is uncertainty in the category
assignments.  For instance, we might have a string and be only 90\%
sure it's about hot weather.  In this case, we can train the hot
catgory with a count of 0.9 and the cold category with a count of 0.1
for the same training example.  This latter form of training is
particularly useful for semi-supervised learning, which we consider
below.  For now, we will just describe the mechanisms for carrying it
out and see what actually happens.

We provide an example in the class \code{ItemWeighting} in this
chapter's package.  For the most part, it is the same as the
\code{AdditiveSmooth} class in the way it sets up the classifier and
prints out the probability estimates.  The difference is that it weights
training examples rather than using the default weight of 1.0.
%
\codeblock{ItemWeighting.1}
%
We've used a smaller example set to focus on the effect of weighting.

There is an ant target \code{item-weight} that prints out the
resulting estimates, which are as follows.

\commandlinefollow{ant item-weight}
\begin{verbatim}
p(cold)=0.536    p(hot)=0.464

p( boiling|cold)=0.098    p( boiling| hot)=0.333
p(  warmly|cold)=0.255    p(  warmly| hot)=0.112
p(   dress|cold)=0.255    p(   dress| hot)=0.112
p(    mild|cold)=0.196    p(    mild| hot)=0.112
p(     out|cold)=0.196    p(     out| hot)=0.333
\end{verbatim}
%
There are two cold training instances trained with weights of 0.8 and
0.5.  Now rather than adding 1 to the category count for each
instance, we add its weight, getting a total observed weight of 1.3.
With a prior category count of 1.0, the efective count for the cold
category is 2.3.  There is one training instance for the hot category,
trained with a weight of 0.99, so the effective hot category count is
1.99, and the resulting probability of the cold category is
$2.3/(2.3+1.99) \approx 0.536$.

The tokens get weighted in the same way.  For instance, dress is
weighted 0.8, and there is a prior count of 0.5 for tokens, so the
effective count of \stringmention{dress} in the cold category is 1.3;
its effective count in the hot category is just the prior count, 0.5.
The token \stringmention{out} is weighted 1.0 in the cold category and
1.49 in the hot category.  The total count for cold tokens corresponds
to all counts and priors, with effective counts of 1.3 for
\stringmention{dress}, 1.3 for \stringmention{warmly}, 1.0 for
\stringmention{mild} and 1.0 for \stringmention{cold}.  There is also
a count of 0.5 for \stringmention{boiling}, even though it was never
used in a cold example.  Thus the probability estimate for
\stringmention{warmly} is $1.3/(1.3 + 1.3 + 1.0 + 1.0 + 0.5) \approx
0.255$.

\subsection{Training with Conditional Classifications}

Because it can train with weighted examples, the traditional naive
Bayes classifier supports a method to train using a conditional
classification.  The method \code{trainConditional()} takes four
arguments, a \code{CharSequence} for the input text, a
\code{ConditionalClassification} for the result, as well as a count
multiplier and minimum category virtual count, the effect of which we
describe below. This result is the same as if weighted training had
been carried out for every category with weight equal to the
conditional probability of the category in the classification times
the count multiplier.  If the virtual count consisting of the
probability times the count multiplier is less than the minimum
virtual count, the category is not trained.


\section{Document Length Normalization}\label{section:naive-bayes-length-norm}

It is common in applications of naive Bayes classification to mitigate
the attenuating effect of document length and correlation among the
tokens by treating each document as if it were the same length.  This
can result in better probabilistic predictions by reducing the
tendency of conditional probability estimates for categories to be
zero or one.  Length normalization does not affect the ranking of
categories for a single document, and thus returns the same first-best
guess as to category for a document.  Even though it doesn't change
rankings, length normalization does rescale the probabilty estimates
of categories given text.  This can lead to better calibration of
probabilities across documents, by making the probability estimates
less dependent on document length and token correlation.

Document length normalization works by multiplying the weight of each
token in the document by the actual document length divided by the
length norm.  Thus if the document is ten tokens long and the length
normalization is four tokens, each token of the ten in the input
counts as if it were only 0.4 tokens, for a total length of four
tokens.  If the length norm is four tokens and the input is two tokens
long, each token counts as if it were two tokens.  If the numbers are
round, the effects will be the same as removing that many tokens.  For
instance, if the input is \stringmention{hee hee haw haw haw haw} and
the length normalization is 3, the result is the same as presenting
\stringmention{hee haw haw} to an un-length-normalized naive Bayes
instance.

As we will see below in an example, length normalization applies
before unknown tokens are discarded.  Thus the behavior before and
after length normalization is different if there are unknown tokens
present.  The decision to implement things this way is rather
arbitrary; the model would be coherent under the decision to throw
away unknown tokens before rather than after computing length.  As
implemented in LingPipe, unknown tokens increase uncertainty, but
do not change the relative ranking of documents.  

Because of the multiplicative nature of probability, overall
scores are geometric averages, so that the reweighting takes place
in the exponent, not as a simple multiplicative factor.


\subsection{Demo: Length Normalization}

\subsubsection{Setting Length Normalization}

The length normalization value, which is the effective length used for
all documents, may be set in the constructor or using the method
\code{setLengthNorm(double)}.  The current value of the length norm is
oreturned by \code{lengthNorm()}.  Length norms cannot be negative,
zero, or infinite.  Setting the length norm to \code{Double.NaN} turns
off length normalization.

The class \code{LengthNorm} in this chapter's package trains exactly
the same model using exactly the same data as in our first example in
\refsec{nb-getting-started}.  To repeat, his laughs are trained on
three examples, \stringmention{haw}, \stringmention{haw hee haw}, and
\stringmention{haw haw}, and hers are trained on three instances,
\stringmention{haw hee}, \stringmention{hee hee hee haw}, and
\stringmention{haw}.  

The length normalization is read in as the second command-line
argument after the text to classify, and parsed into the \code{double}
variable \code{lengthNorm}.  The length norm for the classifier is
then set with the \code{setLengthNorm()} method.  
%
\codeblock{LengthNorm.1}
%
Note that we set the value before running classification.  It
doesn't matter when the length norm is set before classification.
We could have also set it in the constructor.

\subsubsection{Running Examples}

The Ant target \code{length-norm} runs the demo, with the text being
supplied through property \code{text} and the length norm through
property \code{length.norm}.  For example, we can evaluate a length
norm of 3.0 tokens as follows (with the rank 1 results elided):
%
\commandlinefollow{ant -Dtext="hee hee haw" -Dlength.norm=3.0 length-norm}
\begin{verbatim}
Input=|hee hee haw|   Length Norm=   3.00
Rank= 0  cat=hers  p(c|txt)=0.79  log2 p(c,txt)= -3.85
\end{verbatim}
%
Unlike the situation without length normalization, the number of
times the input is repeated no longer matters.  If we repeat it twice,
we get the same result.
%
\commandlinefollow{ant -Dtext="hee hee haw hee hee haw" -Dlength.norm=3.0 length-norm}
\begin{verbatim}
Input=|hee hee haw hee hee haw|   Length Norm=   3.00
Rank= 0  cat=hers  p(c|txt)=0.79  log2 p(c,txt)= -3.85
\end{verbatim}

We can also see that with length normalization, unknown tokens
are included in the length normalization.  For example, consider
the following, which adds two unknown tokens to the input.
%
\commandlinefollow{ant -Dtext="hee hee haw foo bar" -Dlength.norm=3.0 length-norm}
\begin{verbatim}
Input=|hee hee haw foo bar|   Length Norm=   3.00
Rank= 0  cat=hers  p(c|txt)=0.69  log2 p(c,txt)= -2.71
\end{verbatim}
%
With length normalization, the presence of unknown tokens drive
the probability estimates closer to 0.5 because they participate
in the length calculation but are not discriminative.  

Providing a length-norm value of \code{Double.NaN} turns off
length normalization.  For the input of \stringmention{hee hee haw},
this provides the same result as setting length normalization to
3.0.
%
\commandlinefollow{ant -Dtext="hee hee haw" -Dlength.norm=NaN length-norm}
\begin{verbatim}
Input=|hee hee haw|   Length Norm=    NaN
Rank= 0  cat=hers  p(c|txt)=0.79  log2 p(c,txt)= -3.85
\end{verbatim}
%
But now if we duplicate the document, we see the attenuating
effect of length again.
%
\commandlinefollow{ant -Dtext="hee hee haw hee hee haw" -Dlength.norm=NaN length-norm}
\begin{verbatim}
Input=|hee hee haw hee hee haw|   Length Norm=    NaN
Rank= 0  cat=hers  p(c|txt)=0.94  log2 p(c,txt)= -6.71
\end{verbatim}



\section{Serialization and Compilation}

The traditional naive Bayes classifier class implements both Java's
\code{Serializable} interface and LingPipe's \code{Compilable}
interface.  See \refsec{io-object-data-io} for object reading and
writing, including serializability, and \refsec{io-compilable} for a
general description of LingPipe's compilation interface.

Serializing a traditional naive Bayes classifier and reading it
back in results in an instance of \code{TradNaiveBayesClassifier} in
exactly the same state as the serialized classifier.  In particular,
a deserialized traditional naive Bayes classifier may be trained
with further examples.  

For more efficiency, a traditional naive Bayes classifier may be
compiled.  Among other optimizations, all the logarithms required
to prevent underflows and convert multiplications to additions
are precomputed.  

Compiling a traditional naive Bayes classifier and reading it back in
results in an implementation of \code{JointClassifier<CharSequence>}
if there are more than two categories.  If there are only two
categories, the class read back in will only implement the interface
\code{ConditionalClassifier<CharSequence>}.  The reason for this is
that the binary (two-category) case is heavily optimized for computing
the conditional probabilities and adding in the data to compute the
joint probabilities would double the size and halve the speed of
binary compiled classifiers.

In order to serialize a traditional naive Bayes classifier, its
tokenizer factory must be serializable.  For compilation, the
tokenizer factory must be either compilable or serializable.  If
the tokenizer factory implements LingPipe's \code{Compilable} interface
it'll be compiled, otherwise it'll be serialized.%
%
\footnote{This is carried out with LingPipe's static utility method
  \code{compileOrSerialize()}, which may be found in the
  \code{AbstractExternalizable} class in the \code{com.aliasi.util}
  package.}

\subsection{Serialization and Compilation Demo}

The demo class \code{SerializeCompile} in this chapter's package
illustrates both serialization and compilation.  Because
deserialization in java may throw class-not-found and I/O exceptions,
the main method starts off declaring that.
%
\codeblock{SerializeCompile.1}

The first two command-line arguments are for the text to be classified
and a (temporary) file name into which the classifier is serialized
(or compiled).

After constructing and training with the same code as in the first
example in \refsec{nb-getting-started}, compiling a naive Bayes
classifier to a file and reading it back in may be accomplished as
follows.
%
\codeblock{SerializeCompile.2}
%
The static utility method \code{compileTo()} from Java's class
\code{AbstractExternalizable} class (in the \code{util} package) is
used to do the writing.  This could also be done through LingPipe's
\code{Compilable} interface directly using the traditional naive Bayes
class's method \code{compileTo(ObjectOut)}.  We deserialized using
another utility method, \code{readObject()}, which reads and returns
serialized objects from files (there are also utility methods to read
compiled or serialized models from the class path as resources).

Usually, one program would write the compiled file and another
program, perhaps on another machine or at a different site, would read
it.  Here, we have put the two operations together for reference.
Note that the unchecked cast warning on reading back in is suppressed;
an error may still result at runtime from the cast if the file
supplied to read back in has an object of a different type or isn't a
serialized object at all. Note that when read back in, it is
assigned to a \code{JointClassifier<CharSequence>}; attempting to
cast to a \code{TradNaiveBayesClassifier} would fail, as the compiled
version is not an instance of that class.

We do the same thing for serialization, using a different utility method
to serialize, but the same \code{readObject()} method to deserialize. 
%
\codeblock{SerializeCompile.3}
%
Here, we are able to cast the deserialized object back to the original
class, \code{TradNaiveBayesClassifier}.  

Because deserialization results in a traditional naive Bayes
classifier, we may provide more training data.  Repeating the
serialization and deserialization with a different variable,
we can go on to train.
%
\codeblock{SerializeCompile.4}



\subsubsection{Running the Demo}

The Ant target \code{serialize-compile} runs the class, classifying
a text supplied as property \code{text} and using a file derived
from the value of the property \code{file}.  Running it shows
the result of classifying with the original, compiled, deserialized,
and then deserialized and further trained classifier.

\commandlinefollow{ant -Dtext="hee haw hee hee" serialize-compile}
\begin{verbatim}
text=hee haw hee hee

Results for: Original
Rank= 0  cat=hers  p(c|txt)=0.91

Results for: Compiled
Rank= 0  cat=hers  p(c|txt)=0.91

Results for: Serialized
Rank= 0  cat=hers  p(c|txt)=0.91

Results for: Serialized, Additional Training
Rank= 0  cat=hers  p(c|txt)=0.97
\end{verbatim}

Note that the conditional probability assigned to her laughing are the
same for the original, compiled, and serialized model.  The estimate
is different for the model with additional training data, as we
would expect.  We have not reported log joint probabilities because
they are not produced by the compiled model.


\section{Training and Testing with a Corpus}

In this section, we show how to use a \code{Corpus} implementation to
train and test a classifier.  In our demo of LingPipe's \code{Corpus}
abstract base class in \refsec{corpus-20-newsgroups-corpus}, we showed
how to create a corpus from the 20 Newsgroups data set (the data
and download location are described in \refsec{corpora-20-newsgroups}).

We will re-use our corpus code in this demo, though we have copied the
implementation so as to not make the imports in the demos too
confusing.  Thus you'll find the implementation in
\code{TwentyNewsgroupsCorpus} in this chapter's demo package
(\code{com.lingpipe.book.naivebayes}).

\subsection{Training Naive Bayes with a Corpus}

Training a classifier like naive Bayes with a corpus is particularly
straightforward.  As usual, the demo code lives in the \code{main()}
method and starts by marshalling the command-line arguments, here just
the name of the tar gzipped file in which the corpus was saved as
the \code{File} variable \code{corpusTgzFile}.  We
then just use the constructor to create the corpus. 
%
\codeblock{Nb20NewsCorpus.1}
%
Given the corpus, we apply the static method \code{getCatSet()} to
extract the categories from the corpus.  The set is then used to generate
an array of categories, which is then sorted.

The static utility category extraction method is defined using an
anonymous class used as a callback, as follows.
%
\codeblock{Nb20NewsCorpus.0}
%
The set is declared to be final so that it can be used within the
anonymous handler's method.  The corpus method \code{visitCorpus()} is
then applied to a newly defined object handler instance; in general,
the \code{visitCorpus()} method sends all of the classified objects in
the corpus to the specified handler.  In this case, our handler is
anonymous and handles a classified object by adding its
classification's first best category assignment to the category set.
The generic \code{<T>} is being used for the type of object being
classified, which doesn't matter here, but must be kept consistent
across the corpus and the inner class handling the callbacks.

The next block of code creates a fully parameterized traditional
naive Bayes classifier.
%
\codeblock{Nb20NewsCorpus.2}
%
LingPipe's built-in Indo-Euroepan tokenizer factory singleton is used
for tokenization (see \refchap{tokenization} for more on tokenization
in general and \refsec{tok-indo-euro} for more information on
LingPipe's Indo-European tokenizer factory).  Further note that the
category prior count is set to 1.0 and the token count prior to 0.1,
and the length normalization is set to 10.0; these were explained
earlier in this chapter.

After setting up the classifier, the corpus's \code{visitTrain()}
method is used to send all of the training instances to the
classifier instance.  

After visiting the training data, the classifier's ready to use.
Instead of using it directly, we compile it to a more efficient form.
This is done with the utility method \code{compile()} in LingPipe's
\code{AbstractExternalizable} class (see \refsec{io-compilable} for
more information on LingPipe's general compilation interface).  The
required cast may throw I/O exceptions and the implicit
deserialization may throw a class-not-found exception, so those are
declared on the top-level \code{main()} method.  Note that we have
assigned the compiled object to a more general type,
\code{ConditionalClassifier}.  That's because the compiled form of the
traditional naive Bayes classifiers are not instances of
\code{TradNaiveBayesClassifier}.

\subsection{Evaluation with a Corpus}\label{section:naive-bayes-corpus-eval}

Because our corpus was built with a training and test section
according to the way the 20 Newsgroup corpus is distributed, we can
also use the corpus for testing.  The first step is to set up the
evaluator.  
%
\codeblock{Nb20NewsCorpus.4}
%
We use an evaluator for conditional classifications, which
we construct with the compiled classifier, the array of categories, and
a flag indicating whether or not to store inputs, which we set to
\code{false} here.  If you want to inspect the true positives, false
positives, etc., you need to set this flag to \code{true}.  

After setting up the classifier evaluator, we call the corpus
method \code{visitTest()} to supply the test cases to the 
evaluator's \code{handle()} method.  

Rather than just printing the evaluator, which provides a very verbose
dump, we have focused on the most common evaluation metrics for
all-versus-all and one-versus-all evaluations.  See
\refchap{classifier-evaluation} for a complete description of
available classifier evaluations.

\subsubsection{All-Versus-All Results}

The first thing we do is extract the results for accuracy,
and macro-averaged precision, recall, F-measure, and kappa
for the overall system
%
\codeblock{Nb20NewsCorpus.5}
%
These are then printed using code not displayed here.  See
\refsec{classifier-eval-macro-avg} for more information on
interpreting macro averaged results; they're essentially averages
of the one-versus-all results, to which we turn next.


\subsubsection{One-Versus-All Results}

In one versus all evaluations, we treat the twenty newsgroup
categories as being decomposed into twenty one-versus-all evaluations.
In these cases, we treat each problem as a binary classification problem.
See \refsec{classifier-eval-one-versus-all} for more information on
one-versus-all evaluations.  In code, we just loop over the
categories, extracting the one-versus-all evaluations as
precision-recall evaluation objects.
%
\codeblock{Nb20NewsCorpus.6}
%
From these, we pull out the true and false positives and negative
counts, overall accuracy, along with precision, recall (aka sensitivity),
specificity (aka rejection recall), and F measure.  These are then
printed in code not shown.

Further, for each category, we print out the non-zero cells of
the confusion matrix itself.  These indicate counts for the
number of times a document of a given category was classified
as something else, and vice-versa, though we only show the first
case.
%
\codeblock{Nb20NewsCorpus.7}
%
The confusion matrix as an integer array was created outside of the
loop as part of the last code block using the \code{matrix()} method
from the confusion matrix class.  The entries \code{matrix[k1][k2]}
indicate the number of times a document with true category \code{k1}
was misclassified as being of category \code{k2}.

\subsection{Running the Demo}

There's an Ant target \code{20-news-corpus} that runs the demo.
It reads the name of the tar gzipped data file from
the property \code{20-news-file}, which we specify here
on the command line.
%
\commandline{ant -D20-news-file=../../data-dist/20news-bydate.tar.gz 20-news-corpus}
%
The first thing it prints out after it finishes is the confusion
matrix accuracies for all-versus-all:
%
\begin{verbatim}
All Versus All
  correct/total = 5638 / 7532
  accuracy=0.749
  Macro Avg prec=0.785  rec=0.728   F=0.716
  Kappa=0.735
\end{verbatim}
%
After this, it prints the one-versus-all results.  For instance,
consider the fourth category, \stringmention{talk.politics.mideast},
for which we see the following results.
%
\begin{verbatim}
Category[17]=talk.politics.mideast versus All
  TP=313 TN=7137 FP=19 FN=63
  Accuracy=0.989
  Prec=0.943  Rec(Sens)=0.832  Spec=0.997  F=0.884
    * => alt.atheism : 6
    * => comp.graphics : 1
    * => comp.windows.x : 5
    * => misc.forsale : 1
    * => rec.autos : 1
    * => rec.motorcycles : 2
    * => rec.sport.baseball : 5
    * => rec.sport.hockey : 2
    * => sci.electronics : 3
    * => sci.space : 1
    * => soc.religion.christian : 34
    * => talk.politics.guns : 2
    * => talk.politics.mideast : 313
    alt.atheism => * : 5
    comp.graphics => * : 1
    sci.med => * : 2
    talk.politics.guns => * : 2
    talk.politics.misc => * : 5
    talk.religion.misc => * : 4
\end{verbatim}
%
After dumping the number true and false positive and negatives, it
prints the overall accuracy, which is 98.9\% in this case.  The other
one-versus-all evaluations have similarly high accuracies in the high
90\% range.  But overall accuracy was only around 75\%.  The reason
for this is that any negative is counted as correct, with overall
accuracy being the number of true positives plus true negatives
divided by the total cases.  The true negatives dominate these
numbers, and aren't sensitive to getting the correct category as long
as it's not the category being evaluated one versus all.  This is
reflected in relatively high precision and specificity values (94.3\%
and 99.7\% here), but lower recall values (83.2\% here).

The remaining lines indicate the actual confusions, with an asterisk
replacing the category being evaluated.  For instance, six instances
of documents which were in the \stringmention{talk.politics.mideast}
category were mistakenly classified as being of category
\stringmention{alt.atheism}.  Similarly, there were five cases of
documents whose true category was \stringmention{sci.med} that were mistakenly
classified as \stringmention{talk.politics.mideast}.


\section{Cross-Validating a Classifier}\label{section:naive-bayes-xval}

Our next demonstration is of cross validation, which were introduced in
\refsec{corpus-xval}.  

\subsection{Implementing the Cross-Validating Corpus}

The implementation of the cross-validating corpus for the Twenty
Newsgroups data is in the class \code{TwentyNewsXvalCorpus} in this
chapter's package, \code{naivebayes}.  For the implementation, we
extend LingPipe's \code{XValidatingObjectCorpus} class.%
%
\footnote{In general, cross-validating object corpora may be used
  directly without. An equally perspicuous approach for this example,
  in which the extension defines no new methods, would be to
  define a static factory method that returned an instance of
  \code{XValidatingObjectCorpus}.}
%
\codeblock{TwentyNewsXvalCorpus.1}
%
Note that we have supplied the generic argument
\code{Classified<CharSequence>}, which is the type of classifications
making up the training and evaluation data.  We have a simple
constructor taking the name of the gzipped tar file in which the
corpus may be found as well as the number of folds for cross
validation.  The number of folds is used in the explicit call to
\code{super(int)}, which is required because the only constructor for
\code{XValidatingObjectCorpus} requires an integer argument specifying
the number of folds.  The constructor is declared to throw an I/O
exception if it has trouble reading the corpus.%
%
\footnote{As tempting as it may be to capture I/O exceptions locally
  and ignore them, we believe it is better to flag them in static
  corpora like Twenty Newsgroups so that parsing errors in the corpus
  may be found.  This also makes coding easier, as the exceptions are
  simply propagated. In a production system, the marshalling of data
  would need to be made more robust.}

The actual parser has not been abstracted, but rather is copied almost
verbatim from the example in \refsec{corpora-20-newsgroups}.  The
main action is in the loop driving over the entries in the tar input
stream.
%
\codeblock{TwentyNewsXvalCorpus.2}
%
Within the block, we repeat the parsing code from
\refsec{corpora-20-newsgroups} to pull out the data (not shown here),
until we can retrieve the text and classification.  
%
\codeblock{TwentyNewsXvalCorpus.3}
%
The difference from the previous usage is that we call the
cross-validating corpus method \code{handle(E)}, where \code{E} is the
generic type of the corpus, here \code{Classified<CharSequence>}.  The
implementation is inherited from the superclass.  This call to the
handle method adds the classified character sequence to the corpus.
When we are done, we close the tar input stream, and are ready to go.


\subsection{Implementing the Evaluation Driver}

Now that we have a cross-validating corpus, we can see how to use it
to do a cross-validating evaluation.  The example is implemented in
\code{main()} method of class \code{TwentyNewsXvalDemo} in this
chapter's package, \code{naivebayes}.  As usual, we start the
main method by reading command-line arguments.  
%
\codeblock{TwentyNewsXvalDemo.1}
%
As is usual in commands to do cross-validation, we have an argument
for the data file.  In addition, we have command-line arguments for
the parameters we wish to tune, here the the category prior, the token
prior, the length normalization.  Finally, we have a parameter that
controls the randomization used for cross-validation itself.  

Because we will reuse it, we also declare the tokenizer factory (here,
it's really just a local variable shortcut to save space).
%
\codeblock{TwentyNewsXvalDemo.2}
%

Next, we create the corpus itself using its constructor.
%
\codeblock{TwentyNewsXvalDemo.3}
%
After it is created, we permute using a randomizer based on the seed
number supplied as a command-line argument.  Then we extract the set
of categories from the corpus using the static utility method, as
previously shown in \refsec{naive-bayes-corpus-eval}.

We're finally ready to create the evaluator.  For cross-validation,
it's typical to have an overall evaluation, which is used for
every fold.  Here is how it's constructed.
%
\codeblock{TwentyNewsXvalDemo.4}
%
Note that we supplied a \code{null} classifier argument.  We will
set the classifier for the evaluation once we have trained it
within a fold.

We then set up a loop over the folds.
%
\codeblock{TwentyNewsXvalDemo.5}
%
It loops over the folds (counting from zero).  Within the loop, it
sets the fold on the corpus using the \code{setFold(int)} method.

Then, it constructs a new classifier instance using the parameters
read from the command line.  Note how this is done within the loop
body.  If we had constructed a classifier on the outside, it would've
been reused within the loop, with the result being that we'd be
testing on training data as we went along.

After setting up the classifier, we call the corpus's
\code{visitTrain()} method, supplying the new classifier instance as
an argument.  This causes all the training items in the corpus to be
supplied to the classifier's \code{handle(Classified<CharSequence>)}
method.

We next compile the classifier and use it for evaluation.
%
\codeblock{TwentyNewsXvalDemo.6}
%
The compilation is only for efficiency, and is carried out using a
static utility method in LingPipe's \code{AbstractExternalizable}
class.

Next, we set the global classifier evaluator's classifier to be the
result of compiling our trained naive Bayes classifier.  We then call
the corpus's \code{visitTest()} method to send all the test instances
to the evaluator's \code{handle(Classified<CharSequence>)} method.
As a result, the global evaluator will have the evaluation statistics
for all of the folds in a single evaluation.  

After updating our global evaluation, we create a local evaluator
that we will use just for the current fold.  It's set up just like
the global evaluator and invoked in the same way using the corpus's
\code{visitTest()} method.  The code to print the local evaluations
per fold and the global evaluation in the end are not shown.

\subsection{Cross Validating}

There are two purposes of cross validation.  One is to use as much
evaluation data as possible to get a tighter read on the system's
performance.  The second uses the first to tune the parameters of the
system.  Because the same data is continually being reused,
cross-validated performance tends to be better than held out
performance on new data.

\subsubsection{Running the Demo}

The Ant target \code{ 20-news-xval} is setup up to run the
\code{main()} method of \code{TwentyNewsXvalDemo}.  The Ant target
passes the values of these properties to the command.  Here's
an example illustrating all of the arguments.
%
\commandlinefollow{ant -Dcat-prior=2.0 -Dtok-prior=0.1 -Dlength-norm=10.0 -Drandom-seed=85 20-news-xval}
\begin{verbatim}
  20-news-file=C:\lpb\data-dist\20news-bydate.tar.gz
  cat-prior=2.0
  tok-prior=0.1
  length-norm=10.0
  random-seed=85
  tok-fact=com.aliasi.tokenizer.IndoEuropeanTokenizerFactory

FOLD 0  totalCount=  1884   acc=0.825   conf95=0.017
  macro avg: prec=0.847  rec=0.809  F1=0.800

FOLD 1  totalCount=  1885   acc=0.816   conf95=0.017
  macro avg: prec=0.836  rec=0.801  F1=0.786

...

FOLD 9
  totalCount=  1885   acc=0.831   conf95=0.017
  macro avg: prec=0.854  rec=0.813  F1=0.805

Global Eval
  totalCount= 18846   acc=0.827   conf95=0.005
  macro avg: prec=0.848  rec=0.808  F1=0.802
\end{verbatim}
%
After printing the command-line arguments, it prints the results per
fold.  In this case, we have only chosen to print the count of
examples in the fold, the overall accuracy (and 95\% +/- value), and
the macro-averaged results; any of the other evaluation statistics
could be treated in the same way.  Further, by replacing the base
classifier evaluator with a conditional classifier evaluator, even
more statistics would be available (at the cost of speed and storage).

The main thing to take away from this display is the variation between
the folds.  Each represents training on a random 90\% of the data and
testing on the remaining 10\%.  The confidence intervals are computed
in the usual way from a normal approximation to a binomial
distribution.  As discussed in \refsec{classifier-eval-acc-conf}, the
way to read these is as a 95\% interval is as $0.825 \pm 0.017$.

An immediate takeaway message is that using three decimal places of
accuracy to report these results is highly misleading.  The confidence
interval tells us taht we're roughly $\pm 0.02$, so the second decimal
place isn't even likely to be accurate.  The rough validity of these
confidence intervals can be seen from looking at the range of values
produced.  The way to interpret the 95\% intervals resulting from one
fold can best be explained by example.  For instance, in fold 0, the
accuracy estimate is 0.825 with a confidence interval of $\pm 0.017$.
This is saying that (roughly because of the normal approximation)
if we truly had an 0.825 accuracy nad run 1884 trials (the total count
for the fold), 95\% of the time we would expect the measured
accuracy estimate to fall in the interval $0.825 \pm 0.017$.

After the final fold (fold 9, because we numbered from 0), we print
the global evaluation.  This is based on all the cases, so note the
tenfold higher total count.  The accuracy here is just the average of
the per-fold accuracies (weighted by count, of course).   Note
that because we have ten times as many evaluation points, our
confidence interval is much tighter.  Though it's still not tight
enough to report more than two decimal places. 

We can conclude with the settings we have that our system
is roughly 83\% accurate.

\subsection{Tuning a System}

The goal of tuning is to find the settings that work the best for a
given system.  Usually this is done by optimizing some measure of
performance, such as accuracy or macro-averaged F measure.  The
two standard approaches have been illustrated already for naive
Bayes, namely setting aside an official train/test split of the
data and the second is by cross validation.

The advantage of cross validation is that it uses more data, which all
else being equal, is a good thing.  The main drawback to
cross-validation is well illustrated with the twenty newsgroups data,
namely that the set of examples in the corpus is not stationary.  In
particular, the twenty newsgroups corpus is the result of a
typewritten dialog taking place on a newsgroup over time.  The later
articles are often closely related to the earlier articles by
inclusion of quotes and responses.  Thus they are related exactly and
also topically.  This is wholly typical of natural language and many
other domains in which forecasting is much harder than backcasting (or
``aftcasting'' if you prefer a more nautical terminology). 

One reason for this in language data is easily understood by imagining
ten years of newswire data.  Over time, different political figures
and sports figures come into and out of public focus and different
events come into and out of focus.  Often with a cyclic schedule; we
would not be surprised to find the news mentioning the word
\stringmention{holiday} and the phrase \stringmention{new year} more
often in December and January than in other months.  The terms
\stringmention{World Cup} and \stringmention{Winter Olympics} tend to
occur on four-year cycles.  Basically, language is not neatly
homogeneous over time (what a statistician would call ``stationary'').

For tuning, it's usually easiest to start with reasonable default
parameters and then move a single parameter at a time until results
look roughly optimal, then move onto the next parameter, continuing
to cycle through all the parameters until you run out of time,
energy or improvements.


\subsection{Tuning Naive Bayes}

The three quantitative tuning parameters for naive Bayes are reflected
in the command-line arguments to our demo, namely the two priors and
the length normalization.  

\subsubsection{Category Prior}

The category prior indicates the prior count of examples to be added
to each category's count.  During prediction, naive Bayes adds the
prior count and observed count in training to derive an estimate of
the probability of seeing a given category of text.  

Increasing the category prior will drive the category distribution
closer to uniform, meaning that the categories will have similar
probabilities.  This is easy to see with an example with two
categories, $A$ and $B$.  Suppose our corpus has 10 instances of $A$
and 40 instances of $B$.  With a prior count of 0, the probability
estimate for $A$ is 40/(10+50), or 0.8.  But if we have a prior count
of 20, the estimate is (40+20)/(10+20 + 50+20), or 0.6.  The higher
prior drives the probability closer to 0.5, representing equal odds
of either category appearing.

The twenty newsgroups is a balanced corpus, meaning that there are
roughly the same number of items in each category.  This is not
typical of the real data from which it was gathered.  In the wild,
different newsgroups have wildly different numbers of messages posted
to them.  If someone were to give us a posting, we would ideally be
able to use our knowledge of the frequency of postings as well as the
text.  

For balanced corpora like twenty newsgroups, the prior will have no
effect.  If ther training data counts were 20 and 20 for categories
$A$ and $B$, then the estimated probabilities are 0.5 no matter
what the prior count.  As a result, changing this parameter will
have no effect on the outcomes.

As with all count-based priors, the more data that is seen,
the smaller the effect of the prior.  This is also easily
seen by example.  If we have a prior count of 10, then
we get very different estimates with counts of 4 and 1,
40 and 10, and 4000 and 1000.  As the size of the data
increases, the estimates converge to the maximum likelihood
estimate that arises from a zero prior.


\subsubsection{Token Prior}

Like the category prior, the token prior smooths estimates,
specifically the estimate of the probability of seeing a given token
in a document drawn from a particular category.  This parameter has a
profound effect on naive Bayes classifiers.  For instance, here's
the overall accuracies based on different choices of token prior,
leaving the category prior at 2.0 and the length normalization at 10.0.
%
\begin{center}
\begin{tabular}{r|r}
{\it Token Prior} & {\it Accuracy} 
\\ \hline
0.0001 & 0.91
\\
 0.0010 & 0.91
\\
 0.0100 & 0.89
\\
 0.1000 & 0.83
\\
 1.0000 & 0.69
\\
10.0000 & 0.44
\end{tabular}
\end{center}
%
In this case, performance goes up as we lower the token count prior.
Typically, performance will begin to drop as the prior is reduced
below its optimal value.  While sometimes a zero prior will work (it's
equivalent to a maximum likelihood estimator), in practice it can
result in zero probability estimates, which can lead to numerical
instability in the evaluations.  For instance, setting the prior to
0.0 will crash naive Bayes, but setting it to a very small value still
gives performance of roughly 0.905.  

For robustness, it's usually safest to leave the parameters
close to where the performance begins to get good rather than to
extreme values.  For instance, from the above measurements, we'd
set the token prior to 0.001, not 0.0001.  By more robust, we mean
that the performance will likely be better for test examples
which are not exactly like the training examples.

If we were to continue to raise the prior, the performance
would degrade to chance accuracy, which is 0.05 given the
balanced 20 categories.

\subsubsection{Tuning Length Normalization}

The length normalization treats the entire document as if it were the
length norm number of tokens long.  Setting the value to \code{Double.NaN}
turns off length normalization (that's done with command-line
argument \stringmention{NaN} in this demo program).

The length normalization has a greater effect on probabilility
estimates than on first-best classification.  Its effect on first-best
results depends on the length of the document.  If the document is
shorter than the length norm, the document's model of tokens gets
upweighted relative to the overall category distribution.  If the
document is longer than the length norm, the reverse happens, with
more weight being given to the distribution over the categories than
would occur in the unnormalized case.

\subsubsection{Tuning Tokenization}

Because naive Bayes runs purely off of the observed tokens, the
tokenizer factory's behavior has a very large effect on how the
classifier behaves.  In these examples, we have used LingPipe's
default tokenizer for Indo-European languages (like English).  

It is common to apply a number of filters to a basic tokenizer.
Specifically, it is common to case normalize, remove stop words, and
reduce to a stemmed (or lemmatized form).  See \refchap{tokenization}
for a thorough description of LingPipe's built-in tokenizers.

Although it is possible to use n-gram based tokenization for naive
Bayes, either at the character or token level, these tokenizers
further violate the assumption of independence underlying naive Bayes.
For instance, if we have the word \stringmention{fantastic} and are
looking at 4-grams, then the first two tokens, \stringmention{fant}
and \stringmention{anta} are highly related; in fact, we know that the
character 4-gram following \stringmention{fant} must begin with
\stringmention{ant}.


\section{Formalizing Naive Bayes}

The naive Bayes model has a particularly simple structural form.
Setting aside document-length normalization, and assuming tokenization
has already happened, the model is nothing more than a multinomial
mixture model.  

\subsection{The Model}

The easiest way to write it is using sampling notation.  


\subsubsection{Observed Data}

The data we observe is structured as follows.
%
\begin{center}
\begin{tabular}{l|l}
{\it Data} & {\it Description}
\\ \hline
$K \in \nats$ & number of categories
\\ 
$N \in \nats$ & number of documents
\\
$V \in \nats$ & number of distinct tokens
\\
$M_n \in \nats$ & number of words in document $n$
\\
$x_{n,m} \in 1{:}V$ & word $m$ in document $n$
\\
$z_n \in 1{:}K$ & category of document $n$
\end{tabular}
\end{center}

\subsubsection{Model Parameters}

The parameters of the model are as follows.
%
\begin{center}
\begin{tabular}{l|l}
{\it Parameter} & {\it Description}
\\ \hline
$\alpha \in \posreals$ &  prior count for categories plus 1
\\
$\beta \in \posreals$ & prior count for tokens in categories plus 1
\\
$\pi_k \in [0,1]$ & probability of category $k$
\\
$\phi_{k,v} \in [0,1]$ & probability of token $v$ in document of category $k$
\end{tabular}
\end{center}
%
Note that the Dirichlet prior parameters $\alpha$ and $\beta$ are set
to the prior counts (as defined in our interface) plus 1.  In general,
prior count parameters for a Dirichlet may be effectively negative; they
are not allowed in LingPipe because they can easily lead to situations
where there is no maximum a posteriori estimate (see \refsec{naive-bayes-map}).


\subsubsection{Probability Model}

The generative model for the entire corpus (all observed data and
parameteters) is defined in sampling notation as follows.
%
\begin{align}
\pi &\sim \mbox{\sf Dirichlet}(\alpha)
\\
\phi_{k} &\sim \mbox{\sf Dirichlet}(\beta)
& \mbox{ for } k \in 1{:}K
\\
z_n &\sim \mbox{\sf Discrete}(\pi) & \mbox{ for } n \in 1{:}N
\\
x_{n,m} &\sim \mbox{\sf Discrete}(\phi_{z_n}) & \mbox{ for } n \in 1{:}N, m \in 1{:}M_n
\end{align}
%
For us, we fix the prior parameters $\alpha$ and $\beta$ to constants.
Specifically, these values are one plus the prior counts as described
in the API.%
%
\footnote{This is because, in general, the prior count can be
  conceptually negative for Dirichlet priors; LingPipe doesn't
  accomodate this case, because the MAP estimates often diverge with
  prior counts below zero.}
%

To start, we generate the category discrete distribution parameter
$\pi$ using a Dirichlet prior.  This parameter determines the
distribution of categories in the (super)population.%
%
\footnote{The finite population sampled may have a different
  distribution of categories.  This is easy to see.  If I roll a
  six-sided die twelve times, it's unlikely that I'll get each number
  exactly twice even though each has a 1/6 probability in each throw.
  The parameter represents the distribution over a theoretical
  superpopulation.  The law of large numbers states that as the corpus
  size ($N$) grows, the sampled proportions approach the
  superpopulation proportions.}
%
Next, for each category $k$, we generate the discrete distribution
parameter $\phi_k$ which models the probability of each token
appearing at any point in a document of category $k$.  

Now, we are ready to generate the observed data.  For each document
$n$, we generate its category $z_n$ based on the category distribution
$\pi$.  Then, for each word $m$ in document $n$, we generate its token
$x_{n,m}$ using the distributioin $\phi_{z_n}$ over words for category
$z_n$.  That's the entire model.

The sampling notation determines the entire probability of all of the
parameters (excluding the constant prior parameters) and the observed
data.
%
\begin{align}
p(\pi,&\phi,z,x) 
\\
&{} = \mbox{\sf Dir}(\pi|\alpha)
\, \times \,  \prod_{k=1}^K \mbox{\sf Dir}(\phi_k|\beta)
\, \times \,  \prod_{n=1}^N \mbox{\sf Disc}(z_n|\pi)
\, \times \,  \prod_{n=1}^N \, \prod_{m=1}^{M_n} \mbox{\sf Disc}(w_{n,m}|\phi_{z_n}).
\nonumber
\end{align}


\subsection{Predictive Inference}

For now, we will assume we know the parameters for a model; in
the next section we show how to estimate them.  Our prediction
problem for classification is to model the probability distribution
over categories given an observed document, which has the form
%
\begin{equation}
p(z_n | w_n, \pi, \phi).
\end{equation}
%
Applying Bayes's rule and a little algebra reveals that
%
\begin{align}
p(z_n|w_n, \pi, \phi) 
&{} = \frac{p(z_n,w_n|\pi,\phi)}{p(w_n|\pi,\phi)}
\\[6pt]
&{} \propto p(z_n,w_n|\pi,\phi)
\\[3pt]
&{} = p(z_n|\pi) \times \prod_{m=1}^{M_n} p(w_{n,m}|z_n,\phi).
\end{align}
%
To compute the actual probabilities, we just renormalize,
computing 
%
\begin{equation}
p(z_n|\pi,\phi) 
= \frac{p(z_n|\pi) \times \prod_{m=1}^{M_n} p(w_{n,m}|z_n,\phi)}
       {\sum_{k=1}^K p(k|\pi) \times \prod_{m=1}^{M_n} p(w_{n,m}|k,\phi)}.
\end{equation}


\subsection{Parameter Estimation}\label{section:naive-bayes-map}

Given training data, our job is to estimate the parameters for
a model.%
%
\footnote{This is an instance of so-called ``point estimation'', which
  may be contrasted with full Bayesian inference.  The latter averages
  over the uncertainty of the inferred parameters, whereas the former
  just assumes they are correct (or good enough).  Bayesian inference
  tends to be more robust, but inferences based on point estimates
  are much easier and faster, and tend to provide similar results.}

What we do for naive Bayes is known as maximum a posteriori (MAP)
estimate, because we are finding the parameters that maximize the
Bayesian posterior $p(\pi,\phi|z,w)$, which represents the probability
distribution over the parameters $\pi$ and $\phi$ given the observed
data $z$ and $w$; here we have suppressed the implicit conditioning on
the constants $\alpha$, $\beta$, $N$, $M$, and $K$.  The MAP estimates
for parameters are written as $\hat{pi}$ and $\hat{\phi}$ and defined
by 
%
\begin{equation}
(\hat{\pi},\hat{\phi})
= \arg\max_{\pi,\phi} \ p(\pi,\phi|z,w).
\end{equation}  
%
Because of the convenient form we have chosen for the priors, namely
Dirichlet, this equation can be solved analytically (as long as the
priors $\alpha$ and $\beta$ are greater than or equal to 1).
We just formalize what we said earlier, namely that we add the prior
counts to the observed counts.  For instance, for the category prevalence
parameter $\pi$, we have
%
\begin{equation}
\hat{\pi}_k \propto \alpha + \sum_{n=1}^N \indicator{z_n = k},
\end{equation}
%
where the indicator function $\indicator{z_n = k}$ has value 1 if the
condition is true and value 0 otherwise.  In words, its value is the
count of training examples for which the category $z_n$ was equal to
$k$.  This expression normalizes to
%
\begin{equation}
\hat{\pi}_k 
\ = \ \frac{\alpha + \sum_{n=1}^N \indicator{z_n = k}}
       {\sum_{k=1}^K (\alpha + \sum_{n=1}^N \indicator{z_n = k})}
\ = \ \frac{\alpha + \sum_{n=1}^N \indicator{z_n = k}}
       {K \times \alpha + N}.          
\end{equation}
%
The denominator is derived as the total of counts contributed by
training instances ($N$) and by the prior, which adds a prior count of
$\alpha$ for each category ($K \times \alpha$).














