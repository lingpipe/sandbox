From: <trec@nist.gov>
To: <carp@aliasi.com>
Date: Tuesday, September 28, 2004 11:23 AM


Dear TREC participant:

Attached are the evaluation results for your run(s):
	aliasiBase
	aliasiTerms

Also attached are summary tables for this task.

The summary tables give the per-topic best, worst, and median
scores for precision at 10 documents retrieved, precision
at 100 documents retrieved, and uninterpolated average precision.
One set of statistics is computed over the 10 manual runs
that were submitted to the track, and the other set is computed
over the 37 automatic runs that were submitted to the track.

These tables of min, max, and median scores across runs for each
topic are the only cross-systems scores we release prior to the
conference in November.  Those statistics are enough to give
a general feel for how effective your run was without giving
specific rankings.  (Generally, better systems out-perform
the median for a majority of topics, but not necessarily all topics.)
We don't release all scores for all groups before the conference
because we want people to concentrate on the methods they used
rather than on comparisons to other groups in their talks/posters.
That is, we like to make the emphasis at the conference the
sharing of different retrieval techniques, and to minimize the
competition aspect of the evaluation.  TREC has found not releasing
scores until the conference to be an effective way to keep
the competitiveness at healthy levels.

----------------------------

Dear TREC participant:

Attached are the evaluation results for your run(s):
	aliasiBase
	aliasiTerms

Also attached are summary tables for this task.

This report uses the corrected version of the qrels file.

The summary tables give the per-topic best, worst, and median scores for precision at 10 documents retrieved, precision at 100 documents retrieved, and uninterpolated average precision. One set of statistics is computed over the 10 manual runs that were submitted to the track, and the other set is computed over the 37 automatic runs that were submitted to the track.
