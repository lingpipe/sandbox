\chapter{Classifiers and Evaluation}\label{chap:classifier-evaluation}

We are going to introduce the classifier interface, discuss what
classifiers do, and then show how to evaluate them.  In subsequent
chapters, we consider a selection of the classifier implementations
offered in LingPipe.

\section{What is a Classifier?}

A classifier takes inputs, which could be just about anything, and
return a classification of the input over a finite number of discrete
categories.  For example, a classifier might take a biomedical
research paper's abstract and determine if it is about genomics or
not.  The outputs are ``about genomics'' and ``not about genomics.''

Classifiers can have more than two outputs.  For instance, a
classifier might take a newswire article and classify whether it is
politics, sports, entertainment, science and technology, or world or
local news; this is what Google and Bing's news services do.%
%
\footnote{At \url{http://news.google.com} and
  \url{http://news.bing.com}.}
%
Other applications of text classifiers, either alone or in concert
with other components, range from classifying documents by topic,
identifying the language of a snippet of text, analyzing whether a
movie review is postive or negative, linking a mention of a gene name
in a text to a database entry for that gene, or resolving the sense of
an ambiguous word like \stringmention{bank} as meaning a savings
institution or a flowing body of water.  In each of these cases, we
have a known finite set of outcomes and some evidence in the form of
text.

\subsection{Exclusive and Exhaustive Categories}

In the standard formulation of classification, which LingPipe follows,
the categories are taken to be both exhaustive and mutually exclusive.
Thus every item being classified has exactly one category.

One way to get around the exhaustiveness problem is to include an
``other'' category that matches any input that doesn't match one of
the other categories.  The other category is sometimes called a
``sink'' or ``trash'' category, and may be handled differently than
the other categories during training.

Exclusivity is more difficult to engineer.  While it is possible to
allow categories that represent more than one outcome, if we have $n$
base categories, we'll have ${n \choose 2}$ unique pairs of
categories.  The combinatorics quickly gets out of hand.

If an item needs to be classified for two cross-cutting
categorizations, we can use two classifiers, one for each
categorization.  For instance, we might classify MEDLINE citations as
being about genomics or not, and about being about clinical trials or
not.  The two binary classifiers produce four possible outcomes.
Another example of this would be to use two binary classifiers for
sentiment, one indicating if a text had positive sentiment or not, and
another indicating if it had negative sentiment or not.  The result is
a four-way classification, with a neutral article having neither
positive nor negative sentiment, a mixed review having both positive
and negative sentiment, and a positive or negative review having one
or the other.

The latent Dirichlet allocation (LDA) model we consider in
\refchap{lda} assigns more than one category to each document, under
the assumption that every document is a mixture of topics blended
according to some document-specific ratio that the model infers.

\subsection{First-Best, Ranked and Probabilistic Results}

A simple first-best classifier need only return its best guess for the
category of each input item.  Some applications only allow a single
best guess, and thus some evaluations are geared toward evaluating
only a classifiers' first-best guess for an input.

A ranking classifier returns the possible categories in rank order of
their match to the input.  The top ranked answer is the first-best
result.  We still assume exhaustive and exclusive categories, but the
classfier supplies its second-best guess, third-best guess and so on.
In cases where there are large numbers of categories, an application
might return more than one possible answer to a user.

A scoring classifier goes one step further and assigns a (floating
point) score to each categories.  These may then be sorted to provide
a ranking and a first-best result, with higher scores taken to be
better matches.  For example, LingPipe's implementations of averaged
perceptrons returns scored results.

Scores for categories given an input are often normalized so that they
represent an estimate of the conditional probability of a category
given the input.  This is the case for LingPipe's logistic regression
classifiers and k-nearest neighbors classifiers.  For instance, a
classifier might see a MEDLINE citation about testing for a disease
with a known genetic component and estimate a 40\% probability it is
about genomics and 60\% probability that it is not about genomics.
Because the outcomes are exhaustive and exclusive, the probabilities
assigned to the categories must sum to 1.

In the case of generative statistical models, such as naive Bayes
classifiers or hidden Markov models, the score represents the joint
probabilty of the output category and the input being classified.
Given the joint probabilities, we can use the rule of total
probability to compute conditional probabilities of categories given
inputs.

\subsection{Ordinals, Counts, and Scalars}

A classifier is an instance of what statisticians call categorical
models.  The outcome of a classification is a category, not a number.

Classifiers deal with categorical outcomes.  Ordinal outcomes are like
categorical outcomes, only they come with an order.  Examples of
ordinal outcomes include rating movies on a $\{ 1, 2, 3, 4, 5 \}$
scale, answering a survey question with strongly-disagree, disagree,
neutral, agree, or strongly agree, and rating a political attitude as
left, center, or right.

In many cases, we will be dealing with counts, which are non-negative
natural numbers.  Examples of count variables include the number of
times a word appears in a document, the length of a document's title
in characters, and the number of home runs a baseball player gets in a
season.  Classifiers like naive Bayes convert word count data into
categorical outcomes based on a probability model for documents (see
\refchap{naive-bayes}).

Scalar outcomes are typically continuous.  Examples of continuous
scalar variables include a person's height, the length of the vowel
sequence in milliseconds in the pronunciation of the word
\charmention{lion}, or the number of miles between two cities.  Wes

Rating movies on an ordinal 1-5 scale doesn't allow ratings like
1.793823.  Half-star ratings could be accomodated by including
additional discrete outcomes like 1.5, 2.5, 3.5 and 4.5, leaving a
total of 9 possible ratings.  When there are 100 possible ratings, it
makes less sense to treat the outcome as ordinal.  Often, it is
approximated by a continuous scalar variable.

Models that predict or estimate the values of ordinal, count and
scalar values all have different evaluation metrics than categorical
outcomes.  In this section, we focus exclusively on categorical
outcomes.


\subsection{Reductions to Classification Problems}

Many problems that don't at first blush appear to be classification
problems may be reduced to classification problems.  For instance, the
standard document search problem (see \refchap{lucene}) may be recast
as a classification problem.  Given a user query, such as
\searchquery{be-bim-bop recipe}, documents may be classified as
relevant or not-relevant to the search.  

Another popular reduction is that for ranking.  If we want to rank a
set of items, we can build a binary classifier for assessing whether
one item is greater than another in the ordering.  In this case, it is
then a challenge to piece back together all these binary decisions to
get a final rank ordering.

Ordinal classification problems may also be reduced to
classifications.  Suppose we have a three-outcome ordered result, say
$N \in \{ 1, 2, 3 \}$.  We can develop a pair of binary classifiers
and use them as an ordinal three-way classifier.  The first classifier
will test if $N < 2$ and the second if $N < 3$.  If the first
classifier returns true, the response is 1, else if the second
classifier returns true the response is 2, else the response is 3.
Note that if the first classifier returns true and the second false,
we have an inconsistent situation, which we have resolved by returning
1.

Just about any problem that may be cast in a hypothesize-and-test
algorithm may use a classifier to do the testing, with a simple binary
outcome of accept or reject.  For instance, we can generate possible
named-entity mention chunkings of a text and then use a binary
classifier to evaluate if they are correct or not.  

Such reductions are often not interpetable probabilistically in the
sense of assigning probabilities to possible outcomes.



\section{Gold Standards, Annotation, and Reference Data}

For classification and other natural language tasks, the categories
assigned to texts are designed and applied by humans, who are
notoriously noisy in the semantic judgments for natural language.  

For instance, we made up the classification of genomics/non-genomics
for MEDLINE citations.  In order to gather evaluation data for a
classifier, we would typically select a bunch of MEDLINE citations at
random (or maybe look at a subset of interest), and label them
as to whether they are about genomics or not.

At this point, we have a problem.  If one annotator goes about his or
her merry way and annotates all the data, everything may seem fine.
But as soon as you have a second annotator try to annotate the same
data, you will see a remarkable number of disagreements over what seem
like reasonably simple notions, like being ``about'' genomics.  For
instance, what's the status of a citation in which DNA is only
mentioned in passing?  What about articles that only mention proteins
and their interactions?  What about an article on the sociology of the
human genome project?  These boundary cases may seem outlandish, but
try to label some data and see what happens.

In the Message Understanding Conference evaluations, there were a
large number of test instances involving the word \stringmention{Mars}
used to refer to the planet.  The contestants' systems varied in
whether they treated this as a location or not.  The reference data
did, but there weren't any planets mentioned in the training data.

\subsection{Evaluating Annotators}

The simplest way to compare a pair of annotations is to look at
percentage of agreement between the annotators.  

Given more than two annotators, pairwise statistics may be calculated.
These may then be aggregated, most commonly by averaging, to get an
overall agreement statistic.  It is also worth inspecting the counts
assigned to categories to see if any annotators are biased toward
too many or too few assignments to a particular category.

In general, pairs of annotators may be evaluated as if they were
themselves classifiers.  One is treated as the reference (gold
standard) and one as the response (system guess), and the response
is evaluated against the reference.  

It is common to see Cohen's $\kappa$ statistic used to evaluate
annotators; see \refsec{classifier-eval-kappa} for more information.


\subsection{Majority Rules, Adjudication and Censorship}

Data sets are sometimes created by taking a majority vote on the
category for items in a corpus.  If there is no majority (for
instance, if there are two annotators and they disagree), there are
two options.  First, the data can be censored, meaning that it's
removed from consideration.  This has the adverse side effect of
making the actual test cases in the corpus easier than a random
selection might be because the borderline cases are removed.  Perhaps
one could argue this is an advantage, because we're not even sure what
the categories are for those borderline cases, so who cares what the
system does with them.  

The second approach to disagreements during annotation is
adjudication.  This can be as simple as having a third judge look at
the data.  This will break a tie for a binary classification problem,
but may not make the task any clearer.  It may also introduce noise
as borderline cases are resolved inconsistently.

A more labor intensive approach is to consider the cases of
uncertainty and attempt to update the notion of what is being
annotated.  It helps to do this with batches of examples rather than
one example at a time.  

The ideal is to have a standalone written coding standard explaining
how the data was annotated that is so clear that a third party could
read it and label the data consistently with the reference data.  


\section{Precision-Recall Evaluation}

We start by considering the evaluation of a first-best classifier on a
two-category classification problems; we consider first-best
classifiers for classification problems in
\refsec{classifier-eval-confusion-matrix}.  We treat these
classification problems asymmetrically by labeling one catgory
``positive'' and the other ``negative'' (equivalently
``accept''/''reject'' or ``true''/''false'').

\subsection{$2 \times 2$ Confusion Matrices}

Suppose we are evaluating a first-best classifier on a two-category
classification problem for which we have reference answers.  These
reference answers are usually taken to be whatever is considered
the ``true'' answer, but this is not a requirement.  We can evaluate
two classifiers against each other, for instance, or two annotators
who labeled the same corpus.  In many of these cases, it doesn't
matter which system is taken as the reference and the response.  The
statistics we report are almost all either symmetric or implemented
in both directions.





\section{Confusion Matrices}\label{section:classifier-eval-confusion-matrix}

One of the fundamental tools for evaluating first-best classifiers is
the confusion matrix.  A confusion matrix reports on the number of
agreements between a classifier and the reference (or ``true'')
categories.  This can be done for classification problems with any
number of categories.

\subsection{Example: Blind Wine Classification by Grape}

Confusion matrices are perhaps most easily understood with an example.
Consider blind wine tasting, which may be viewed as an attempt by a
taster to classify a wine, whose label is hidden, as to whether it is
made primarily from the syrah, pinot (noir), or cabernet (sauvignon)
grape.  We have to assume that each wine is made primarily from one of
these three grapes so the categories are exclusive and exhaustive.  An
alternative, binary classification problem would be to determine if
a wine had syrah in it or not.

Suppose our taster works their way through 27 wines, assigning a
single grape as a guess for each wine.  For each of the 27 wines,
we have assumed there is a true answer among the three grapes
syrah, pinot and cabernet.  The resulting confusion matrix might look
as in \reffig{blind-wine-confusion}.
%
\begin{figure}
\begin{center}
\begin{tabular}{r|r|c|c|c|c}
\multicolumn{2}{c}{ } & \multicolumn{3}{c}{\tblhead{\bfseries Response}}
\\ \cline{3-5}
\multicolumn{2}{c|}{ } & \tblhead{cabernet} & \tblhead{syrah} & \tblhead{pinot}
\\ \cline{2-5}
\multirow{3}{0.15\textwidth}{\hfill\tblhead{\bfseries Reference}}
& \tblhead{cabernet} & 9 & 3 & 0 & {\it 12}
\\ \cline{2-5}
& \tblhead{syrah} & 3 & 5 & 1 & {\it 9}
\\ \cline{2-5}
& \tblhead{pinot} & 1 & 1 & 4 & {\it 6}
\\ \cline{2-6}
\multicolumn{2}{c}{ } & \multicolumn{1}{c}{\it 13} & \multicolumn{1}{c}{\it 9} & \multicolumn{1}{c}{\it 5} & \multicolumn{1}{|c}{\it\bfseries 27}
\end{tabular}%
\end{center}
\caption{Confusion matrix for responses of a taster guessing the grape
  of 27 different wines.  The actual grapes in the wines make up the
  reference against which the taster's responses are judged.  Values
  within the table indicate the number of times a reference grape was
  guessed as each of the three possibilities. Italicized values
  outside the box are totals, and the bold italic 27 in the lower
  right corner is the total number of test cases.}\label{fig:blind-wine-confusion}
\end{figure}
%
Each row represents results for a specific reference category.  In the
example, the first row represents the results for all wines that were
truly cabernets.  Of the 12 cabernets presented, the taster guessed
that 9 were cabernets, 3 were syrah, and none were guessed to be a
pinot.  The total number of items in a row is represented at the end
in italices, here {\it 12}.  The second row represents the taster's
resuls for the 9 syrahs in the evaluation.  Of these, 5 were correctly
classified as syrahs, whereas 3 were misclassified as cabernets and
one as a pinot.  The last row is the pinots, with 4 correctly
identified and 1 misclassified as cabernet and 1 as syrah.  Note that
the table is not symmetric.  One pinot was guessed to be a cabernet,
but no cabernets were guessed to be pinots.  

The totals along the right are total number of reference items, of
which there were 12 cabernets, 9 syrahs, and 6 pinots.  The totals
along the bottom are for responses.  The taster guessed 13 wines were
cabernets, 9 were syrahs, and 5 were pinots.  The overall total in the
lower right is 27, which is sum of the values in all the cells, and
equal to the total number of wines evaluated.

\subsection{Accuracy}

A number of useful statistics may derived from confusion matrices.  We
have already seen the basic counts of reference/response pairs, as
well as reference and response counts by category, as well as the overall counts.

\subsubsection{Overall Accuracy}

The overall accuracy is just the number of correct responses divided
by the total numbe of responses.  A correct response for an item
occurs when the response category matches the reference category.  The
count of correct responses are thus on the diagonal of the confusion
matrix.  There were 9 correct responses for reference cabernets, 5 for
syrahs, and 4 for pinots, for a total of 18 correct responses.  The
total accuracy of the classifier is thus $18/27 \approx 0.67$.  

Total accuracy estimates the probability that a classifier will make
the correct categorization of the next item it faces, assuming the
items to be evaluated are sampled the same way as the test data.

\subsubsection{Accuracy Confidence Intervals} 

Because overall accuracy is a simple binomial statistic, we can
compute a normal approximation to a 95\% confidence interval directly
(see \refsec{stats-binomial-variance} for an explanation of the
formula).  In this case, the 95\% interval is approximately plus or
minus 0.18 and the 99\% interval approximately plus or minus 0.23.  As
usual, with so few evaluation cases, we don't get a tight estimate of
our system's accuracy.

In almost all cases, including this one, confidence in our accuracy
numbers will be inversely proportional to the square root of the
number of test cases.  For instance, with 16 test cases, our
confidence interval is sized proportionally to $1/\sqrt{16} = 1/4$,
and with 64 test cases, it's proportional to $1/\sqrt{64} = 1/8$.  So
even though we multiply the number of cases by 4, the confidence only
shrinks by $\sqrt{4}$, or a half.


\subsection{$\kappa$ Statistics for Chance-Adjusted Agreement}\label{section:classifier-eval-kappa}

\subsubsection{Random Accuracy}

Suppose that the response is chosen randomly according to the
distribution of response answers.  In our running example, the taster
responded cabernet 13 times out of 27, syrah 9 of 27, and pinot 5 of
27.  We could choose a response randomly by choosing cabernet 13/27
times, syrah 9/27, and pinot 5/27.  

If we assume test cases appear in the proportions they do in the test
set, we have cabernet 12/27 times, syrah 9/27, and pinot 6/27 times.

Now if the responses are chosen randomly, our probabilty of getting
the right answer is 
%
\begin{equation}
\left(\frac{13}{27} \times \frac{12}{27}\right)
+ \left(\frac{9}{27} \times \frac{9}{27}\right)
+ \left(\frac{5}{27} \times \frac{6}{27}\right)
\approx 0.3663.
\end{equation}
%
(We use so many decimal places here to compare two different versions
of $\kappa$ below.)  The best possible random accuracy is achieved
when the distribution of responses matches that of the references.

\subsubsection{Cohen's $\kappa$ Statistic}

Cohen's $\kappa$ statistic uses the random accuracy for a
classification problem to adjust accuracy for chance.%
%
%
The basic idea is that performance evaluations should be discounted
based on what could be achieved by randomly guessing (in proportions
based on the marginal response categories of a responder).  Cohen
defined $\kappa$ for a specific classification problem, reference
answers and responses as
%
\begin{equation}\label{eq:kappa-statistic}
\kappa = \frac{a - e}{1 - e}
\end{equation}
%
where $a$ is a response's total accuracy, and $e$ is the random
accuracy, computed as above.  The notation involves an $e$ because its
value is also the expected accuracy from guessing randomly (according
to the response's own proportions).  Thus we expect a system that only
does as well as random, so that $a = e$, to have a $\kappa$ score of
0.  The maximum possible value of $\kappa$ is 1, which occurs only for
perfectly accurate systems where $a = 1$.  The minimum possible value,
for a perfectly innacurate system with $a = 0$ is $-e/(1-e)$, which is
coincidentally the (negative) odds for a randomly chosen item to be
guessed correctly with a random guess.

Cohen's $\kappa$ is often used to evaluate the agreement between a
pair of annotators for a data labeling task.  Because accuracy only
depends on the diagonal items, the $\kappa$ statistic provides the
same result no matter which annotator is treated as the reference and
which as the response.  LingPipe's confusion matrix class can compute
$\kappa$, as we see in the demo in the next section.

\subsubsection{Siegel and Castellan's $\kappa$ Statistic}

Depending on how expected accuracy is calculated, the result of the
$\kappa$ calculation, as given in \refeq{kappa-statistic}, varies.

A popular alternative to Cohen's version is Siegel and Castellan's
$\kappa$, 
%
%
which uses the average of the reference and response
proportions to compute the chance accuracy.  

Continuing our wine example, recall that the reference had 12/27
cabernet and the response 13/27 cabernet.  In the Siegel and Castellan
version of $\kappa$, we average these two to get 12.5/27; for syrah,
averaging 9/27 and 9/27 yields 9/27, and for pinot, 5/27 and 6/27 make
5.5/27.  The average of values summing to 1 also sums to 1, so we use
them as probabilities to compute expected accuracy as
%
\begin{equation}
\left( \frac{12.5}{27} \right)^2
+ \left( \frac{9}{27} \right)^2
+ \left( \frac{5.5}{27} \right)^2
\approx 0.3669.
\end{equation}
%
In general, Siegel and Castellan's expected agreement will be higher
than with Cohen's calculation.

Given this definition of expected accuracy, we just plug it into the
definition of the $\kappa$ statistic, $\kappa = (a - e)/(1 - e)$.


\subsubsection{Byrt et al.'s $\kappa$ Statistic}

Redefining $\kappa$ is a popular sport, the most recent widely cited
version being provided by Byrt, Bishop and Carlin.%
%
xo%
In this version, $\kappa$ is computed directly as
%
\begin{equation}
\kappa = 2 a - 1
\end{equation}
%
where $a$ is the accuracy.  The minimum value, for 0\% accuracy,
is -1, the maximum value, for 100\% accuracy is 1, and the
breakeven poin is 50\% accuracy, yielding a $\kappa$ value of 0.


\subsection{Information-Theoretic Measures}

A good classifier will produce decisions whose decision matrices imply
low entropy in the decision making process (we define entropy
precisely in \refsec{stats-entropy}).  Low entropy arises when the
uncertainty in the category of an item given the classifier's decision
is low.  The minimal entropy is 0, which occurs when the classifier is
perfect in the sense that given the classifiers decision, there was no
uncertainty at all about the actual category.  

In all of the information-theoretic measures, we look at the received
distribution of data in the confusion matrix to estimate a reference
and response distribution.  For instance, in the reference, 12/27
wines were cabernets, 9/27 syrahs, and 6/27 pinots.  We can use these
frequencies computed from the data to produce probability
distributions, which can then be measured information theoretically.%
%
\footnote{These are essentially maximum likelihood estimates.  The
  distributions could be given a prior and maximum a posterior
  estimates or Bayesian point estimates could be used, or we could
  even use full Bayesian posteriors.  This goes not just for the
  information-theoretic measures, but also the other measures based on
  received data.}


\subsubsection{Reference Entropy}

If we treat the references and responses as data from which to
estimate probability distributions, we can evaluate the entropy of the
reference and response distributions on their own.  We did this before
taking the reference to be distributed with 12/27 cabernet, 9/27
syrah, and 6/27 pinot.  The reference entropy is just the entropy of
the three-outcome discrete probability distribution with probabilty
12/27 of cabernet, 9/27 of syrah, and 6/27 of pinot.  The entropy for
these values is computed as
%
\begin{equation}
\left( \frac{12}{27} \times \log_2 \frac{12}{27} \right)
+ \left( \frac{9}{27} \times \log_2 \frac{9}{27} \right)
+ \left( \frac{6}{27} \times \log_2 \frac{6}{27} \right)
\approx 1.53.
\end{equation}

The entropy of the reference measures how well we can do by randomly
guessing according to the distrubiton of the reference.
Unfortunately, the reference distribution is rarely known; instead, we
have to work with estimates of the reference distribution derived from
data.  Distributions that are highly skewed have lower entropy than
uniform distributions where all outcomes are equally likely.

\subsubsection{Conditional Entropy}

In evaluating a classifier, we are really interested in how much the
classifier reduces the undertainty in the outcome.  Without the classfier,
the uncertainty is measured by the entropy of the reference distribution,
as calculated above.  With the classifier in place, we need to measure
the entropy of the outcome given the response of the classifier.  

Let's focus on the response in the case of wines whose reference
category is pinot.  In those cases, of which there were 6, the
classifier classified 1 as cabernet, 1 as syrah, and 4 as pinot.  
This corresponds to a discrete distribution with probabilities
1/6, 1/6 and 4/6.  Calculating the entropy for this outcome, we
get
%
\begin{equation}
\left( \frac{1}{6} \times \log_2 \frac{1}{6} \right)
+ \left( \frac{1}{6} \times \log_2 \frac{1}{6} \right)
+ \left( \frac{4}{6} \times \log_2 \frac{4}{6} \right)
\approx 1.25.
\end{equation}
%
We will refer to this value as the conditional entropy of the outcome
given the reference category is pinot and observing the classifer's
response.

To get the overall conditional entropy, we just weight these
by-reference-category conditional entropies by the fraction of
outcomes they represent.  For instance, we weight the 1.25 conditional
entropy of pinot by multiplying by 6/27, because there are 6 wines
whose reference grape is pinot in the evaluation.

\subsubsection{Cross Entropy}

Cross entropy is a measurement of how well one distribution predicts
another one (see \refsec{stats-cross-entropy}.  For classifiers, this
measures just the distributions of the references and responses.  For
our example, the reference has 12/27 cabernet, 9/27 syrah, and 6/27
pinot, whereas the response had 13/27 cabernet, 9/27 syrah, and 5/27
pinot.  If we take these response ratios as probability distributions,
the cross-entropy evaluation measures how well the response
distribution matches the reference distribution.  Note that this is
not a measure of classification accuracy.  The lowest cross entropy
possible is the entropy of the reference distribution, and occurs when
the response distribution exacty matches the reference distribution.

\subsubsection{KL Divergence}

In addition to cross entropy, the classifier evaluators compute the KL
divergence of the response distribution from the reference
distribution (see \refsec{stats-divergence}).  This is also known as
relative entropy, or information gain, and it's a measure of how much
information the response distribution provides about the reference
distribution.

\subsubsection{Mutual Information}

Classifier evaluators are also able to compute the mutual information
between the reference and response distributions (see
\refsec{stats-mutual-information}).  This is just entropy of the
responses minus the conditional entropy of the reference given the
response.  


\subsection{$\chi^2$ Independence Tests}

The $\chi^2$ independence test is for the hypothesis that the the
reference and response categories for an item were assigned
independently (see \refsec{stats-chi-squared-distribution}).  In the
blind-wine tasting case, this would correspond guessing the wine
before tasting it by simply generating a random category based on the
taster's distribution of answers.%
%
\footnote{This random guessing is the same kind that underlies the
expected correct answers by chance used in the definition of
Cohen's $\kappa$ statistic in \refsec{classifier-eval-kappa}.}

The number of degrees of freedom is just the square of the number of
categories minus one.  We can use the $\chi^2$ distribution with this
many degrees of freedom to test our hypothesis.  If the value of our
test statistic is very unlike what would be expected under the
assumption of independence (as measured by the $\chi^2$ distribution),
then we can reject the hypothesis that the results were generated
independently.  LingPipe does not provide the necessary implementation
of the $\chi^2$ cumulative density functions which would be required
to compute a $p$-value.%
%
\footnote{For our example with a test statistic of $X^2 = 15.5256$ and
  4 degrees of freedom, we could use the R function
  \code{pchisq(15.5256,df=4)} for our example to compute the
  cumulative probability of a value less than 15.5256, which is 0.996,
  corresponding to a $p$-value of .004.  This is low enough to allow
  us to fairly confidently reject the null hypothesis that the
  responses were generated independently of the true answer.}


\subsection{$\varphi^2$ and Cram\'er's V statistic}

A related measure to $\chi^2$ is Pearson's $\varphi^2$ index of what
is known as mean square contingency.  It's defined by dividing the
$X^2$ statistic underlying the $\chi^2$ test by the number of test
items. 



\subsection{Index of Predictive Association}



\subsection{The \code{ConfusionMatrix} Class}

A confusion matrix is represented in LingPipe as an instance of the
class \code{ConfusionMatrix}, which is in package
\code{com.aliasi.classify}.  

\subsubsection{Constructing a Confusion Matrix}

A confusion matrix minimally requires an array of categories for
construction, with constructor \code{ConfusionMatrix(String[])}.  It
may optionally take a matrix of counts, with constructor
\code{ConfusionMatrix(String[],int[][])}.

\subsubsection{Getters and Increments}

We can retrieve (a copy of) the categories as an array using
\code{categories()}.  The evaluation keeps a symbol table under the
hood (see \refchap{symbol-tables}) to support mappings from categories
to their indices.  This is available indirectly through the method
\code{getIndex(String)}, which returns the index for a category in the
array of category names or -1 if there is no such category.

It's also possible to retrieve (a copy of) the entire matrix, using
\code{matrix()}, which returns an \code{int[][]}.  

We can get the count in a particular cell using \code{count(int,int)},
where the first index is for the reference category.  If we want to
get the count by category, we can use the \code{getIndex()} method
first.

We can increment the values in the confusion matrix by reference and
response category, or by index.  The method \code{increment(int,int)}
takes a reference and response category index and increments the
counts by one.  The method \code{increment(String,String)} takes
the names of the categories.  There is also a method
\code{incrementByN(int,int,int)}, which takes two category indices
and an amount by which to increment.

Often classifiers do not perform equally well for all categories.  To
help assess performance on a per-category basis, we can look at the
category's row or column of the confusion matrix.  The confusion
matrix class also supports one-versus-all evaluations, as
discussed in \refsec{classifier-eval-one-versus-all}.


\subsubsection{Demo: Confusion Matrices}

In the rest of this section, we will illustrate the use of confusion
matrices to compute the statistics defined earlier in this section.
The demo is in class \code{ConfusionMatrixDemo}.

\subsubsection{Code Walkthrough}

The work is all in the \code{main()} method, which starts by
allocating the contents of the confusion matrix and then the matrix
itself, with data corresponding to our running blind-wine classication
example.
%
\codeblock{ConfusionMatrixDemo.1}
%
After that, it's just a bunch of getters and assigns, which we will
print at the enc of the code.  First, the categories, total count,
total correct and accuracies, with confidence intervals,
%
\codeblock{ConfusionMatrixDemo.2}
%
Next, the three versions of the $\kappa$ statistic,
%
\codeblock{ConfusionMatrixDemo.3}
%
Then the information theoretic measures,
%
\codeblock{ConfusionMatrixDemo.4}
%
Finally, we have the $\chi^2$ and related statistics and the $\lambda$ statistics,
%
\codeblock{ConfusionMatrixDemo.5}


\subsubsection{Running the Demo}

The Ant target \code{confusion-matrix} runs the demo.
%
\commandlinefollow{ant confusion-matrix}
\begin{verbatim}
categories[0]=cabernet
categories[1]=syrah
categories[2]=pinot

totalCount=27
totalCorrect=18
totalAccuracy=0.6666666666666666
confidence95=0.17781481095759366
confidence99=0.23406235319928148

randomAccuracy=0.36625514403292175
randomAccuracyUnbiased=0.3669410150891632
kappa=0.4740259740259741
kappaUnbiased=0.4734561213434452
kappaNoPrevalence=0.33333333333333326

referenceEntropy=1.5304930567574824
responseEntropy=1.486565953154142
crossEntropy=1.5376219392005763
jointEntropy=2.619748965432189
conditionalEntropy=1.089255908674706
mutualInformation=0.39731004447943596
klDivergence=0.007128882443093773
conditionalEntropyCab=0.8112781244591328
conditionalEntropySyr=1.3516441151533922
conditionalEntropyPin=1.2516291673878228

chiSquared=15.525641025641026
chiSquaredDegreesOfFreedom=4.0
phiSquared=0.5750237416904084
cramersV=0.5362013342441477

lambdaA=0.4
lambdaB=0.35714285714285715
\end{verbatim}



\subsection{One-Versus-All Evaluations}\label{section:classifier-eval-one-versus-all}

We often want to evaluate the performance of a classifier on a single
reference category.  LingPipe's confusion matrix classes support this
kind of single-category evaluation by projecting a confusion matrix
down to a $2 \times 2$ confusion matrix comparing a single category to
all other categories.  Because our categories are exhaustive and
exclsuive, the reduced matrix compares a single category to its
complement.

\subsection{Example: Blind Wine (cont.)}

Going back to our sample confusion matrix in
\reffig{blind-wine-confusion}, we can render the results for the three
different wines into the following three $2 \times 2$ confusion
matrices, which we show in \reffig{blind-wine-one-versus-all}.
%
\begin{figure}
\hfill
\begin{tabular}{|r|c|c|}
\multicolumn{1}{c}{ } & \multicolumn{2}{c}{\tblhead{\bfseries Resp}}
\\ \cline{2-3}
\multicolumn{1}{c}{\tblhead{\bfseries Ref}} & \multicolumn{1}{|c|}{\tblhead{cab}} & \tblhead{not}
\\ \cline{1-3}
\tblhead{cab} & 9 & 3
\\ \cline{1-3}
\tblhead{not} & 4 & 11
\\ \cline{1-3}
\end{tabular}
\hfill
\begin{tabular}{|r|c|c|}
\multicolumn{1}{c}{ } & \multicolumn{2}{c}{\tblhead{\bfseries Resp}}
\\ \cline{2-3}
\multicolumn{1}{c}{\tblhead{\bfseries Ref}} & \multicolumn{1}{|c|}{\tblhead{syr}} & \tblhead{not}
\\ \cline{1-3}
\tblhead{syr} & 5 & 4
\\ \cline{1-3}
\tblhead{not} & 4 & 14
\\ \cline{1-3}
\end{tabular}
\hfill
\begin{tabular}{|r|c|c|}
\multicolumn{1}{c}{ } & \multicolumn{2}{c}{\tblhead{\bfseries Resp}}
\\ \cline{2-3}
\multicolumn{1}{c}{\tblhead{\bfseries Ref}} & \multicolumn{1}{|c|}{\tblhead{pin}} & \tblhead{not}
\\ \cline{1-3}
\tblhead{pin} & 4 & 2
\\ \cline{1-3}
\tblhead{not} & 1 & 20
\\ \cline{1-3}
\end{tabular}
\hfill { }
%
\caption{The one-versus-all confusion matrices defined by joining
  categories of the $3 \times 3$ confusion matrix shown in
  \reffig{blind-wine-confusion}.}\label{fig:blind-wine-one-versus-all}
\end{figure}
%
The one number that stays the same is the number of correct
identifications of the category.  For instance, for syrah, the taster
had 9 correct identifications.  Thus there is a 9 in the
cabernet-cabernet cell of the cabernet-versus-all matrix.  The count
for cabernet reference and non-cabernet response sums the two original
cells for cabernet reference/syrah response and cabernet
reference/pinot response, which were 3 and 0, yielding a total of 3
times that a cabernet was misclassified as something other than
cabernet.  Similarly, the cell for non-cabernet reference and cabernet
response, representing non-cabernets classified as cabernets, is 4,
for the 3 syrahs 1 pinot and the taster classified as cabernets.  All
other values go in the remaining cell of not-cabernet reference and
not-cabernet response.  The total is 11, being the sum of the syrah
reference/syrah response, syrah reference/pinot response, pinot
reference/syrah response, and pinot reference/pinot response.  Note
that this includes both correct and incorrect classifications for the
original three-way categorization scheme.  Here, the 11
classifications of non-cabernets as non-cabernets are treated as
correct.  The other two tables, for syrah-versus-all and
pinot-versus-all are calculated in the same way.
  












