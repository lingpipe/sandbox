\chapter{Further Reading}\label{appendix:reading}

\firstchar{I}n order to fully appreciate contemporary approaches to natural language
processing requires three fairly broad areas of expertise:
linguistics, statistics, and algorithms.  We don't pretend to
introduce these areas in any depth in this book.  Instead, we
recommend the following textbooks, sorted by area, from among the many
that are currently available.

See the companion volume, {\it Text Processing in Java 6}, for 
more background on Unicode, Java and its libraries, and general
programming.


\section{Algorithms}

\noindent
Because natural language processing requires comptuational
implementations of its models, a good knowledge of algorithm analysis
goes a long way in understanding the approaches found in a tool kit
like LingPipe.

\begin{itemize}
%
\bibanno{Cormen, Thomas H., Charles E.\ Leiserson, Ronald L.\ Rivest, and
Clifford Stein. 
2009.
{\it Introduciton to Algorithms}, Third Edition.
MIT Press.}
{A thorough yet readable introduction to algorithms.}
%
\bibanno{Durbin, Richard, Sean R.\ Eddy, Anders Krogh, Grame Mitchison.
{\it Biological Sequence Analysis: Probabilistic Models of Proteins
and Nucleic Acids}.
Cambridge University Press.}
{Even though it focuses on biology, it has great
descriptions of several key natural language processing algorithms
such as hidden Markov models and probabilistic context-free grammars.}
%
\bibanno{Gusfield, Dan.
1997.
{\it Algorithms on Strings, Trees and Sequences}.
Cambridge University Press.}
{Incredibly detailed and thorough introduction to
string algorithms, mainly aimed at computational biology, but
useful for all string and tree processing.}
%
\end{itemize}






\section{Probability and Statistics}

\noindent
Most of the tools in LingPipe are statistical in nature.  To fully
understand modern natural language processing it's necessary to
understand the statistical models which make up their foundation.
To do this requires an understanding of the basics of statistics.
Much of the work in natural language processing is Bayesian, so
understanding Bayesian statistics is now a must for understanding
the field.  Much of it also relies on information theory, which is
also introduced in the machine learning textbooks.

\begin{itemize}
%
\bibanno{
Bulmer, M.~G.
1979.
{\it Principles of Statistics}.
Dover.}
{Packs an amazing amount of information into
a readable little volume, though it assumes some mathematical
sophistication.}
%
\bibanno{Cover, Thomas M.\ and Joy A.\ Thomas.
2006.
{\it Elements of Information Theory}, Second Edition.
Wiley.}
{This is the classic textbook for information theory.}
%
\bibanno{DeGroot, Morris H.\ and Mark J.\ Schervish.
2002.
{\it Probability and Statistics},  Third Edition.
Addison-Wesley.}
{Another introduction to classical frequentist
statistical hypothesis testing and estimation but with some Bayesian
concepts thrown into the mix.}
%
\bibanno{Gelman, Andrew and Jennifer Hill.
2006.
{\it Data Analysis Using Regression and Multilevel/Hierarchical Models}.
Cambridge University Press.}
{A great practical introduction to regression from a Bayesian
perspective with an emphasis on multilevel models.  Much less mathematically
demanding than the other introductions, but still very rigorous.}
%
\bibanno{Gelman, Andrew, John B.~Carlin, Hal S.\ Stern, and Donald B.\ Rubin.
2003.
{\it Bayesian Data Analysis}, Second Edition.
Chapman and Hall.}
{The bible for Bayesian methods.  Presupposes high
mathematical sophistication and a background in classical probability
and statistics to the level of DeGroot and Schervish or Larsen
and Marx's books.}
%
\bibanno{Larsen, Richard J.\ and Morris L.\ Marx.
2005.
{\it Introduction to Mathematical Statistics and Its Applications}, Fourth Edition.
Prentice-Hall.}
{A nice intro to classical frequentist statistics.}
%

\bibanno{Agresti, A. 2002. {\it Categorical Data Analysis}, Second Edition. Wiley.}
{A thorough survey of classical approaches to analyzing categorical data,
focusing on contingency tables.}

\end{itemize}


\section{Machine Learning}

\noindent
Machine learning, in contrast to statistics, focuses on implementation
details and tends to concentrate on predictive inference for larger
scale applications.  As such, understanding machine learning requires
a background in both statistics and algorithms.

\begin{itemize}
\bibanno{Bishop, Christopher M.
2007.
{\it Pattern Recognition and Machine Learning}.
Springer.}%
{Introduction to general machine learning
techniques that is self contained, but assumes more mathematical
sophistication than this book.}
%
\bibanno{Hastie, Trevor, Robert Tibshirani and Jerome Friedman.
2009.
{\it The Elements of Statistical Learning}, Second Edition.
Springer.}
{Presupposes a great deal of mathematics, providing
fairly rigorous introductions to a wide range of algorithms and
statistical models.}
%
\bibanno{MacKay, David J.~C.
2002.
{\it Information Theory, Inference, and Learning Algorithms}.
Cambridge University Press.}
{More of a set of insightful case studies than a coherent textbook.
Presupposes a high degree of mathematical sophistication.}
%
\bibanno{Witten, Ian H.\ and Eibe Frank. 2005. {\it Data Mining: Practical
Machine Learning Tools and Techniques} (Second Edition).  Elsevier.}
{The most similar to this book, explaining general machine
learning techniques at a high level and 
linking to the Java implementations (from their widely used Weka toolkit).}
%
\end{itemize}




\section{Linguistics}

\noindent
Natural language processing is about language.  To fully appreciate
the models in LingPipe, it is necessary to have some background in
linguistics.  Unfortunately, most of the textbooks out there are based
on traditional Chomskyan theoretical linguistics which divorces
language from its processing, and from which natural language
processing diverged several decades ago.

\begin{itemize}
%
\bibanno{Bergmann, Anouschka, Kathleen Currie Hall, and Sharon Miriam Ross (editors).
2007.
{\it Language Files}, Tenth Edition.
Ohio State University Press.}
{Easy to follow and engaging overview of just about every
aspect of human language and linguistics.  Books on specific areas of
linguistics tend to be more theory specific.}
%
\end{itemize}


\section{Natural Language Processing}

\noindent
Natural language processing is largely about designing and coding computer
programs that operate over natural language data.  Because most NLP is
statistical these days, this requires not only a knowledge of linguistics,
but also of algorithms and statistics.

\begin{itemize}
\bibanno{Jurafsky, Daniel, James H.\ Martin.
2008.
{\it Speech and Language Processing}, Second Edition.
Prentice-Hall.}
{High-level overview of both speech and language
processing.}
%
\bibanno{Manning, Christopher D., Raghavan Prabhakar, and Hinrich Sch\"utze.
2008.
{\it Introduction to Information Retrieval}.
Cambridge University Press.}
{Good intro to IR and better intro to some NLP than
Manning and Sch\"utze's previous book, which is starting to feel
dated.}
%
\bibanno{Manning, Christopher D.\ and Hinrich Sch\"utze.
1999.
{\it Foundations of Statistical Natural Language Processing}.
MIT Press.}
{Needs a rewrite to bring it up to date, but still
worth reading given the lack of alternatives.}
\end{itemize}

% HACK TO GET RID OF UGLINESS ON MARKED PAGES
% \clearpage
% \thispagestyle{empty}
% \cleardoublepage
