\chapter{Tokenization}\label{chapter:tokenization}

Many natural-language processing algorithms operate at the word level,
such as most part-of-speech taggers and named-entity chunkers, some
classifiers, some parameterizations of spelling correction, etc.

A token is a generalized kind of word that is derived from segmenting
an input character sequence and potentially transforming the segments.
For instance, a search engine query like
\searchquery{London restaurants} might be converted into a boolean
search for the (case normalized) token \stringmention{london} and the
(plurality normalized) token \stringmention{restaurant}.

\section{Tokenizers and Tokenizer Factories}

LingPipe provides a package \code{com.aliasi.tokenizer} for handling
tokenization.  

\subsection{The \code{TokenizerFactory} Interface}

The \code{TokenizerFactory} factory interface defines a single method,
\code{tokenizer(char[],int,int)}, which takes a slice of a character
array as an argument and returns an instance of \code{Tokenizer}.

LingPipe's tokenizer factory implementations come in two flavors Basic
tokenizer factories are constructed from simple parameters.  For the
basic tokenizers with no parameters, a singleton instance is supplied
as a static constant in the class.  

Tokenizer filters are constructed from other tokenizer factories and
modify their outputs in some way, such as by case normalization,
stemming, or stop-word filtering.

In order to bundle a tokenizer factory with a model, it must be
serializable.  All of LingPipe's tokenizer factories are serializable,
including ones made up by composing a sequence of filters.


\subsection{The \code{Tokenizer} Base Class}

All tokenizers extend the abstract base class \code{Tokenizer}.
Tokenizers provide a stream of tokens.  An instance of
\code{Tokenizer} represents the state of tokenizing a particular
string.  

\subsubsection{Constructing a Tokenizer}

There is no state represented in the \code{Tokenizer} abstract class,
so there is a single no-argument constructor \code{Tokenizer()}.

Because tokenizers are usually created through the 
\code{TokenizerFactory} interface, most.
classes extending \code{Tokenizer} are not delcared to be public.
Instead, only the factory is visible, and the documentation for a
tokenizer's behavior will be in the factory's class documentation.

\subsubsection{Streaming Tokens}

The only method that is required to be implemented is
\code{nextToken()}, which returns the next token in the token stream
as a string, or \code{null} if there are no more tokens.  There is no
reference in a tokenizer itself to the underlying sequence of
characters.  

\subsubsection{Token Positions}

We often wish to maintain the position of a token in the underlying
text.  Given that tokens may be modified or even dropped altogether,
the position of a token is not necessarily going to be recoverable
from the sequence of tokens and whitespaces.  So the \code{Tokenizer}
class supplies methods \code{lastTokenStartPosition()} and
\code{lastTokenEndPosition()}, which return the index of the first
character and of one past the last character.  If no tokens have yet
been returned, these methods both return -1.  These positions are
relative to the slice being tokenized, not to the underlying character
array.  

The token position methods are implemented in the \code{Tokenizer}
base class to throw an \code{UnsupportedOperationException}.
Subclasses that want token positions should override these methods.
Tokenizer filters should almost always just pass the positions of the
tokens being modified.

\subsubsection{Iteration}

The method \code{iterator()} returns an iterator over strings
representing tokens.  In the \code{Tokenzer} base class, the iterator
is defined by delegation to the \code{nextToken()} method.  Thus
subclasses do not usually need to redefine this method.

This \code{iterator()} method allows the \code{Tokenizer} class to
implement the \code{Iterable<String>} interface.  Thus the tokens
can be read from a tokenizer with a for loop.  Given a tokenizer
factory \code{tokFac} and the character slice for input, the
usual idiom is
%
\begin{verbatim}
Tokenizer tokenizer = tokFac.tokenizer(cs,start,length);
for (String token : tokenizer) { ... }
\end{verbatim}

\subsubsection{Bulk Tokenization}

The method \code{tokenize()} returns an array of the remaining tokens
and \code{tokenize(List,List)} writes the remaining tokens and
whitespaces to the specified list.

\subsubsection{Serializability and Thread Safety}

Because they involve dynamic state, tokenizers are almost never
serializable and almost never thread safe.  

 
\subsubsection{Streaming Whitespaces}

Over time, LingPipe has moved from the use of whitespace returned from
tokenizers to token start and end positions.  Unless otherwise noted,
tokenizers need not concern themselves with whitespace.  LingPipe's
built-in tokenizers almost all define the whitespace to be the 
string between the last token and the next token, or the empty string
if that is not well defined.

The method \code{nextWhitespace()} returns the next whitespace from
the tokenizer.  ``White space'' is the general term for the material
between tokens, because in most cases, all non-whitespace is part of
some token.  LingPipe's \code{Tokenizer} class generalizes the notion
of whitespace to arbitrary strings.

Each token is preceded by a whitespace and the sequence ends with a
whitespace.  That is, the sequence goes whitespace, token, whitespace,
token, \ldots, whitespace, token, whitespace.  So the number of
whitespaces is one greater than the number of tokens, and the minimum
output of a tokenizer is a single whitespace.

If the \code{nextWhitespace()} method is not implemented by a
subclass, the implementation inherited from the \code{Tokenizer} base
class simply returns a string consisting of a single space character,
\unicode{0020}, \unicodedesc{space}.

If the \code{nextWhitespace()} method is implemented to return the
text between tokens, tokens do not overlap, and the string for a token
is not modified in any way, then concatenating the sequence of
whitespaces and tokens will produce the underlying characters that
were tokenized.


\subsection{Token Display Demo}

We provide a demo program \code{DisplayTokens}, which runs a tokenizer
over the command-line argument.  The \code{main()} method of the command
calls a utility method on a string variable \code{text} supplied
on the command line, using a built-in tokenizer
%
\codeblock{DisplayTokens.1}
%
The \code{IndoEuropeanTokenizerFactory} class is in
\code{com.aliasi.tokenizer}, and provides a reusable instance through the static constant
\code{INSTANCE}.

The \code{displayTextPositions()} method just prints out the string on
a single line followed by lines providing indexes into the string.
This method won't display properly if there are newlines in the string
itself.

The work is actually done in the subroutine, the body of which is
%
\codeblock{DisplayTokens.2}
%
We first convert the character sequence \code{in} to a 
character array using the utility method \code{toCharArray(CharSequence)}
from the class \code{Strings} in the package \code{com.aliasi.util}.
then, we create the tokenizer from the tokenizer factory.
Next, we just iterate over the tokens, extract their start and end
positions, and print the results.

We provide a corresponding Ant target \code{display-tokens}, which
is given a single command-line argument consisting of the value of
the property \code{text}.  
%
\commandlinefollow{ant -Dtext="The note says, 'Mr. Sutton-Smith owes \$15.20.'" display-tokens}
\begin{verbatim}
The note says, 'Mr. Sutton-Smith owes $15.20.'
0123456789012345678901234567890123456789012345
0         1         2         3         4

START   END  TOKEN           START   END TOKEN
    0     3  |The|              26    27  |-|
    4     8  |note|             27    32  |Smith|
    9    13  |says|             33    37  |owes|
   13    14  |,|                38    39  |$|
   15    16  |'|                39    44  |15.20|
   16    18  |Mr|               44    45  |.|
   18    19  |.|                45    46  |'|
   20    26  |Sutton|
\end{verbatim}
%
In writing the string, we put the ones-place indexes below it, then
the tens-place indexes, and so on.  Because we count from zero, the
very first \charmention{T} has index 0, whereas the \charmention{M} in
\stringmention{Mr} has index 16 and the final apostrophe index 45.
The string is 46 characters long; the last index is always one less
than the length.

We then show the tokens along with their start and end positions.  As
always, the start is the index of the first character and the end is
one past the index of the last character.  Thus the name
\stringmention{Smith} starts at character 27 and ends on character 31.
This also means that the end position of one token may be the start
position of the next when there is no space between them.  For
example, \charmention{says} has an end position of 13 (which is
exclusive) and the following comma a start position of 13 (which is
inclusive).  


\section{LingPipe's Base Tokenizer Factories}

LingPipe provides several base tokenizer factories.  These may be combined
with filters, which we describe in the next section, to create compound
tokenizers.

\subsection{The \code{IndoEuropeanTokenizerFactory} Class}

LingPipe's \code{IndoEuropeanTokenizer} is a fairly fine-grained
tokenizer in the sense that it splits most things apart.  The notable
exception in this instance is the number \stringmention{15.20}, which
is kept as a whole token.  Basically, it consumes as large a token
as possible according to the following classes.

\subsubsection{Kinds of Tokens}

An alphanumeric token is a sequence of one or more digits or letters
as defined by the utility methods \code{isDigit()} and
\code{isCharacter()} methods in Java's \code{Character} class.  Thus
\stringmention{aaa}, \stringmention{A1}, and \stringmention{1234} are all
considered single tokens.

A token may consist of a combination of digits with any number of
token-internal periods or commas.  Thus \stringmention{1.4.2} is
considered a token, as is \stringmention{12,493.27}, but
\stringmention{a2.1} is three tokens, \stringmention{a2},
\stringmention{.} (a period), and \stringmention{1}.

A token may be an arbitrarily long sequence of hyphens (\code{-}) or
an arbitrarily long sequence of equal signs (\code{=}).  The former
are often used as en-dashes and em-dashes in ASCII-formatted text,
and longer sequences are often used for document structure.

Double grave accents (\code{``}) and double apostrophes (\code{''}) are
treated as single tokens, as they are often used to indicate quotations.

All other non-space characters, such as question marks or ampersands,
are considered tokens themselves.

\subsubsection{Construction}

The static constant \code{INSTANCE} refers to an instance of
\code{IndoEuropeanTokenizerFactory} that may be reused.   The
no-argument constructor may also be used.


\subsubsection{Thread Safety and Serializability}

The \code{IndoEuropeanTokenizerFactory} class is both thread safe and
serializable.  The deserialized object will be reference identical
to the factory picked out by the \code{INSTANCE} constant.



\subsection{The \code{CharacterTokenizerFactory} Class}

The \code{CharacterTokenizerFactory} treats each non-whitespace
\code{char} value in the input as a token.  The definition of
whitespace is from Java's \code{Character.isWhitespace()} method.
The whitespaces returned will consist of
all of the whitespace found between the non-whitespace characters.

For instance, for the string \stringmention{a dog}, there are four
tokens, \stringmention{a}, \stringmention{d}, \stringmention{o}, and
\stringmention{g}.  The whitespaces are all length zero other than 
for the single space between the \stringmention{a} and \stringmention{d}
characters.

This tokenizer factory is particularly useful for Chinese, where there
is no whitespace separating words, but words are typically only one or
two characters (with a long tail of longer words).

This class produces a token for each \code{char} value.  This means
that a surrogate pair consisting of a high surrogate UTF-16 value and
low surrogate UTF-16 value, which represent a single Unicode code
point, will produce two tokens.  Specifically, unicode code points
outside of the basic multilingual plane (BMP), that is with values at
or above \unicode{10000}, will produce two tokens (see \refsec{utf-16} for more
information).

The class is thread safe and serializable.  There is no constructor,
only a static constant \code{INSTANCE}.  The instance is also the
result of deserialization.


\subsection{The \code{RegExTokenizerFactory} Class}

For parsing programming languages, the lexical analysis stage
typically breaks a program down into a sequence of tokens.
Traditional C packages like Lex and Java packages like JavaCC specify
regular expressions for tokens (and context-free grammars with side
effects for parse trees).  Regular expression-based tokenization is
also popular for natural language processing.

LingPipe's \code{RegExTokenizerFactory} implements tokenizers where
the tokens are defined by running regular expression matchers in find
mode (see \refsec{regex-find}).  An instance may be constructed from a
\code{Pattern}, or from a \code{String} representing a regular
expression with optional integer flags (see \refsec{pattern-modes} for
information on how to use the flags and their numerical values).

The demo class \code{RegexTokens} implements a simple regular expression
tokenizer factory based on three command line arguments, one for the
regular expression, one for the flags, and one for the text being tokenized.
%
\codeblock{RegexTokens.1}
%

The Ant target \code{regex-tokens} runs the command, setting three
command line arguments for three properties, \code{regex} for the
string representation of the regular expression, \code{flags} for the
integer flags, and \code{text} for the text to be tokenized.
%
\commandlinefollow{ant -Dregex="{\bk}p\{L\}+" -Dflags=32 -Dtext="John likes 123-Jello." regex-tokens}
\begin{verbatim}
regex=|\p{L}+|
flags=32 (binary 100000)

John likes 123-Jello.
012345678901234567890
0         1         2

START   END TOKEN
    0     4  |John|
    5    10  |likes|
   15    20  |Jello|
\end{verbatim}
%
First we print the regex and the flags (in both decimal and binary
forms), then the string with indexes, and finally the tokens.  Because
we used the pattern \code{{\bk}p\{L\}+}, our tokens consist of
maximally long sequences of letter characters.  Other characters such
as spaces, numbers, punctuation, etc., are simply skipped so that they
become part of the whitespace.

Instances of \code{RegExTokenizerFactory} are serializable.  They are
also thread safe.


\subsection{The \code{NGramTokenizerFactory} Class}

An \code{NGramTokenizerFactory} produces tokens from an input string
consisting of all of the substrings of the input within specified size
bounds.  Substrings of an input string of length $n$ are typically
called $n$-grams (and sometimes $q$-grams).

Thus such a factory is constructed using a minimum and
maximum sequence size, with \code{NGramTokenizerFactory(int,int)}.
The sequences produced include the whitespaces between what we
typically think of as tokens.  Tokenizers such as these are especially
useful for extracting features for classifiers.

We provide an example implementation in the demo class
\code{NGramTokens}.  The only thing novel is the construction of
the tokenizer factory itself,
%
\codeblock{NGramTokens.1}
%

It takes three command-line arguments, the
minimum and maximum sequence lengths and the text to analyze.
These are supplied to Ant target \code{ngram-tokens} as 
properties \code{minNGram}, \code{maxNGram}, and \code{text}.
%
\commandlinefollow{ant -DminNGram=1 -DmaxNGram=3 -Dtext="I ran." ngram-tokens}
\begin{verbatim}
minNGram=1
maxNGram=3

I ran.
012345

START   END TOKEN          START   END TOKEN
    0     1  |I|               2     4  |ra|
    1     2  | |               3     5  |an|
    2     3  |r|               4     6  |n.|
    3     4  |a|               0     3  |I r|
    4     5  |n|               1     4  | ra|
    5     6  |.|               2     5  |ran|
    0     2  |I |              3     6  |an.|
    1     3  | r|
\end{verbatim}
%
You can see from the output that the tokenizer first outputs the
1-grams (unigrams), then the 2-grams (bigrams), then the 3-grams (trigrams).
%
\footnote{Curiously, we get a transition in common usage from Latin to Greek at
4-grams (tetragrams), and 5-grams (pentagrams), 6-grams (hexagrams)
etc.  Some authors stick to all Greek prefixes, preferring the terms
``monogram'' and ``digram'' to ``unigram'' and ``bigram.''}







\subsection{The \code{LineTokenizerFactory} Class}

A \code{LineTokenizerFactory} treats each line of input as a single
token.  It is just a convenience class extending
\code{RegExTokenizerFactory} supplying the regex \code{.+}.  

Line termination is thus defined as for regular expression patterns (see
\refsec{regex-lines} for the full set of line-end sequences
recognized).  Final empty lines are not included in the sequence of
tokens.

There is a static constant \code{INSTANCE} which may be used.  This is
also the value of deserialization.  Line tokenizer factories are thread
safe.


\section{LingPipe's Filtered Tokenizers}

Following the same filter-based pattern as we saw for Java's I/O,
LingPipe provides a number of classes whose job is to filter the
tokens from an embedded tokenizer.  In all cases, there will be
a base tokenizer providing tokens, the output 

