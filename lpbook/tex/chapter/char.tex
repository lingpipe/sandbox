\chapter{Characters and Strings}\label{chap:char}

\firstchar{F}ollowing Java, LingPipe uses Unicode character
representations.  In this chapter, we describe the Unicode character
set, discuss character encodings, and how strings and other character
sequences are created and used in Java.


\section{Character Encodings}

For processing natural language text, we are primarily concerned with
the representation of characters in natural languages.  A set of
(abstract) characters is known as a character set.  These range from
the relatively limited set of 26 characters (52 if you count upper and
lower case characters) used in English to the tens of thousands of
characters used in Chinese.


\subsection{What is a Character?}

Precise definitions of characters in human writing systems is a
notoriously tricky business.  

\subsubsection{Types of Character Sets}

Characters are used in language to represent sounds at the level of
phonemic segments \eg{Latin}, syllables \eg{Japanese Hirigana and
Katakana scripts, Linear B, and Cherokee}, or words or morphemes, also
known as logograms \eg{Chinese Han characters, Egyptian
hieroglyphics}.  

These distinctions are blurry.  For instance, both Chinese characters
and Egyptian hieroglyphs are typically pronounced as single syllables
and words may require multiple characters.

\subsubsection{Abstraction over Visualization}

One problem with dealing with characters is the issue of visual
representation versus the abstraction.  Character sets abstract away
from the visual representations of characters.  

A sequence of characters is also an abstract concept.  When
visualizing a sequence of characters, different writing systems lay
them out differently on the page.  When visualizing a sequence of
characters, we must make a choice in how to lay them out on a page.
English, for instance, traditionally lays out a sequence of characters
in left to right order horizontally across a page. 
Arabic, on the other hand, lays out a sequence of
characters on a page horizontally from right to left.  In contast,
Japanese was traditionally set from top to bottom with the following
character being under the preceding characters.

English stacks lines of text from top to bottom, whereas traditional
Japanese orders its columns of text from right to left.  

When combining multiple pages, English has pages that ``turn'' from
right to left, whereas Arabic and traditional Japanese conventionally
order their pages in the opposite direction.  Obviously, page order
isn't relevant for digital documents rendered a page at a time, such
as on e-book readers.

Even with conventional layouts, it is not uncommon to see caligraphic
writing or signage laid out in different directions.  It's not
uncommon to see English written from top to bottom or diagonally from
upper left to lower right.


\subsubsection{Compound versus Primitive Characters}

Characters may be simple or compound.  For instance, the character
\charmention{\"o} used in German is composed of a plain Latin \charmention{o}
character with an umlaut diacritic on top.  Diacritics are visual
representations added to other characters, and there is a broad range
of them in European languages.

Hebrew and Arabic are typically written without vowels.  In very
formal writing, vowels may be indicated in these languages with
diacritics on the consonants.  Devanagari, a script used to write
languages including Hindi and Nepali, includes pitch accent marks.

The Hangul script used for Korean is a compound phonemic system often
involving multiple glyphs overlaid to form syllabic units.


\subsection{Coded Sets and Encoding Schemes}

Eventually, we need to represent sequences of characters as sequences
of bytes.  There are two components to a full character encoding.

The first step is to define a coded character set, which assigns each
character in the set a unique non-negative integer code point.  For
instance, the English character 'A' (capital A) might be assigned the
code point 65 and 'B' to code point 66, and so on.  Code points are
conventionally written using hexadecimal notation, so we would
typically use the hexadecmial notation 0x41 instead of the decimal
notation 65 for capital A.

The second step is to define a character encoding scheme that maps
each code point to a sequence of bytes (bytes are often called octets
in this context).

Translating a sequence of characters in the character set to a
sequence of bytes thus consists of first translating to the sequence
of code points, then encoding these code points into a sequence of
bytes.


\subsection{Legacy Character Encodings}

We consider in this section some widely used character encodings, but
because there are literally hundreds of them in use, we can't be
exhaustive.  

The characters in these and many other character sets are part of
Unicode.  Because LingPipe and Java both work with Unicode, typically
the only thing you need to know about a character encoding in order
to convert data in that encoding to Unicode is the name of the
character encoding.

\subsubsection{ASCII}

ASCII is a character encoding consisting of a small character set of
128 code points.  Being designed by computer scientists, the numbering
starts at 0, so the code points are in the range 0--127 (inclusive).
Each code point is then encoded as the corresponding single unsigned
byte with the same value.  It includes characters for the standard
keys of an American typewriter, and also some ``characters''
representing abstract typesetting notions such as line feeds and
carriage returns as well as terminal controls like ringing the
``bell''.

The term ``ASCII'' is sometimes used informally to refer to character
data in general, rather than the specific ASCII character encoding.

\subsubsection{Latin1 and the Other Latins}\label{section:latin1}

Given that ASCII only uses 7 bits and bytes are 8 bits, the natural
extension is to add another 128 characters to ASCII.  Latin1
(officially named ISO-8859-1) did just this, adding common accented
European characters to ASCII, as well as punctuation like upside down
exclaimation points and question marks, French quote symbols, section
symbols, a copyright symbol, and so on.  

There are 256 code points in Latin1, and the encoding just uses the
corresponding unsigned byte to encode each of the 256 characters
numbered 0--255.

The Latin1 character set includes the entire ASCII character set.
Conveniently, Latin1 uses the same code points as ASCII for the ASCII
characters (code points 0--127).  This means that every ASCII encoding
is also a Latin1 encoding of exactly the same characters.

Latin1 also introduced ambiguity in coding, containing both an 'a'
character, the '\"{}' umlaut character, and their combination '\"{a}'.
It also includes all of 'a', 'e', and the compound '\ae', as well as
'1', '/', '2' and the compound $\frac{1}{2}$.

With only 256 characters, Latin1 couldn't render all of the characters
in use in Indo-European languages, so a slew of similar standards,
such as Latin2 (ISO-8859-2) for (some) Eastern European languages,
Latin3 (ISO-8859-1) for Turkish, Maltese and Esperanto, up through
Latin16 (ISO-8859-16) for other Eastern European languages, new styles
of Western European languages and Irish Gaelic.  

\subsubsection{Windows 1252}

Microsoft Windows, insisting on doing things its own way, uses a
character set known as Windows-1252, which is almost identical to Latin1,
but uses code points 128--159 as graphical characters, such as curly
quotes, the euro and yen symbols, and so on.

\subsubsection{Big5 and GB(K)}

The Big5 character encoding is used for traditional Chinese scripts
and the GB (more recently GBK) for simplified Chinese script.  Because
there are so many characters in Chinese, both were designed to use a
fixed-width two-byte encoding of each character.  Using two bytes
allows up to $2^{16} = 65,536$ characters, which is enough for most
of the commonly used Chinese characters.

\subsection{Unicode}

Unicode is the de facto standard for character sets of all kinds.  The
latest version is Unicode 5.2, and it contains over 1,000,000 distinct
characters drawn from hundreds of languages (technically, it assumes
all code points are in the range 0x0 to 0x10FFFF (0 to 1,114,111 in
decimal notation).

One reason Unicode has been so widely adopted is that it contains
almost all of the characters in almost all of the widely used
character sets in the world.  It also happens to have done so in a
thoughful and well-defined manner.

Conveniently, Unicode's first 256 code points (0--255) exactly match
those of Latin1, and hence Unicode's first 128 code points (0--127)
exactly match those of ASCII.

Unicode code points are conventionally displayed in text using a
hexadecimal representation of their code point padded to at least four
digits with initial zeroes, prefixed by \code{U+}.  For instance, code
point 65 (hex 0x41), which represents capital A in ASCII, Latin1 and
Unicode, is conventionally written \unicode{0041} for Unicode.

The problem with having over a million characters is that it would
require three bytes to store each character ($2^{24} = 16,777,216$).
This is very wasteful for encoding languages based on Latin characters
like Spanish or English.

\subsubsection{Unicode Transformation Formats}

There are three standard encodings for Unicode code points that are
specified in the Unicode standard, UTF-8, UTF-16, and UTF-32 (``UTF''
stands for ``Unicode transformation format'').%
%
\footnote{There are also non-standard encodings of (subsets of) 
Unicode, like Apache Lucene's and the one in Java's
\code{DataOutput.writeUTF()} method.}
%
The numbers represent the coding size; UTF-8 uses single bytes,
UTF-16 pairs of bytes, and UTF-32 quadruples of bytes.

\subsubsection{UTF-32}

The UTF-32 encoding is fixed-width, meaning each character occupies
the same number of bytes (in this case, 4).  As with ASCII and
Latin1, the code points are encoded directly as bits in base 2.

There are actually two UTF-32 encodings, UTF-32BE and UTF-32LE,
depending on the order of the four bytes making up the 32 bit blocks.
In the big-endian (BE) scheme, bytes are ordered from left to right
from most significant to least significant digits (like for bits in a
byte).  

In the little-endian (LE) scheme, they are ordered in the
opposite direction.  For instance, in UTF-32BE, \unicode{0041}
(capital A), is encoded as 0x00000041, indicating the four byte
sequence 0x00, 0x00, 0x00, 0x41.  In UTF32-LE, it's encoded as
0x41000000, corresponding to the byte sequence 0x41, 0x00, 0x00, 0x00.

There is also an unmarked encoding scheme, UTF-32, which tries to
infer which order the bytes come in.  Due to restrictions on the
coding scheme, this is usually possible.  In the simplest case,
0x41000000 is only legal in little endian, because it'd be out of
range in big-endian notation.  

Different computer processors use different endian-ness
internally.  For example, Intel and AMD x86 architecture is little
endian, Motorola's PowerPC is big endian.  Some hardware, such
as Sun Sparc, Intel Itanium and the ARM, is called bi-endian, because
endianness may be switched.

\subsubsection{UTF-16}\label{section:utf-16}

The UTF-16 encoding is variable length, meaning that different code
points use different numbers of bytes.  For UTF-16, each character is
represented using either two bytes (16 bits) or four bytes (32 bits).

Because there are two bytes involved, their order must be defined.  As
with UTF-32, UTF-16BE is the big-endian order and UTF-16LE the
little-endian order.

In UTF-16, code points below \unicode{10000} are represented using a
single byte in the natural base-2 encodings.  For instance, our old
friend \unicode{0041} (capital A) is represented in big endian as
the sequence of bytes 0x00, 0x41.  

Code points at or above \unicode{10000} are represented using four
bytes arranged in two pairs.  Calculating the two pairs of bytes
involves bit-twiddling.  For instance, given a code point
\code{codepoint}, the following code
calculates the four bytes.
%
\footnote{This code example and the following one for decoding were
adapted from the example code supplied by the Unicode Consortium in
their FAQ at 
\urldisplay{http://unicode.org/faq/utf\_bom.html}
}
%
\begin{verbatim}
static final int LEAD_OFFSET = 0xD800 - (0x10000 >> 10);
static final int SURROGATE_OFFSET = 0x10000 - (0xD800 << 10) - 0xDC00;

int lead = LEAD_OFFSET + (codepoint >> 10);
int trail = 0xDC00 + (codepoint & 0x3FF);

int byte1 = lead >>> 8;
int byte2 = lead & 0xFF;
int byte3 = trail >>> 8;
int byte4 = trail & 0xFF;
\end{verbatim}
%
Going the other way around, given the four bytes, we
can calculate the code point with the following code.
%
\begin{verbatim}
int lead = byte1 << 8 | byte2;
int trail = byte3 << 8 | byte4;

int codepoint = (lead << 10) + trail + SURROGATE_OFFSET;
\end{verbatim}

Another way to view this transformation is through the following
table.%
%
\footnote{Adapted from {\it Unicode Standard Version 5.2}, Table 3-5.}
%
\begin{center}
\begin{tabular}{|r|r|}
\hline
\tblhead{Code Point Bits} & \tblhead{UTF-16 Bytes} 
\\ \hline
\code{xxxxxxxx xxxxxxxx} & \code{xxxxxxxx xxxxxxxx}
\\ \hline
\code{000uuuuuxxxxxxxxxxxxxxxx} & \code{110110ww wwxxxxxx 110111xx xxxxxxxx}
\\ \hline
\end{tabular}
\end{center}
%
Here \code{wwww} = \code{uuuuu} - 1 (interpreted as numbers then
recoded as bits).  The first line indicates is that if the value fits
in 16 bits, those 16 bits are used as is.  Note that this
representation is only possible because not all code points between 0
and $2^{16}-1$ are legal.  The second line indicates that if we have a
code point above \unicode{10000}, we strip off the high-order five
bits uuuuu and subtract one from it to get four bits wwww; again, this
is only possible because of the size restriction on uuuuu.  Then,
distribute these bits in order across the four bytes shown on the
right.  The constants 110110 and 110111 are used to indicate what are
known as surrogate pairs.

Any 16-bit sequence starting with 110110 is the leading half of a
surrogate pair, and any sequence starting with 110111 is the trailing
half of a surrogate pair.  Any other sequence of initial bits means a
16-bit encoding of the code point.  Thus it is possible to determine
by looking at a pair of bytes whether we have a whole character, the
first half of a character represented as a surrogate pair, or the
second half of a character represented as a pair.

\subsubsection{UTF-8}\label{section:utf-8}

UTF-8 works in much the same way as UTF-16 (see the previous section),
only using single bytes as the minimum code size.  We
use the table-based visualization of the encoding scheme.%
%
\footnote{Adapted from {\it Unicode Standard Version 5.2},  Table 3-6.}
%
\begin{center}
\begin{tabular}{|r|r|}
\hline
\tblhead{Code Point Bits} & \tblhead{UTF-8 Bytes}
\\ \hline
\code{0xxxxxxxx} & \code{0xxxxxxx}
\\ \hline
\code{00000yyyyyxxxxxx} & \code{110yyyyy 10xxxxxx}
\\ \hline
\code{zzzzyyyyyyxxxxxx} & \code{1110zzzz 10yyyyyy 10xxxxxx}
\\ \hline
\code{000uuuuuzzzzyyyyyyxxxxxx} & \code{11110uuu 10uuzzzz 10yyyyyy 10xxxxxx}
\\ \hline
\end{tabular}
\end{center}
%
Thus 7-bit ASCII values, which have values from \unicode{0000} to
\unicode{007F}, are encoded directly in a single byte.  Values from
\unicode{0080} to \unicode{07FF} are represented with two bytes, 
values between \unicode{0800} and \unicode{FFFF} with three bytes, 
and values between \unicode{10000} and \unicode{10FFFF} with four bytes.


\subsubsection{Non-Overlap Principle}

The UTF-8 and UTF-16 encoding schemes obey what the Unicode Consortium
calls the ``non-overlap principle.''  Technically, the leading,
continuing and trailing code units (bytes in UTF-8, pairs of bytes in
UTF-16) overlap.  Take UTF-8 for example.  Bytes starting with
\code{0} are singletons, encoding a single character in the range 
\unicode{0000} to \unicode{007F}.  Bytes starting with \code{110}
are the leading byte in a two-byte sequence representing a character
in the range \unicode{0080} to \unicode{07FF}.  Similarly, bytes
starting with \code{1110} represent the leading byte in a three-byte
sequence, and \code{11110} the leading byte in a four-byte sequence.
Any bytes starting with \code{10} are continuation bytes.

What this means in practice is that it is possible to reconstruct
characters locally, without going back to the beginning of a file.
At most, if a byte starts with \code{10} you have to look back
up to three bytes to find the first byte in the sequence.

Furthermore, the corruption of a byte is localized so that the rest of
the stream doesn't get corrupted if one byte is corrupted.

Another advantage is that the sequence of bytes encoding one character
is never a subsequence of the bytes making up another character.  This
makes applications like search more robust.


\subsubsection{Byte Order Marks}\label{section:byte-order-marks}

In the encoding schemes which are not explicitly marked as being
little or big endian, namely UTF-32, UTF-16 and UTF-8, it is also
legal, according to the Unicode standard, to prefix the sequence of
bytes with a byte-order mark (BOM).  It is not legal to have
byte-order marks preceding the explicitly marked encodings, UTF-32BE,
UTF-32LE, UTF-16BE, UTF-16LE.

For UTF-32, the byte-order mark is a sequence of four bytes indicating
whether the following encoding is little endian or big endian.  the
sequence 0x00, 0x00, 0xFE, 0xFF indicates a big-endian encoding and
the reverse, 0xFF, 0xFE, 0x00, 0x00 indicates little endian.

For UTF-16, two bytes are used, 0xFE, 0xFF for big endian, and the
reverse, 0xFF, 0xFE for little endian.

Although superfluous in UTF-8, the three byte sequence 0xEF, 0xBB,
0xBF is a ``byte order mark'', though there is no byte order to mark.  
As a result, all three UTF schemes may be distinguished by inspecting
their initial bytes.

Any text processor dealing with Unicode needs to handle the byte order
marks.  Some text editing packages automatically insert byte-order
marks for UTF encodings and some don't.

\subsubsection{Character Types and Categories}\label{section:unicode-categories}

The Unicode specification defines a number of character types and
general categories of character.  These are useful in Java
applications because characters may be examined programatically for
their class membership and the classes may be used in regular
expressions.

For example, category ``Me'' is the general category for enclosing
marks, and ``Pf'' the general category for final quote punctuation,
whereas ``Pd'' is the general category for dash-like punctuation, and
``Sc'' the category of currency symbols.  Unicode supplies a notion of
case, with the category ``Lu'' used for uppercase characters and
``Ll'' for lowercase.

Unicode marks the directionality in which characters are typically
wirtten using character types like ``R'' for right-to-left
and ``L'' for left-to-right.

The full set of character categories recognized by Java is detailed in
the constant documentation for \code{java.lang.Character}.  There's a
missing component to the implementation, which makes it hard to take a
given \code{char} value (or code point) and find its Unicode
properties.  To that end, we've written a utility class
\code{UnicodeProperties} that displays properties of Unicode.  The
code just enumerates the constants in \code{Character}, indexes them
by name, writes out a key to the values produced by
\code{Character.getType(char)} along with their standard abbreviations
(as used in regular expressions) and names.

The code may be run from the Ant target \code{unicode-properties}.
This target will print the key to the types, then the type of
each character in a range specified by the properites 
\code{low.hex} and \code{high.hex} (both inclusive).
%
\commandlinefollow{ant -Dlow.hex=0 -Dhigh.hex=FF unicode-properties}
\begin{verbatim}
KEY
 0=Cn UNASSIGNED
 1=Lu UPPERCASE_LETTER
 2=Ll LOWERCASE_LETTER
 3=Lt TITLECASE_LETTER
 4=Lm MODIFIER_LETTER
 5=Lo OTHER_LETTER
 6=Mn NON_SPACING_MARK
 7=Me ENCLOSING_MARK
 8=NSM DIRECTIONALITY_NONSPACING_MARK
 9=BN DIRECTIONALITY_BOUNDARY_NEUTRAL
 a=Nl LETTER_NUMBER
 b=No OTHER_NUMBER
 c=Zs SPACE_SEPARATOR
 d=Zl LINE_SEPARATOR
 e=Zp PARAGRAPH_SEPARATOR
 f=LRO DIRECTIONALITY_LEFT_TO_RIGHT_OVERRIDE
10=Cf FORMAT
11=RLO DIRECTIONALITY_RIGHT_TO_LEFT_OVERRIDE
12=Co PRIVATE_USE
13=Cs SURROGATE
14=Pd DASH_PUNCTUATION
15=Ps START_PUNCTUATION
16=Pe END_PUNCTUATION
17=Pc CONNECTOR_PUNCTUATION
18=Po OTHER_PUNCTUATION
19=Sm MATH_SYMBOL
1a=Sc CURRENCY_SYMBOL
1b=Sk MODIFIER_SYMBOL
1c=So OTHER_SYMBOL
1d=Pi INITIAL_QUOTE_PUNCTUATION
1e=Pf FINAL_QUOTE_PUNCTUATION

VALUES
 0 LRO DIRECTIONALITY_LEFT_TO_RIGHT_OVERRIDE
...
20 Zs SPACE_SEPARATOR
21 Po OTHER_PUNCTUATION
...
24 Sc CURRENCY_SYMBOL
...
28 Ps START_PUNCTUATION
29 Pe END_PUNCTUATION
...
2b Sm MATH_SYMBOL
...
2d Pd DASH_PUNCTUATION
...
30 BN DIRECTIONALITY_BOUNDARY_NEUTRAL
...
41 Lu UPPERCASE_LETTER
...
5e Sk MODIFIER_SYMBOL
5f Pc CONNECTOR_PUNCTUATION
...
61 Ll LOWERCASE_LETTER
...
a6 So OTHER_SYMBOL
...
ab Pi INITIAL_QUOTE_PUNCTUATION
...
ad Cf FORMAT
...
b2 No OTHER_NUMBER
...
bb Pf FINAL_QUOTE_PUNCTUATION
\end{verbatim}
%
We specified the entire Latin1 range, but only display the first
instance of each type above rather than all 256.  To help interpret
the table, let's consider the types we have.  

\unicode{0000}, \unicodedesc{<control> null}, is the null control
character, which is typed as a directionality override; backspaces,
linefeeds, and other control characters get the same type.  This can
be confusing for newlines and their ilk, which aren't treated as space
characters in the Unicode typology, but are in contexts like regular
expressions.  

The first character typed as a space is \unicode{0020},
\unicodedesc{space}, the ordinary (ASCII) space character.

\unicode{0021}, \unicodedesc{exclamation mark} (\charmention{!}) is
classified as other punctuation, meaning it's not half of a matched
pair like brackets.  Characters \unicode{0028} and \unicode{0029},
\unicodedesc{left parenthesis} (\charmention{(}) and
\unicodedesc{right parenthesis} (\charmention{)}), the round
parentheses, are assigned start and end punctuation types
respectively.  Other brackets such as the ASCII curly and square
varieties, get the same categories.

\unicode{0024}, \unicodedesc{dollar sign} (\charmention{\$}), is
marked as a currency symbol, as are the signs for the euro, the yen,
and other currencies.  

\unicode{002B}, \unicodedesc{plus sign} (\charmention{+}), the symbol
for arithmetic addition, is characterized as a math symbol.

\unicode{002D}, \unicodedesc{hyphen-minus} (\charmention{-}), used for
hyphenation and for arithmetical subtraction, is typed as a dash
punctuation.  This ambiguous symbol presents a problem for a typology
based on unique types for characters; the problem with whitespace and
newline is similar.  \unicode{005F}, \unicodedesc{low line}
(\charmention{\_}), the ASCII underscore character, is typed as
connector punctuation.

\unicode{0030}, \unicodedesc{digit zero} (\charmention{0}), is oddly
typed as a direcitonality-boundary-neutral character; it'd seem more
natural to mark it as a number.  The first character marked as a
number is \unicode{00B2}, \unicodedesc{superscript two}
(${}^{\rm\small \charmention{2}}$), which is assigned to the other
number type.

\unicode{0041}, \unicodedesc{latin capital letter a}
(\charmention{A}), and \unicode{0061}, \unicodedesc{latin small letter
  a} (\charmention{a}), are typed as uppercase and lowercase letters
respectively.  

\unicode{005E}, \unicodedesc{circumflex accent} (\charmention{\^{}}),
is typed as a modifier symbol.  \unicode{00A6}, the Latin1
\unicodedesc{broken bar}, which is usually displayed as a broken
horizontal bar, is typed as an other symbol.

\unicode{00AB}, \unicodedesc{left-pointing double angle quotation
  mark} (\charmention{}), and \unicode{00BB},
\unicodedesc{right-pointing double angle quotation mark}
(\charmention{}), traditionally known as the left and right guillemet,
and used in French like open and close quotation marks in English, are
initial quote and final quote punctuation categories.  The ``curly''
quotes are also divided into open and initial and final quote
punctuation.  In contrast, \unicode{0022}, \unicodedesc{quotation
  mark} (\charmention{"}), and \unicode{0027},
\unicodedesc{apostrophe} (\charmention{'}), the standard ASCII
undirected quotation mark and apostrophe, are marked as other
punctuation, even though they may be used for quotations.

\unicode{00AD}, \unicodedesc{soft hyphen}, is typed as a format
character.  It's used in typesetting for continuation hyphens used to
split words across lines.  This allows a word split with a hyphen
across lines to be put back together.  This character shows up in
conversions from PDF documents generated by \LaTeX, such as the one
for this book.







\subsubsection{Normalization Forms}\label{section:unicode-normalization-forms}

In order to aid text processing efforts, such as search, Unicode
defines several notions of normalization.  These are defined in great
detail in Unicode Standard Annex~\#15, \booktitle{The Unicode
Normalization Forms}.

First, we consider decomposition of precomposed characters.  For
example, consider the three Unicode characters: \unicode{00E0},
\unicodedesc{latin small letter a with grave}, rendered
\charmention{\`a}, \unicode{0061}, \unicodedesc{latin small letter a}, 
rendered as \charmention{a}, and \unicode{0300},
\unicodedesc{combining grave accent}, which is typically rendered
by combining it as an accent over the previous character.  For most
purposes, the combination \unicode{00E0} means the same thing in text
as \unicode{0061} followed by \unicode{0300}.

There are four basic normalization forms, reproduced here from Table 1
in the normalization annex document.
%
\begin{center}
\begin{tabular}{|l|p{0.5\textwidth}|}
\hline
\tblhead{Normalization Form} & \tblhead{Description} 
\\ \hline
NFD & canonical decomposition
\\ \hline
NFC & canonical decomposition, followed by canonical composition
\\ \hline
NFKD & compatibility decomposition
\\ \hline
NFKC & compatibility decomposition, followed by canonical composition
\\ \hline
\end{tabular}
\end{center}
%
The first two forms, NFD and NFC, first perform a canonical
decomposition of character sequences.  This breaks compound characters
into a base character plus accent marks, sorting multiple accent marks
into a canonical order.  For instance, the canonical decomposition of
the sequence \unicode{0061}, \unicode{0300} (small a followed by grave
accent) is the same as that of the single character \unicode{00E0} (small a
with grave accent), namely the sequence
\unicode{0061}, \unicode{0300}.  

In general, a full canonical decomposition continues
to decompose characters into their components until a sequence of
non-decomposible characters is produced.  Furthermore, if there is
more than one nonspacing mark (like \unicode{0300}), then they are
sorted into a canonical order.

NFC differs from NFD in that it further puts the characters back
together into their canonical composed forms, whereas NFD leaves them
as a sequence.  Applying NFC normalization to either the sequence
\unicode{0061}, \unicode{0300} or the precomposed character
\unicode{00E0} results in the canonical composed character \unicode{00E0}.

The second two forms, NFKD and NFKC, replace the canonical
decomposition with a compatibility decomposition.  This reduces
different Unicode code points that are in some sense the same
character to a common representation.  As an example, the character
\unicode{2075}, \unicodedesc{superscript five}, is typically written
as a superscripted version of \unicode{0035}, \unicodedesc{digit five},
namely as ${}^{\mbox{\footnotesize\charmention{5}}}$.
Both involve the underlying character \charmention{5}.  With
canonical decomposition, \unicode{2075} is normalized to \unicode{0035}.
Similarly, the so-called ``full width'' forms of ASCII characters, 
\unicode{FF00} to \unicode{FF5E}, are often used in Chinese; each
has a corresponding ASCII character in \unicode{0021} to \unicode{007E}
to which it is compatibility normalized.  

There are special behaviors for multiple combining marks.  When
recombining, only one mark may wind up being combined, as the
character with multiple marks may not have its own precomposed code
point.  

There is also special behavior for composites like the ligature
character \charmention{fi}, which in Unicode gets its own precomposed
code point, \unicode{FB01}, \unicodedesc{latin small ligature fi}.
Only the compatibility version decomposes the ligature into
\unicode{0066}, \unicodedesc{latin small f}, followed by \unicode{0069},
\unicodedesc{latin small i}.

Two strings are said to be canonical equivalent if they have
the same canonical decomposition, and compatibility equivalent
if they have the same compatibility decomposition.  

The normalizations are nice in that if strings are canonical
equivalents, then their NFC normalizations are the same.  If two
strings are canonical equivalents, their NFD normalizations are also
the same.  The NFD normalization and NFC normalization of a single
string are not necessarily the same.  The same properties hold for
compatibility equivalents and NFKC and NFKD normalizations.  Another
nice property is that the transformations are stable in that applying
a normalization twice (in the sense of applying it once, then applying
it again to the output), produces the same result as applying it
once \ie{its an idemopotent transformation}. 

We show how to convert character sequences to their canonical forms
programatically using the ICU package in \refsec{icu-unicode-normalization}.


\section{Encoding Java Programs}

Java has native support for many encodings, as well as the ability to
plug in additional encodings as the need arises (see
\refsec{supported-encodings}).  Java programs themselves may be
written in any supported character encoding.

If you use characters other than ASCII characters in your Java
programs, you should provide the \code{javac} command with a
specification of which character set you used.  If it's not specified,
the platform default is used, which is typically Windows-1252 on
Windows systems and Latin1 on other systems, but can be modified as
part of the install.  

The command line to write your programs in Chinese using Big5 encoding
is
%
\begin{verbatim}
javac -encoding Big5 ...
\end{verbatim}
%
For Ant, you want to use the \code{encoding} attribute on the
\code{javac} task, as in
%
\begin{verbatim}
<javac encoding="Big5" ...
\end{verbatim}


\subsection{Unicode Characters in Java Programs}\label{section:char-unicode-java}

Even if a Java program is encoded in a small character set encoding
like ASCII, it has the ability to express arbitrary Unicode characters
by using escapes.  In a Java program, the sequence
\code{{\bk}u}\codeVar{xxxx}  behaves like the character
\unicode{\codeVar{xxxx}}, where \codeVar{xxxx} is a hexadecimal
encoded value padded to four characters with leading zeroes.

For instance, instead of writing

\begin{verbatim}
int n = 1;
\end{verbatim}
%
we could write
%
\begin{verbatim}
\u0069\u006E\u0074\u0020\u006E\u0020\u003D\u0020\u0031\u003B
\end{verbatim}
%
and it would behave exactly the same way, because
\charmention{i} is \unicode{0069}, \charmention{n} is
\unicode{006E}, \charmention{t} is \unicode{0074},
the space character is \unicode{0020}, and so on, up
through\charmention{;}, which is \unicode{003B}.

As popular as Unicode is, it's not widely enough supported in
components like text editors and not widely enough understood among
engineers.  Writing your programs in any character encoding other than
ASCII is thus highly error prone.  Our recommendation, which we follow
in LingPipe, is to write your Java programs in ASCII, using the full
\code{{\bk}u}\codeVar{xxxx} form of non-ASCII characters.


\section{\code{char} Primitive Type}

Java's primitive \code{char} data type is essentially a 16-bit (two
byte) unsigned integer. Thus a \code{char} can hold values between 0
and $2^16-1 = 65535$.  Furthermore, arithmetic over \code{char} types
works like an unsigned 16-bit representation, including casting a
\code{char} to a numerical primitive type \ie{\code{byte}, \code{short}, 
\code{int}, or \code{long}}.

\subsubsection{Character Literals}\label{section:character-literals}

In typical uses, instances of \code{char} will be used to model text
characters.  For this reason, there is a special syntax for character
literals.  Any character wrapped in single quotes is treated as
a character in a program.  For instance, we can assign a character
to a variable with 
%
\begin{verbatim}
char c = 'a'; 
\end{verbatim}
%
Given the availability of arbitrary Unicode characters in Java
programs using the syntax \code{{\bk}u}\codeVar{xxxx}, it's easy
to assign arbitrary Unicode characters.  For instance, 
to assign the variable \code{c} to the character \charmention{\`a},
\unicode{00E0}, use
%
\begin{verbatim}
char c = '\u00E0'; 
\end{verbatim}
%
Equivalently, because values of type \code{char} act like unsigned
short integers, it's possible to directly assign integer values
to them, as in
%
\begin{verbatim}
char c = 0x00E0;
\end{verbatim}
%
The expression \code{{\bk}0x00E0} is an integer literal in hexadecimal
notation.  In general, the compiler will infer which kind of integer
is intended.  

Because the Java \code{char} data type is only 16 bits, the Unicode
escape notation only goes up to \unicode{FFFF}.

Several characters may not be used directly within character (or
string) literals: new lines, returns, form feeds, backslash, or a
single quote character.  We can't just drop in a Unicode escape
like \code{{\bk}u000A} for a newline, because that behaves just
like a newline itself.  To get around this problem, Java provides
some special escape sequences which may be used in character and
string literals.  
%
\begin{center}
\begin{tabular}{lll}
\tblhead{Escape} & \tblhead{Code Point} & \tblhead{Description}
\\
\code{{\bk}n} & \unicode{000A} & \unicodedesc{line feed}
\\
\code{{\bk}t} & \unicode{0009} & \unicodedesc{character tabulation}
\\
\code{{\bk}b} & \unicode{0008} & \unicodedesc{backspace}
\\
\code{{\bk}r} & \unicode{000D} & \unicodedesc{carriage return}
\\
\code{{\bk}f} & \unicode{000C} & \unicodedesc{form feed}
\\
\code{{\bk}{\bk}} & \unicode{005C} & \unicodedesc{reverse solidus}
\\
\code{{\bk}'} & \unicode{0027} & \unicodedesc{apostrophe}
\\
\code{{\bk}"} & \unicode{0022} & \unicodedesc{quotation mark}
\end{tabular}
\end{center}
%
For instance, we'd write
%
\begin{verbatim}
char c = '\n';
\end{verbatim}
%
to assign the newline character to the variable \code{c}.

\subsubsection{Character Arithmetic}

A character expression may be assigned to an integer result, as in
%
\begin{verbatim}
int n = 'a';
\end{verbatim}
%
After this statement is executed, the value of the variable \code{n}
will be 97.  This is often helpful for debugging, because it allows
the code points for characters to be printed or otherwise examined.

It's also possible, though not recommended, to do arithmetic with Java
character types.  For instance, the expression \code{'a'+1} is equal
to the expression \code{'b'}.

Character data types behave differently in the context of string
concatenation than integers.  The expressions \code{'a' + "bc"} and
\code{{\bk}u0061 + "bc"} are identical because the compiler treats
\code{{\bk}u0061} and \code{'a'} identically, because the Unicode
code point for \charmention{a} is \unicode{0061}.  Both expressions
evaluate to a string equal to \code{"abc"}.  

The expression \code{0x0061 + "bc"} evaluates to \code{"97bc"},
because \code{0x0061} is taken as a hexadecimal integer literal, which
when concatenated to a string, first gets converted to its
string-based decimal representation, \code{"97"}.  There are static methods in
\code{java.lang.Integer} to convert integers to string-based
representations.  The method \code{Integer.toString(int)} may be used
to convert an integer to a string-based decimal notation;
\code{Integer.toHexString(int)} does the same for hex.

\subsubsection{Characters Interpreted as UTF-16}

Java made the questionable decision to use 2-byte representations for
characters.  This is both too wide and too narrow.  It's wasteful for
representing text in Western European languages, where most characters
have single-byte representations.  It's also too narrow, in that
any code point above \unicode{FFFF} requires two characters to
represent in Java; to represent code points would require an
integer (primitive \code{int} type, which is 4 bytes).
%
\footnote{The designers' choice is more understandable given that there were no
code points that used more than a single pair of bytes for UTF-16 when
Java was designed, though Unicode all along advertised that it would
continue to add characters and was not imposing a 16-bit upper limit.}

When \code{char} primitives are used in strings, they are interpreted
as UTF-16.  For any code point that uses only two bytes in UTF-16,
this is just the unsigned integer representation of that code point.
This is why UTF-16 seemed so natural for representing characters in
the original Java language design.  Unfortunately, for code points
requiring four bytes in UTF-16, Java requires two characters, so now
we have all the waste of 2-bytes per character and all the
disadvantages of having Unicode code points that require multiple
characters.


\section{\code{Character} Class}

Just like for the numerical types, there is a class,
\code{Character}, used to box an underlying primitive
of type \code{char} with an immutable reference.  As for numbers, the
preferred way to acquire an instance of \code{Character} is with the
static factory method \code{Character.valueOf(char)}.  Autoboxing and
unboxing works the same way as for numerical values.  

Like the numerical classes, the \code{Character} class is also
serializable and comparable, with comparison being carried out based
on interpreting the underlying \code{char} as an unsigned integer.
There are also utilities in Java to sort sequences of \code{char}
values (such as strings) in a locale-specific way, because not every
or dialect of a language uses the same lexicographic sorting.

Equality and hash codes are also defined similarly, so that two
character objects are equal if and only if they reference the same
underlying character.

\subsection{Static Utilities}\label{section:char-character-utils}

The \code{Character} class supports a wide range of Unicode-sensitive
static utility constants and methods in addition to the factory method
\code{valueOf()}.

For each of the Unicode categories, there is a constant, represented
using a byte, because the class predates enums.  For instance, the
general category ``Pe'' in the Unicode specification is represented by
the constant \code{END\_PUNCTUATION} and the general category of
mathematical symbols, ``Sm'' in Unicode, is represented by the
constant\code{MATH\_SYMBOL}.

For many of these constants, there are corresponding methods.  For
instance, the method \code{getDirectionality(char)} returns the
directionality of a character as a byte value, which can then be tested
against the possible values represented as static constants.  There
are also useful methods for determining the category of a characer,
such as \code{isLetter(char)} and \code{isWhitespace(char)}.

Because multiple Java \code{char} instances might be needed to
represent a code point, there are also methods operating on code
points directly using integer (primitive \code{int}) arguments.  For
instance, \code{isLetter(int)} is the same as \code{isLetter(char)}
but generalized to arbitrary code points.  The method
\code{charCount(int)} returns the number of \code{char} values
required to represent the code point (so the value will be 1 or 2,
reflecting 2 or 4 bytes in UTF-16).  The methods
\code{isLowSurrogate(char)} and \code{isHighSurrogate(char)} determine
if the \code{char} represents the first or second half of the UTF-16
representation of a code point above \unicode{FFFF}.  

The method \code{charPointAt(char[],int)} determines the code point
that starts at the specified index int he array of \code{char}.  The
method \code{charPointCount(char[],int,int)} returns the number of
code points encoded by the specified \code{char} array slice.


\section{\code{CharSequence} Class}

Java provides an interface for dealing with sequences of characters
aptly named \code{CharSequence}.  It resides in the package
\code{java.lang}, so it's automatically imported.  Implementations
include strings and string builders, which we describe in the next
sections.

The character sequence interface specifies a method \code{length()}
that returns the number of characters in the sequence.  

The method \code{charAt(int)} may be used to return the character at a
specified position in the string.  Like arrays, positions are numbered
starting from zero.  Thus the method must be passed an integer in the
range from zero (inclusive) to the length of the string (exclusive).
Passing an index that's out of range raises an
\code{IndexOutOfBoundsException}, which is also in the
\code{java.lang} package.

Indexing from position zero (inclusive) to length (exclusive) supports
the conventional \code{for} loop for iterating over items in a sequence,
%
\begin{verbatim}
CharSeqeunce cs = ...;
for (int i = 0; i < cs.length(); ++i) {
    char c = cs.charAt(i);
    ...
}
\end{verbatim}
%
Because lengths are instances of \code{int}, are only allowed to be as
long as the longest positive \code{int} value, $2^{32-1} - 1$, which
is approximately 2 billion.  Any longer sequence of characters needs
to be streamed \eg{with a \code{Reader} or \code{Writer}} or scanned
\eg{with a \code{RandomAccessFile}}.

\subsubsection{Creating Subsequences}

Given a character sequence, a subsequence may be generated using
\code{subSequence(int,int)}, where the two indexes are start position
(inclusive) and end position (exclusive).  As for \code{charAt}, the
indexes must be in range (start greater than or equal to zero and less
than or equal to end, and end less than or equal to the sequence
length). One nice feature of this inclusive-start/exclusive-end
encoding is that the length of the subsequence is the end minus the
start.  

The mutable implementations of \code{CharSequence} approach
subsequencing differently.  The \code{StringBuffer} and
\code{StringBuilder} classes return instances of \code{String}, which
are immutable.  Thus changes to the buffer or builder won't affect any
subsequences generated.  The \code{CharBuffer} abstract class in
\code{java.nio} returns a view into itself rather than an independent
copy.  Thus when the underlying buffer changes, so does the
subsequence.

\subsubsection{Converting Character Sequences to Strings}

The last method in \code{CharSequence} is \code{toString()}, which is
specified to return a string that has the same sequence of characters
as the character sequence.  

\subsubsection{Equality and Hashing}

The \code{CharSequence} interface does not place any constraints on
the behavior of equality or hash codes.  Thus two character sequences
may contain the same sequence of characters while not being equal
and having different hash codes.  We'll discuss equality and hash
codes for particular implementations of \code{CharSequence} in the
next few sections.


\section{\code{String} Class}

Strings are so fundamental to Java that they have special literals and
operators built into the Java language.  Functionally, the class
\code{String}, in the package \code{java.lang}, provides an immutable
implementation of \code{CharSequence}.  Sequence immutability, along
with a careful lazy initialization of the hash code, makes strings
thread safe.

Because \code{String} is declared to be final, there can be no
subclasses defined that might potentially override useful invariants
such as equality, comparability, and serializability.  This is
particularly useful because strings are so widely used as constants
and method inputs and outputs.

\subsection{Constructing a String}

Strings may be constructed from arrays of integer Unicode code points,
arrays of characters, or arrays of bytes.  In all cases, a new array
of characters is allocated to hold the underlying \code{char} values
in the string.  Thus the string becomes immutable and subsequent
changes to the array from which it was constructed will not affect it.

\subsubsection{From Unicode Code Points}

When constructing a string from an array of Unicode code points, the
code points are encoded using UTF-16 and the resulting sequence
converted to a sequence of \code{char} values.  Not all integers are
valid Unicode code points.  This constructor will throw an
\code{IllegalArgumentException} if there are illegal code points in
the sequence.

\subsubsection{From \code{char} Values}

When constructing a string from an array of characters or another
string or character sequence, the characters are just copied into the
new array.  There is no checking to ensure that the sequence of
\code{char} values is a legal UTF-16 sequence.  Thus we never know if
a string is well-formed Unicode.

A string may also be constructed from another string.  In this case, a
deep copy is created with its own fresh underlying array.  this may
seem useless, but consider the behavior of \code{substring()}, as
defined below; in this case, constructing a new copy may actually save
space.  Strings may also be constructed by copying the current
contents of a string builder or string buffer (see the next section
for more about these classes, which implement the builder pattern for
strings).

\subsubsection{From Bytes}

When constructing a string from bytes, a character encoding is used to
convert the bytes into \code{char} values.  Typically character
encodings are specified by name.  Any supported encoding or one of its
aliases may be named; the list of available encodings may be accessed
programatically and listed as shown in \refsec{supported-encodings}.

If a character encoding is not specified, the platform's default is
used, resulting in highly non-portable behavior.

A character encoding is represented as an instance of \code{CharSet},
which is in the \code{java.nio.charset} package.  A \code{CharSet} may
be provided directly to the constructor for \code{String}, or they may
be specified by name.  Instances of \code{CharSet} may be retrieved by
name using the static method \code{CharSet.forName(String)}.

If a byte sequence can't be converted to Unicode code points, its
behavior is determined by the underlying \code{CharSetDecoder}
referenced by the \code{CharSet} instance.  This decoder will
typically replace illegal sequences of bytes with a replacement string
determined by \code{CharSetDecoder.replacement()}, typically a
question mark character.  

If you need more control over the behavior in the face of illegal byte
sequences, you may retrieve the \code{CharSet} by name and then use
its \code{newDecoder()} method to get a \code{CharSetDecoder}.

\subsection{String Literals}

In Java code, strings may be represented as characters surrounded
by double quotes.  For instance,
%
\begin{verbatim}
String name = "Fred";
\end{verbatim}
%
The characters between the quotes can be any characters in the Java
program.  Specifically, they can be Unicode escapes, so we could've
written the above replacing \code{F} and \code{e} with Unicode
escapes, as in
%
\begin{verbatim}
String name = "\u0046r\u0065d";
\end{verbatim}

In practice, string literals essentially create static constants for
the strings.  Because strings are immutable, the value of string
contstants including literals, will be shared.  Thus \code{"abc" ==
  "abc"} will evaluate to true because the two literals will denote
reference identical strings.  This is carried out as if
\code{intern()} had been called on constructed strings, as described
in \refsec{string-intern}.

String literals may contain the same character escape sequences as
character literals (see \refsec{character-literals}).



\subsection{Contents of a String}

Strings are very heavy objects in Java.  First, they are full-fledged
objects, as opposed to the C-style strings consisting only of a
pointer into an array.  Second, they contain references to an array of
characters, \code{char[]}, holding the actual characters, as well as
an integer start index into that array and an integer length.  They
also store their hash code as an integer.  Thus an empty string will
be 36 bytes in the 32-bit Sun/Oracle JVM and 60 bytes in the 64-bit
JVM.%
%
\footnote{The JVM option \code{-XX:+UseCompressedOops} will reduce
the memory footprint of 64-bit objects to the same size as 32-bit
objects in many cases.}

% String:  8   +8
% offset:  4
% len:     4
% hashcode:4
% arr ref: 4   +4
% array: 8+4   +8

\subsection{String Equality and Comparison}

Two strings are equal if they have the same length and the same
character at each position.  Strings are not equal to objects of any
other runtime class, even other implementations of \code{CharSequence}.

Strings implement \code{Comparable<String>} in a way that is
consistent with equality.  The order they use is lexicographic
sort over the underlying \code{char} values.  This is exactly the
way words are sorted in dictionaries.  The easiest way to define
lexicographic sort is with code, here comparing strings \code{s}
and \code{t}.
%
\begin{verbatim}
for (int i = 0; i < Math.min(s.length(),t.length()); ++i)
    if (s.charAt(i) != t.charAt(i))
        return s.charAt(i) - t.charAt(i);
return s.length() - t.length();
\end{verbatim}

We walk over both strings from the beginning until we find a position
where the characters are different, at which point we return the
difference.%
%
\footnote{Conveniently, \code{char} values are converted to integers
  for arithmetic, allowing negative results for the difference of two
  values.  Also, because string lengths are non-negative, there can't
  be underflow or overflow in the difference.}
%
For instance, \codeString{abc} is less than \codeString{abde}
\codeString{ae}, and \codeString{e}.  If we finish either string, we
return the difference in lengths.  Thus \codeString{a} is less than
\codeString{ab}.  If we finish one of the strings and the strings are
the same length, we return zero, because they are equal.

\subsection{Hash Codes}

The hash code for a string \code{s} is computed as if by
%
\begin{verbatim}
int hashCode = 0;
for (int i = 0; i < s.length(); ++i)
    hashCode = 31 * hashCode + s.charAt(i);
\end{verbatim}
%
This definition is consistent with equals in that two equal strings
have the same character sequence, and hence the same hash code.

Because the hash code is expensive to compute, requiring an
assignment, add and multiply for each character, it is lazily
initialized.  What this means is that when hash code is called, if
there is not a value already, it will be computed and stored. This
operation is thread safe without synchronization because integer
assignment is atomic and the sequence of characters is immutable.

\subsection{Substrings and Subsequences}

The \code{substring(int,int)} and method return subsequences of a
string specified by a start position (inclusive) and an end position
(exclusive).  The \code{subSequence(int,int)} method just delegates to
the substring method.

When a substring is generated from a string, it shares the same array
as the string from which it was generated.  Thus if a large string is
created and a substring generated, the substring will actually hold a
reference to a large array, potentially wasting a great deal of
memory.  Alternatively, if a large string is created and a large
number of substrings are created, this implementation potentially
saves space.

A copy of the character array underlying the string is returned
by \code{toCharArray()}.  

\subsection{Simple Pattern Matching}

Simple text processing may be carried out using methods supplied
by \code{String}.  For instance, \code{endsWith(String)} and
\code{startsWith(String)} return boolean values based on whether
the string has the specified exact suffix or prefix.  These
comparisons are carried out based on the underlying \code{char[]}
array.  There is also a multiple argument \code{regionMatches()}
method that compares a fixed number the characters starting at a given
position to be matched against the characters starting at a different
position in a second string.

There is also a method \code{contains(CharSequence)}, which returns
true if the string contains the characters specified by the character
sequence argument as a substring.  And there is a method
\code{contentEquals(CharSequence)} that compares the character content
of the string with a character sequence.

The \code{indexOf(char,int)} method returns the index of the first
appearance of a character after the specified position, and
\code{lastIndexOf(char,int)} does the same thing in reverse,
returning the last instance of the character before the specified
position.  There are also string-based methods, \code{indexOf(String,int)} and
\code{lastIndexOf(String,int)}.

\subsection{Manipulating Strings}

Several useful string manipulation methods are provided, such as
\code{trim()}, which returns a copy of the string with no whitespace
at the front or end of the string.  The methods
\code{toLowerCase(Locale)} and \code{toUpperCase(Locale)} return a
copy of the string in the specified locale.

\subsection{Unicode Code Points}

The string class also defines methods for manipulating Unicode
code points, which may be coded by either one or two underlying
\code{char} values.  The method \code{codePointAt(int)} returns
the integer code point starting at the specified index.  To find out
the length of a string in unicode code points, there is the method
\code{codePointCount(int,int)} which returns the number of Unicode
code points coded by the specified range of underlying characters.  It
is up to the user to get the boundaries correct.

\subsection{Testing String Validity}

LingPipe provides a static utility method
\code{isValidUTF16(CharSequence)} in the \code{Strings} class in
the \code{com.aliasi.util} package.  This method checks that every
high surrogate UTF-16 \code{char} value is followed by a low surrogate
UTF-16 value.  Surrogacy is defined as by Java's
\code{Character.isHighSurrogate()} and
\code{Character.isLowSurrogate()} methods (see \refsec{char-character-utils}).


\subsection{Interning Canonical Representations}\label{section:string-intern}

The JVM keeps an underlying pool of string constants.  A call to
\code{intern()} on a string returns a string in the constant pool.
The method \code{intern()} returns a string that is equal to the
string on which it is called, and is furthermore reference identical
to any other equal string that has also been interned.

For example,  suppose we call
%
\begin{verbatim}
String s = new String("abc");   
String t = new String("abc");   
\end{verbatim}
%
The expression \code{s.equals(t)} will be true, but
\code{s == t} will be false.  The constructor \code{new} always
constructs new objects.  Next, consider calling
%
\begin{verbatim}
String sIn = s.intern();  
String tIn = t.inern();
\end{verbatim}
%
Afterwards, \code{sIn == tIn} will return true, as will
\code{sIn.equals(tIn)}.  The interned strings are equal to their
originals, so that \code{sIn.equals(s)} and \code{tIn.equals(t)} also
return true.


\subsection{Utility Methods}

There are a number of convenience methods in the string class that
reproduce behavior available elsewhere.  

\subsubsection{Regular Expression Utilities}

The string class contains various \code{split()} and
\code{replace}\codeVar{X}\code{()} methods based on regular expressions.
For instance, \code{split(String)} interprets its argument as a
regular expression and returns an array of strings containing the
sequences of text between matches of the specified pattern.
Matching is defined as in the regular expression method
\code{Matcher.find()}.

\subsubsection{String Representations of Primitives}

The string class contains a range of static \code{valueOf()}
methods for converting primitives, objects, and arrays of characters
to strings.  For instance, \code{valueOf(double)} converts a
double-precision floating point value to a string using the same
method as \code{Double.toString(double)}.

\subsection{Example Conversions}

We wrote a simple program to illustrate encoding and decoding between
strings and byte arrays.  
%
\codeblock{ByteToString.1}
%
First, the string \stringmention{D\'ej\`a vu} is
assigned to variable
\code{s}.  The string literal uses two Unicode escapes, 
\code{{\bk}u00E9} for \charmention{\'e} and \code{{\bk}u00E0} for 
\charmention{\`a}.  The name of the character encoding to use for
encoding and decoding are read from the command line.  

Then we convert the string \code{s} to an array of bytes
\code{bs} using the character encoding \code{encode}.  Next, we
reconstitute a string using the bytes we just created using the
character encoding \code{decode}.  Either of these methods may raise
an exception if the specified encoding is not supported, so the 
\code{main()} method is declared to throw an
\code{UnsupportedEncodingException}, imported from \code{java.io}.
The rest of the code just prints the relevant characters and bytes.

There is an Ant target \code{byte-to-string} that's configured to take
two arguments consisting of the names of the character encodings.  The
first is used to convert a string to bytes and the second to convert
the bytes back to a string.  For instance, to use UTF-8 to encode and
Latin1 (ISO-8859-1) to decode, the program prints 
%
\commandlinefollow{ant -Dencode=UTF-8 -Ddecode=Latin1 byte-to-string}
\begin{verbatim}
char[] from string
  44    e9    6a    e0    20    76    75
byte[] from encoding with UTF-8
  44    c3    a9    6a    c3    a0    20    76    75
char[] from decoding with Latin1
  44    c3    a9    6a    c3    a0    20    76    75
\end{verbatim}
%
The characters are printed using hexadecimal notation.  The initial
line of characters is just what we'd expect; for instance the
\code{char} value 44 is the code point and UTF-16 representation of
\unicode{0044}, \unicodedesc{latin capital letter d}, \code{char} e9
picks out \unicode{00E9}, \unicodedesc{latin small letter e with
acute}, and so on.

We have printed out the bytes using unsigned hexadecimal notation (see
\refsec{java-bytes} for more information about signs and bytes).  
Note that there are more bytes than \code{char} values.  This is
because characters above \unicode{0080} require two bytes or more to
encode in UTF-8 (see \refsec{utf-8}).  For instance, the sequence of
bytes 0xC3, 0xA9 is the UTF-8 encoding of
\unicode{00E9}, \unicodedesc{Latin small letter e with acute}.

We use Latin1 to decode the bytes back into a sequence of \code{char}
values.  Latin1 simply translates the bytes one-for-one into unsigned
characters (see \refsec{latin1}.  This is fine for the first byte,
0x44, which corresponds to \unicode{0044}, but things go wrong for the
second character.  Rather than recover the single \code{char} value
\unicode{00E9}, we have 0xC3 and 0xA9, which are \unicode{00C3},
\unicodedesc{latin capital letter a with tilde}, and \unicode{00A9}, 
\unicodedesc{copyright sign}.  Further down, we see 0xA0, which
represents \unicode{00A0}, \unicodedesc{no-break space}.  Thus instead
of getting \stringmention{D\'ej\`a vu} back, we get 
\stringmention{D\copyright\~Aj\copyright~ vu}.  

If we use \code{-Dencode=UTF-16} when invoking the Ant target
\code{byte-to-string}, we get the bytes
%
\begin{verbatim}
byte[] from encoding with UTF-16
  fe    ff     0    44     0    e9     0    6a     0    ...
\end{verbatim}
%
The first two bytes are the byte-order marks \unicode{00FE} and
\unicode{00FF}; in this order, they indicate big-endian 
(see \refsec{byte-order-marks}).  The two bytes after the byte-order
marks are 0x0 and 0x44, which is the UTF-16 big-endian representation
of the character \unicode{0044}, \unicodedesc{latin capital letter d}.
Similarly, 0x0 and 0xE9 are the UTF-16 big-endian representation of
\unicode{00E9}, \unicodedesc{latin small letter e with acute}.  The
UTF-16 encoding is twice as long as the Latin1 encoding, plus an
additional two bytes for the byte-order marks.


\section{\code{StringBuilder} Class}

The \code{StringBuilder} class is essentially a mutable implementation
of the \code{CharSequence} interface.  It contains an underlying array
of \code{char} values, which it resizes as necessary to support a
range of \code{append()} methods.  It implements a standard
builder pattern for strings, where after adding content, \code{toString()}
is used to construct a string based on the buffered values.

A typical use would be to create a string by concatenating the members
of an array, as in
%
\begin{verbatim}
String[] xs = ...;
StringBuilder sb = new StringBuilder();
for (String x : xs)
    sb.append(x);
String s = sb.toString();
\end{verbatim}
%
First a string builder is created, then values are appended to it,
then it is converted to a string.  When a string builder is converted
to a string, a new array is constructed and the buffered contents of
the builder copied into it.

The string builder class lets you append any type of argument, with
the effect being the same as if appending the string resulting from
the matching \code{String.valueOf()} method.  For instance, non-null
objects get converted using their \code{toString()} methods and null
objects are converted to the string \stringmention{null}.  Primitive
numbers converted to their decimal notation, using scientific notation
if necessary, and booleans to either \stringmention{true} or
\stringmention{false}.

The underlying array for buffering characters starts at length 16 by
default, though may be set explicitly in the constructor.  Resizing
adds one to the length then doubles it, leading to roughly log (base
2) resizings in the length of the buffer \eg{16, 34, 70, 142, 286,
  574, \ldots}.  Because string builders are rarely used in locations
that pose a computational bottleneck, this is usually an acceptable
behavior.

In cases where character sequence arguments suffice, this allows us to
bypass the construction of a string altogether.  Thus many of
LingPipe's methods accept character sequence inputs without first
converting to strings.

\subsection{Unicode Support}

The append method \code{appendCodePoint(int)} appends the characters
representing the UTF-16 encoding of the specified Unicode code point.
The builder also supports a \code{codePointAt()}, \code{codePointBefore()},
and \code{codePointCount()} methods with behavior like those for strings.

\subsection{Modifying the Contents}

String builders are mutable, and hence support operations like
\code{setCharAt(int,char)}, which replaces a single character, and
\code{insert(int,String)} and the longer form
\code{replace(int,int,String)}, which splice the characters of a
string into the buffer in place of the specified character slice.
There is also a \code{setLength(int)} method which changes the length
of the charactr sequence, either shrinking or extending the underlying
buffer as necessary.

\subsection{Reversing Strings}

The method \code{reverse()} method in \code{StringBuilder} reverses
the \code{char} values in a string, except valid high-low surrogate
pairs, which maintain the same ordering.  Thus if the string builder
holds a well-formed UTF-16 sequence representing a sequence of code
points, the output will be the well-formed UTF-16 sequence
representing the reversed sequence of code points.  For instance, the
String literal \code{"{\bk}uDC00{\bk}uD800"} constructs a string whose
\code{length()} returns 2.  Because \code{{\bk}uDC00} is a
high-surrogate UTF-16 \code{char} value and \code{{\bk}uD800} is a
low-surrogate, together they pick out the single code point
\unicode{10000}, the first code point beyond the basic multilingual
plane.  We can verify this by evaluating the expression
\code{"{\bk}uDC00{\bk}uD800".codePointAt(0)} and verifying the result 
is 0x10000.


\subsection{Chaining and String Concatenation}

Like many builder implementations, string builders let you chain
arguments.  Thus it's legal to write
%
\begin{verbatim}
int n = 7;  String s = "ABC";  boolean b = true;  
char c = 'C';  Object z = null;
String s = new StringBuilder().append(n).append(s).append(b)
               .append(c).append(z).toString();
\end{verbatim}
%
with the reslt that string \code{s} has a value equal to the
literal \code{"7ABCtrueCnull"}.

In fact, this is exactly the behavior underlying Java's heavily
overloaded addition (\code{+}) operator.  When one of the arguments to
addition is a string, the other argument is converted to a string as
if by the appropriate \code{String.valueOf()} method.  Thus an
equivalent way to write the chaining build above is
%
\begin{verbatim}
String s = 7 + " foo";
\end{verbatim}
%
Multiple additions of strings just get unfolded to multiple appends.


\subsection{Equality and Hash Codes}

The \code{StringBuilder} class does not override the definitions of
equality or hash codes in \code{Object}.  Thus two string builders are
equal only if they are reference equal.  In particular, string
builders are never equal to strings.  There is a utility method on
strings, \code{String.contentEquals(CharSequence)} method which
returns true if the char sequence is the same length as the string and
contains the same character as the string at each position.

\subsection{String Buffers}

The class \code{StringBuffer}, which predates
\code{StringBuilder}, is essentially a string builder with
synchronization.  Because it is rarely desirable to write to a
string buffer concurrently, \code{StringBuffer} has all but
disappeared from contemporary Java programs.


\section{\code{CharBuffer} Class}\label{section:char-charbuffer}

The \code{CharBuffer} class resides in the \code{java.nio} package
along with corresponding classes for other primitive types, such as
\code{ByteBuffer} and \code{IntBuffer}.  These buffers all hold sequences
of their corresponding primitive types and provide efficient means to
bulk load or bulk dump sequences of the primitive values they hold.

\subsection{Basics of Buffer Positions}

The buffers all extend the \code{Buffer} class, also from
\code{java.nio}.  Buffers may be created backed by an array that holds the appropriate
primitive values, though array backing is not required.

Every buffer has four important integer values that determine how it
behaves, its capacity, position, mark and limit.  Buffers may thus
only have a capacity of up to \code{Integer.MAX\_VALUE} (about 2
billion) primitive values.  

The capacity of a buffer is available through the \code{capacity()}
method.  This is the maximum number of items the buffer can hold.

The position of a buffer is available through the \code{position()}
method and settable with \code{setPosition(int)}.  The position is the
index of the next value to be accessed.  The position must always
be between zero (inclusive) and the buffer's capacity (inclusive).

Relative read and write operations start at the current position and
increment the position by the number of primitive values read or
written.  Underflow or overflow exceptions will be thrown if there are
not enough values to read or write respectively.  There are also
absolute read and write operations which require an explicit index.

The limit for a buffer indicates the first element that should not be
read or written to.  The limit is always between the position (inclusive)
and the capacity (inclusive).

Three general methods are used for resetting these values.  The
\code{clear()} method sets the limit to the capacity and the position
to zero.  The \code{flip()} method sets the limit to the current
position and the position to zero.  The \code{rewind()} method just
sets the position to zero.  

All of the buffers in \code{java.nio} also implement the
\code{compact()} operation, which moves the characters from between
the current position and the limit to the start of the buffer,
resetting the position to be just past the moved characters (limit
minus original position plus one).

Finally, the mark for a buffer indicates the position the buffer will
have after a call to \code{reset()}.  It may be set to the current
position using \code{mark()}.  The mark must always be between the
zero (inclusive) and the position (inclusive).

\subsection{Equality and Comparison}

Equality and comparison work on the remaining characters, which is the
sequence of characters between the current position and the limit.
Two \code{CharBuffer} instances are equal if they have the same number
of (remaining) elements and all these elements are the same.  Thus the
value of \code{hashCode()} only depends on the remaining characters.
Similarly, comparison is done lexicographically using the remaining
characters.

Because hash codes for buffers may change as their elements change,
they should not be put into collections based on their hash code
\ie{\code{HashSet} or \code{HashMap}} or their natural order
\ie{\code{TreeSet} or \code{TreeMap}}.

The \code{length()} and \code{charAt(int)} methods from \code{CharSequence}
are also defined relative to the remaining characters.


\subsection{Creating a \code{CharBuffer}}

A \code{CharBuffer} of a given capacity may be created along with
a new backing array using
\code{CharBuffer.allocate(int)}; a buffer may also be created from
an existing array using \code{CharBuffer.wrap(char[])} or from a slice
of an existing array.  Wrapped buffers are backed by the specified
arrays, so changes to them affect the buffer and vice-versa.

\subsection{Absolute Sets and Gets}

Values may be added or retrieved as if it were an array, using
\code{get(int)}, which returns a
\code{char}, and \code{set(int,char)}, which sets the specified index
to the specified \code{char} value.

\subsection{Relative Sets and Gets}

There are also relative set and get operations, whose behavior is
based on the position of the buffer.  The \code{get()} method returns
the character at the current position and increments the position.
The \code{put(char)} method sets the character at the current position
to the specified value and increments the position.  

In addition to single characters, entire \code{CharSequence} or
\code{char[]} values or slices thereof may be put into the array, with
overflow exceptions raised if not all values fit in the buffer.
Similarly, \code{get(char[])} fills the specified character array
starting from the current position and increments the position,
throwing an exception if there are not enough characters left in the
buffer to fill the specified array.  The portion of the array to fill
may also be specified by slice.

The idiom for bulk copying between arrays is to fill the first array
using relative puts, flip it, then copy to another array using
relative gets.  For instance, to use a \code{CharBuffer} to concatenate
the values in an array, we might use%
%
\footnote{Actually, a \code{StringBuilder} is better for this job
because its size doesn't need to be set in advance like a
\code{CharBuffer}'s.}%
%
\begin{verbatim}
String[] xs = ...;
CharBuffer cb = CharBuffer.allocate(1000);
for (String s : ss)
    cb.put(s);
flip();
String s = cb.toString();
\end{verbatim}
%
After the \code{put()} operations in the loop, the position is
after the last character.  If we were to dump to a string at this
point, we would get nothing.  So we call \code{flip()}, which
sets the position back to zero and the limit to the current
position.  Then when we call \code{toString()}, we get the values
between zero and the limit, namely all the characters we appended.
The call to \code{toString()} does not modify the buffer.







 






\subsection{Thread Safety}

Buffers maintain state and are not synchronized for thread safety.
Read-write synchronization would be sufficient.



\section{\code{Charset} Class}

Conversions between sequences of bytes and sequences of characters are
carried out by three related classes in the \code{java.nio.charset}
package.  A character encoding is represented by the confusingly named
\code{Charset} class.  Encoding characters as bytes is carried out by
an instance of \code{CharsetDecoder} and decoding by an instance of
\code{CharsetEncoder}.  All characters are represented as usual in
Java with sequences of \code{char} values representing UTF-16
encodings of Unicode.

Encoders read from character buffers and write to byte buffers and
decoders work in the opposite direction.  Specifically, they use the
\code{java.nio} classes \code{CharBuffer} and \code{ByteBuffer}.  The
class \code{CharBuffer} implements \code{CharSequence}.

\subsection{Creating a \code{Charset}}

Typically, instances of \code{Charset} and their corresponding
encoders and decoders are specified by name and accessed behind the
scenes.  Examples include constructing strings from bytes, converting
strings to bytes, and constructing \code{char} streams from
\code{byte} streams.

Java's built-in implementations of \code{Charset} may be accessed by
name with the static factory method \code{CharSet.forName(String)},
which returns the character encoding with a specified name.  

It's also possible to implement subclasses of \code{Charset} by
implementing the abstract \code{newEncoder()} and \code{newDecoder()}
methods.  Decoders and encoders need to define behavior in the face of
malformed input and unmappable characters, possibly defining
replacement values. So if you must have Morse code, it's possible to
support it directly.  

\subsection{Decoding and Encoding with a \code{Charset}}

Once a \code{Charset} is obtained, its methods \code{newEncoder()} and
\code{newDecoder()} may be used to get fresh instances of
\code{CharsetEncoder} and \code{CharsetDecoder} as needed. These classes
provide methods for encoding \code{char} buffers as \code{byte}
buffers and decoding \code{byte} buffers to \code{char} buffers.  The
\code{Charset} class also provides convenience methods for decoding
and encoding characters.

The basic encoding method is
\code{encode(CharBuffer,ByteBuffer,boolean)}, which maps the
\code{byte} values in the \code{ByteBuffer} to \code{char} values in
the \code{CharBuffer}.  These buffer classes are in \code{java.nio}.
The third argument is a flag indicating whether or not the bytes are
all the bytes that will ever be coded.  It's also possible to use the
\code{canEncode(char)} or \code{canEncode(CharSequence)} methods to
test for encodability.  

The \code{CharsetEncoder} determines the behavior in the face of
unmappable characters.  The options are determined by the values of
class \code{CodingErrorAction} (also in \code{java.nio.charset}).%
%
\footnote{The error action class is the way type-safe enumerations used to
be encoded before \code{enum}s were added diretly to the language, with
a private constructor and a set of static constant implementations.}%
%
The three actions allowed are to ignore unmappable characters (static
constant \code{IGNORE} in \code{CodingErrorAction}), replace them with
a specified sequence of bytes (\code{REPLACE}), or to report the
error, either by raising an exception or through a return value
(\code{REPORT}).  Whether an exception is raised or an exceptional return
value provided depends on the calling method.  For the replacement option,
the bytes used to replace an unmappable character may be set on the
\code{CharsetEncoder} using the \code{replaceWith(byte[])} method.

The \code{CharsetDecoder} class is similar to the encoder class, only
moving from a \code{CharBuffer} back to a \code{ByteBuffer}.  The way
the decoder handles malformed sequences of bytes is similar to the
encoder's handling of unmappable characters, only allowing a sequence
of \code{char} as a replacement rather than a sequence of \code{byte}
values.

The \code{CharsetEncoder} and \code{CharsetDecoder} classes maintain
buffered state, so are not thread safe.  New instances should be
created for each encoding and decoding job, though they may be reused
after a call to \code{reset()}.



\subsection{Supported Encodings in Java}\label{section:supported-encodings}

Each Java installation supports a range of character encodings, with
one character encoding serving as the default encoding.  Every
compliant Java installation supports all of the UTF encodings of
Unicode.  Other encodings, as specified by \code{Charset}
implementations, may be added at the installation level, or
implemented programatically within a JVM instance.

The supported encodings determine what characters can be used to write
Java programs, as well as which character encoding objects are
available to convert sequences of bytes to sequences of characters.

We provide a class \code{com.lingpipe.book.char.SupportedEncodings} to
display the default encoding and the complete range of other encodings
supported.  The first part of the main pulls out the default encoding
using a static method of \code{java.nio.charset.Charset},
%
\codeblock{SupportedEncodings.1}
%
The code to pull out the full set of encodings along with their
aliases is
%
\codeblock{SupportedEncodings.2}

The result of running the program is
%
\commandlinefollow{ant available-charsets}
\begin{verbatim}
Default Encoding=windows-1252

Big5
     csBig5
...
x-windows-iso2022jp
    windows-iso2022jp
\end{verbatim}
%
The program first writes out the default encoding.  This is
\code{windows-1252} on my Windows install.  It's typically
\code{ISO-8859-1} on Linux installs in the United States.
After that, the program writes all of the encodings, with the official
name followed by a list of aliases that are also recognized.  We
ellided most of the output, because there are dozens of encodings
supported.


\section{International Components for Unicode}

The International Components for Unicode (ICU) provide a range of
useful open source tools, licensed under the X license
(see\refsec{x-license}), for dealing with Unicode and
internationalization (I18N).  Mark Davis, the co-developer of Unicode
and president of the Unicode Consortium, designed most of ICU
including the Java implementation, and is still active on its project
management committee.  The original implementation was from IBM, so
the class paths still reflect that for backward compatibility, though
it is now managed independently.

The home page for the package is
%
\urldisplay{http://icu-project.org/}
%
The package has been implemented in both Java and C, with ICU4J
being the Java version.  There is javadoc, and the user guide
section of the site contains extensive tutorial material that goes
far beyond what we cover in this section.  It is distributed
as a Java archive (jar) file, \code{icu4j-}\codeVar{Version}\code{.jar},
which we include in the \relpath{lib}
directory of this book's distribution.  There are additional jar
files available on the site to deal with even more character
encodings and locale implementations.

We are primarily interested in the Unicode utilities for normalization
of character sequences to canonical representations and for
auto-detecting character encodings from sequences of bytes.  ICU also
contains a range of locale-specific formatting utilities, time
calculations and collation (sorting).  It also contains deeper
linguistic processing, including boundary detection for words,
sentences and paragraphs as well as transliteration between texts in
different languages.

\subsection{Unicode Normalization}\label{section:icu-unicode-normalization}

ICU implements all the Unicode normalization schemes (see
\refsec{unicode-normalization-forms}).  The following code shows how to use NFKC,
the normalization form that uses compatibility decomposition, followed
by a canonical composition.
%
\codeblock{NormalizeUnicode.1}
%
The ICU class \code{Normalizer2} is defined in the package
\code{com.ibm.icu.text}.  There is no public constructor, so we use the
rather overloaded and confusing static factory method
\code{getInstance()} to create a normalizer instance.  The first
argument is \code{null}, which causes the normalizer to use ICU's
definitions of normalization rather than importing definitions through
an input stream.  The second argument, \code{"nfkc"} is an
unfortunately named string contstant indicating that we'll use NFKC or
NFKD normalization.  The third argument is the static constant
\code{COMPOSE}, from the static nested class \code{Normalizer2.Mode},
which says that we want to compose the output, hence producing NFKC.
Had we used the constant \code{DECOMPOSE} for the third argument, we'd
get an NFKD normalizer.  Finally, it allows case folding after the NFKC normalization
by specifying the second argument as \code{"nfkc\_cf"}.  

Once we've constructed the normalizer, using it is straightforward.
We just call its \code{normalize()} method, which takes a
\code{CharSequence} argument.  We first created two sequences of Unicode
characters, \unicode{00E0} (small a with grave accent), and
\unicode{0061}, \unicode{0300} (small a followed by grave accent).
After that, we just print out the \code{char} values from the
strings.

\commandlinefollow{ant normalize-unicode}
\begin{verbatim}
s1 char values=  e0
s2 char values=  61, 300
n1 char values=  e0
n2 char values=  e0
\end{verbatim}
%
Clearly the initial strings \code{s1} and \code{s2} are different.
Their normalized forms, \code{n1} and \code{n2}, are both the same as
\code{s1}, because we chose the composing normalization scheme that
recomposes compound characters to the extent possible.


\subsection{Encoding Detection}

Unfortunately, we are often faced with texts with unknown character
encodings.  Even when character encodings are specified, say for an
XML document, they can be wrong.  ICU does a good job at detecting the
character encoding from a sequence of bytes, although it only supports
a limited range of character encodings, not including the pesky
Windows-1252, which is highly confusible with Latin1 and just as
confuisble with UTF-8 as Latin1.  It's also problematic for Java use,
because the default Java install on Windows machines uses Windows-1252 as
the default character encoding.

We created a sample class \relpath{DetectEncoding}, which begins its
\code{main()} method by extracting (and then displaying) the detectable
character encodings,
%
\codeblock{DetectEncoding.1}
%
These are the only encodings that will ever show up as return values
from detection.  

The next part of the program does the actual detection
%
\codeblock{DetectEncoding.2}
%
First, it sets a variable that we'll use for the declared encoding,
ISO-8859-1, a variable for the string we'll decode,
\stringmention{D\'ej\`a vu}, and an array of encodings we'll test.
For each of those encodings, we set the byte array \code{bs} to result
of getting the bytes using the current encoding.  We then create an
instance of \code{CharsetDetector} (in \code{com.ibm.icu.text}) using
the no-arg constructor, set its declared encoding, and set its text to
the byte array.  We then generate an array of matches using the
\code{detecAll()} method on the detector.  These matches are instances
of \code{CharsetMatch}, in the same pacakge.  

The rest of our the code just iterates through the matches and prints them.
%
\codeblock{DetectEncoding.3}
%
For each match, we grab the name name of the character encoding, the
integer confidence value, which will range from 0 to 100 with higher
numbers being better, and the inferred language, which will be a
3-letter ISO code.  We also get the text that is produced by using
the specified character encoding to convert the string.  
These are then printed, but instead of printing the text, we just
print whether its equal to our original string \code{s}.

The result of running the code is
%
\commandlinefollow{ant detect-encoding}
\begin{verbatim}
Detectable Charsets=[UTF-8, UTF-16BE, UTF-16LE, UTF-32BE, 
UTF-32LE, Shift_JIS, ISO-2022-JP, ISO-2022-CN, ISO-2022-KR, 
GB18030, EUC-JP, EUC-KR, Big5, ISO-8859-1, ISO-8859-2, 
ISO-8859-5, ISO-8859-6, ISO-8859-7, ISO-8859-8, windows-1251, 
windows-1256, KOI8-R, ISO-8859-9, IBM424_rtl, IBM424_ltr, 
IBM420_rtl, IBM420_ltr]

encoding=UTF-8 # matches=3
     guess=UTF-8 conf=80 lang=null
         chars= 44 e9 6a e0 20 76 75 2e
     guess=Big5 conf=10 lang=zh
         chars= 44 77c7 6a fffd 20 76 75 2e
     ...

encoding=UTF-16 # matches=4
     guess=UTF-16BE conf=100 lang=null
         chars= feff 44 e9 6a e0 20 76 75 2e
     guess=ISO-8859-2 conf=20 lang=cs
         chars= 163 2d9 0 44 0 e9 0 6a 0 155 0 20 0 ...
     ...

encoding=ISO-8859-1 # matches=0
\end{verbatim}
%
Overall, it doesn't do such a good job on short strings like this.
The only fully correct answer is for UTF-8, which retrieves
the correct sequence of \code{char} values.

The UTF-16BE encoding is the right guess, but it messes up the
conversion to text by concatenating the two byte order marks \code{FE}
and \code{FF} indicating big-endian into a single \code{char} with value
\code{0xFEFF}.  The detector completely misses the Latin1 encoding, which
seems like it should be easy.    

The language detection is very weak, being linked to the character
encoding.  For instance, if the guessed encoding is Big5, the language
is going to be Chinese (ISO code ``zh'').

Confidence is fairly low all around other than UTF-16BE, but
this is mainly because we have a short string.  In part, the
character set detector is operating statistically by weighing how
much evidence it has for a decision.



\subsection{General Transliterations}\label{section:char-unicode-transliterate}

ICU provides a wide range of transforms over character sequences.  For
instance, it's possible to transliterate Latin to Greek, translate
unicode to hexadecimal, and to remove accents from outputs.  Many of
the transforms are reversible.  Because transforms are mappings from
strings to strings, they may be composed.  So we can convert Greek to
Latin, then then remove the accents, then convert to hexadecimal
escapes.  It's also possible to implement and load your own transforms.

Using transliteration on strings is as easy as the other interfaces in
ICU.  Just create the transliterator and apply it to a string.  

\subsubsection{Built-In Transliterations}

We first need to know the available transliterations, the identifiers
of which are avaialble programatically.  We implement a \code{main()}
method in \code{ShowTransliterations} as follows
%
\codeblock{ShowTransliterations.1}
%
The ICU \code{Transliterator} class is imported from the package.
\code{com.ibm.icu.text}.  We use its static method \code{getAvailableIDs()}
to produce an enumeration of strings, which we then iterate through
and print out.%
%
\footnote{The \code{Enumeration} interface in the package \code{java.util} 
from Java 1.0 has been all but deprecated in favor of the
\code{Iterator} interface in the same package, which adds a remove
method and shortens the method names, but is otherwise identical.}
%
An abbreviated list of outputs (converted to two columns) is
%
\commandlinefollow{ant show-transliterations}
\begin{trivlist}
\item
\begin{minipage}[t]{0.45\textwidth}
\begin{verbatim}
Accents-Any
Amharic-Latin/BGN
Any-Accents
Any-Publishing
Arabic-Latin
...
Latin-Cyrillic
Latin-Devanagari
Latin-Georgian
Latin-Greek
...
Han-Latin
Hangul-Latin
Hebrew-Latin
Hebrew-Latin/BGN
\end{verbatim}
\end{minipage}%
\begin{minipage}[t]{0.45\textwidth}
\begin{verbatim}
Hiragana-Katakana
Hiragana-Latin
...
Persian-Latin/BGN
Pinyin-NumericPinyin
...
Any-Null
Any-Remove
Any-Hex/Unicode
Any-Hex/Java
...
Any-NFC
Any-NFD
Any-NFKC
...
\end{verbatim}
\end{minipage}
\end{trivlist}
%
The transliterations are indexed by script, not by language.  Thus
there is no French or English, just Latin, because that's the script.
There is only so much that can be done at the script level, but the
transliterations provided by ICU are surprisingly good.  

In addition to the transliteration among scripts like Arabic and
Hiragana, there are also transforms based on Unicode itself.  For
instance, the \code{Any-Remove} transliteration removes combining
characters, and thus may be used to convert accented characters to
their deaccented forms across all the scripts in Unicode.  There
areal so conversions to hexadecimal representation; for instance
\code{Any-Hex/Java} converts to hexadecimal \code{char} escapes
suitable for inclusion in a Java program.  The normalizations are
also avaialble through \code{Any-NFC} and so on. 

\subsubsection{Transliterating}

The second demo class is \code{Transliterate}, which takes two command
line arguments, a string to transliterate and a transliteration
scheme, and prints the output as a string and as a list of \code{char}
values.  The body of the \code{main()} method performs the
transliteration as follows.
%
\codeblock{Transliterate.1}
%
First we collect the text and transliteration scheme from the two
command-line arguments.  Then, we create a transliterator with the
specified scheme using the static factory method \code{getInstance()}.
Finally, we apply the newly created transliterator's
\code{transliterate()} method to the input to produce the output.

We use the Ant target \code{transliterate} which uses the properties
\code{text} and \code{scheme} for the text to transliterate and coding
scheme respectively.  
%
\commandlinefollow{ant -Dtext="taxi cab" -Dscheme=Latin-Greek transliterate}
\begin{verbatim}
Scheme=Latin-Greek
Input=taxi cab
Output=???? ???
Output char values
  3c4  3b1  3be  3b9   20  3ba  3b1  3b2
\end{verbatim}
%
The reason the output looks like \code{????~???} is that it was cut
and pasted from the Windows DOS shell, which uses the Windows default
Java character encoding, Windows-1252, which cannot represent the
Greek letters (see \refsec{io-stdin-stdout} for information on how to
change the encoding for the standard output).  For convenience, we
have thus dumped the hexadecimal representations of the characters.
For instance, \unicode{03c4},
\unicodedesc{greek small letter tau}, is the 
character $\tau$ and \unicode{03b1}, \unicodedesc{greek small letter
alpha}, is $\alpha$.  The entire string is $\tau\alpha\xi\iota \ \
\kappa\alpha\beta$, which if you sound it out, is roughly
the same as \stringmention{taxi cab} in Latin.

To see how transliteration can be used for dumping out
hex character values, consider
%
\commandlinefollow{ant -Dtext=abe -Dscheme=Any-Hex/Java transliterate}
\begin{verbatim}
Input=abe
Output=\u0061\u0062\u0065
\end{verbatim}
%
The output may be quoted as a string and fed into Java.  Translations
are also reversible, so we can also do it the other way around,
%
\commandlinefollow{ant~-Dtext={\bk}u0061{\bk}u0062{\bk}u0065~-Dscheme=Hex/Java-Any~transliterate}
\begin{verbatim}
Input=\u0061\u0062\u0065
Output=abe
\end{verbatim}

\subsubsection{Filtering and Composing Transliterations}

The scope of transforms may be filtered. For instance, if we write
\code{[aeiou] Upper}, the lower-case ASCII letters are transformed
to upper case.  For example,
using our demo program, we have
%
\commandlinefollow{ant -Dtext="abe" -Dscheme="[aeiou] Upper" transliterate}
\begin{verbatim}
Scheme=[aeiou] Upper
Input=abe
Output=AbE
\end{verbatim}
%

Transforms may also be composed.  For any transforms \code{A} 
and \code{B}, the transform \code{A;~B} first applies the transform
\code{A}, then the transform \code{B}.  
Filters are especially useful when combined with composition.  For
example, the transform
\code{NFD;\ [:Nonspacing Mark:]\ Remove;\ NFC}
composes three transforms, \code{NFD}, \code{[:Nonspacing Mark:]\
Remove}, and \code{NFC}.  The first transform performs canonical
character decomposition (NFD), the second removes the non-spacing
marks, such as combining characters, and the third performs a
canonical composition (NFC).  We can demo this with the string
\code{d\'ej\`a}, which we encode using Java escapes as
\code{{\bk}u0064{\bk}u00E9{\bk}u006a{\bk}u00E0}, by first prefixing
the transform with \code{Hex/Java-Any} to convert the escapes to Unicode.
The output is
%
\begin{verbatim}
Scheme=Hex/Java-Any; NFD; [:Nonspacing Mark:] Remove; NFC
Input=\u0064\u00E9\u006a\u00E0
Output=deja
\end{verbatim}







