\chapter{Classifiers and Evaluation}\label{chap:classifier-evaluation}

We are going to introduce the classifier interface, discuss what
classifiers do, and then show how to evaluate them.  In subsequent
chapters, we consider a selection of the classifier implementations
offered in LingPipe.

\section{What is a Classifier?}

A classifier takes inputs, which could be just about anything, and
return a classification of the input over a finite number of discrete
categories.  For example, a classifier might take a biomedical
research paper's abstract and determine if it is about genomics or
not.  The outputs are ``about genomics'' and ``not about genomics.''

Classifiers can have more than two outputs.  For instance, a
classifier might take a newswire article and classify whether it is
politics, sports, entertainment, science and technology, or world or
local news; this is what Google and Bing's news services do.%
%
\footnote{At \url{http://news.google.com} and
  \url{http://news.bing.com}.}
%
Other applications of text classifiers, either alone or in concert
with other components, range from classifying documents by topic,
identifying the language of a snippet of text, analyzing whether a
movie review is postive or negative, linking a mention of a gene name
in a text to a database entry for that gene, or resolving the sense of
an ambiguous word like \stringmention{bank} as meaning a savings
institution or a flowing body of water.  In each of these cases, we
have a known finite set of outcomes and some evidence in the form of
text.

\subsection{Exclusive and Exhaustive Categories}

In the standard formulation of classification, which LingPipe follows,
the categories are taken to be both exhaustive and mutually exclusive.
Thus every item being classified has exactly one category.

One way to get around the exhaustiveness problem is to include an
``other'' category that matches any input that doesn't match one of
the other categories.  The other category is sometimes called a
``sink'' or ``trash'' category, and may be handled differently than
the other categories during training.

Exclusivity is more difficult to engineer.  While it is possible to
allow categories that represent more than one outcome, if we have $n$
base categories, we'll have ${n \choose 2}$ unique pairs of
categories.  The combinatorics quickly gets out of hand.

If an item needs to be classified for two cross-cutting
categorizations, we can use two classifiers, one for each
categorization.  For instance, we might classify MEDLINE citations as
being about genomics or not, and about being about clinical trials or
not.  The two binary classifiers produce four possible outcomes.
Another example of this would be to use two binary classifiers for
sentiment, one indicating if a text had positive sentiment or not, and
another indicating if it had negative sentiment or not.  The result is
a four-way classification, with a neutral article having neither
positive nor negative sentiment, a mixed review having both positive
and negative sentiment, and a positive or negative review having one
or the other.

The latent Dirichlet allocation (LDA) model we consider in
\refchap{lda} assigns more than one category to each document, under
the assumption that every document is a mixture of topics blended
according to some document-specific ratio that the model infers.

\subsection{First-Best, Ranked and Probabilistic Results}

A simple first-best classifier need only return its best guess for the
category of each input item.  Some applications only allow a single
best guess, and thus some evaluations are geared toward evaluating
only a classifiers' first-best guess for an input.

A ranking classifier returns the possible categories in rank order of
their match to the input.  The top ranked answer is the first-best
result.  We still assume exhaustive and exclusive categories, but the
classfier supplies its second-best guess, third-best guess and so on.
In cases where there are large numbers of categories, an application
might return more than one possible answer to a user.

A scoring classifier goes one step further and assigns a (floating
point) score to each categories.  These may then be sorted to provide
a ranking and a first-best result, with higher scores taken to be
better matches.  For example, LingPipe's implementations of averaged
perceptrons returns scored results.

Scores for categories given an input are often normalized so that they
represent an estimate of the conditional probability of a category
given the input.  This is the case for LingPipe's logistic regression
classifiers and k-nearest neighbors classifiers.  For instance, a
classifier might see a MEDLINE citation about testing for a disease
with a known genetic component and estimate a 40\% probability it is
about genomics and 60\% probability that it is not about genomics.
Because the outcomes are exhaustive and exclusive, the probabilities
assigned to the categories must sum to 1.

In the case of generative statistical models, such as naive Bayes
classifiers or hidden Markov models, the score represents the joint
probabilty of the output category and the input being classified.
Given the joint probabilities, we can use the rule of total
probability to compute conditional probabilities of categories given
inputs.

\subsection{Ordinals, Counts, and Scalars}

A classifier is an instance of what statisticians call categorical
models.  The outcome of a classification is a category, not a number.

Classifiers deal with categorical outcomes.  Ordinal outcomes are like
categorical outcomes, only they come with an order.  Examples of
ordinal outcomes include rating movies on a $\{ 1, 2, 3, 4, 5 \}$
scale, answering a survey question with strongly-disagree, disagree,
neutral, agree, or strongly agree, and rating a political attitude as
left, center, or right.

In many cases, we will be dealing with counts, which are non-negative
natural numbers.  Examples of count variables include the number of
times a word appears in a document, the length of a document's title
in characters, and the number of home runs a baseball player gets in a
season.  Classifiers like naive Bayes convert word count data into
categorical outcomes based on a probability model for documents (see
\refchap{naive-bayes}).

Scalar outcomes are typically continuous.  Examples of continuous
scalar variables include a person's height, the length of the vowel
sequence in milliseconds in the pronunciation of the word
\charmention{lion}, or the number of miles between two cities.  Wes

Rating movies on an ordinal 1-5 scale doesn't allow ratings like
1.793823.  Half-star ratings could be accomodated by including
additional discrete outcomes like 1.5, 2.5, 3.5 and 4.5, leaving a
total of 9 possible ratings.  When there are 100 possible ratings, it
makes less sense to treat the outcome as ordinal.  Often, it is
approximated by a continuous scalar variable.

Models that predict or estimate the values of ordinal, count and
scalar values all have different evaluation metrics than categorical
outcomes.  In this section, we focus exclusively on categorical
outcomes.


\subsection{Reductions to Classification Problems}

Many problems that don't at first blush appear to be classification
problems may be reduced to classification problems.  For instance, the
standard document search problem (see \refchap{lucene}) may be recast
as a classification problem.  Given a user query, such as
\searchquery{be-bim-bop recipe}, documents may be classified as
relevant or not-relevant to the search.  

Another popular reduction is that for ranking.  If we want to rank a
set of items, we can build a binary classifier for assessing whether
one item is greater than another in the ordering.  In this case, it is
then a challenge to piece back together all these binary decisions to
get a final rank ordering.

Ordinal classification problems may also be reduced to
classifications.  Suppose we have a three-outcome ordered result, say
$N \in \{ 1, 2, 3 \}$.  We can develop a pair of binary classifiers
and use them as an ordinal three-way classifier.  The first classifier
will test if $N < 2$ and the second if $N < 3$.  If the first
classifier returns true, the response is 1, else if the second
classifier returns true the response is 2, else the response is 3.
Note that if the first classifier returns true and the second false,
we have an inconsistent situation, which we have resolved by returning
1.

Just about any problem that may be cast in a hypothesize-and-test
algorithm may use a classifier to do the testing, with a simple binary
outcome of accept or reject.  For instance, we can generate possible
named-entity mention chunkings of a text and then use a binary
classifier to evaluate if they are correct or not.  

Such reductions are often not interpetable probabilistically in the
sense of assigning probabilities to possible outcomes.



\section{Gold Standards, Annotation, and Reference Data}

For classification and other natural language tasks, the categories
assigned to texts are designed and applied by humans, who are
notoriously noisy in the semantic judgments for natural language.  

For instance, we made up the classification of genomics/non-genomics
for MEDLINE citations.  In order to gather evaluation data for a
classifier, we would typically select a bunch of MEDLINE citations at
random (or maybe look at a subset of interest), and label them
as to whether they are about genomics or not.

At this point, we have a problem.  If one annotator goes about his or
her merry way and annotates all the data, everything may seem fine.
But as soon as you have a second annotator try to annotate the same
data, you will see a remarkable number of disagreements over what seem
like reasonably simple notions, like being ``about'' genomics.  For
instance, what's the status of a citation in which DNA is only
mentioned in passing?  What about articles that only mention proteins
and their interactions?  What about an article on the sociology of the
human genome project?  These boundary cases may seem outlandish, but
try to label some data and see what happens.

In the Message Understanding Conference evaluations, there were a
large number of test instances involving the word \stringmention{Mars}
used to refer to the planet.  The contestants' systems varied in
whether they treated this as a location or not.  The reference data
did, but there weren't any planets mentioned in the training data.

\subsection{Evaluating Annotators}

The simplest way to compare a pair of annotations is to look at
percentage of agreement between the annotators.  

Given more than two annotators, pairwise statistics may be calculated.
These may then be aggregated, most commonly by averaging, to get an
overall agreement statistic.  It is also worth inspecting the counts
assigned to categories to see if any annotators are biased toward
too many or too few assignments to a particular category.

In general, pairs of annotators may be evaluated as if they were
themselves classifiers.  One is treated as the reference (gold
standard) and one as the response (system guess), and the response
is evaluated against the reference.  

It is common to see Cohen's $\kappa$ statistic used to evaluate
annotators; see \refsec{classifier-eval-kappa} for more information.


\subsection{Majority Rules, Adjudication and Censorship}

Data sets are sometimes created by taking a majority vote on the
category for items in a corpus.  If there is no majority (for
instance, if there are two annotators and they disagree), there are
two options.  First, the data can be censored, meaning that it's
removed from consideration.  This has the adverse side effect of
making the actual test cases in the corpus easier than a random
selection might be because the borderline cases are removed.  Perhaps
one could argue this is an advantage, because we're not even sure what
the categories are for those borderline cases, so who cares what the
system does with them.  

The second approach to disagreements during annotation is
adjudication.  This can be as simple as having a third judge look at
the data.  This will break a tie for a binary classification problem,
but may not make the task any clearer.  It may also introduce noise
as borderline cases are resolved inconsistently.

A more labor intensive approach is to consider the cases of
uncertainty and attempt to update the notion of what is being
annotated.  It helps to do this with batches of examples rather than
one example at a time.  

The ideal is to have a standalone written coding standard explaining
how the data was annotated that is so clear that a third party could
read it and label the data consistently with the reference data.  


\section{Confusion Matrices}

One of the fundamental tools for evaluating first-best classifiers is
the confusion matrix.  A confusion matrix reports on the number of
agreements between a classifier and the reference (or ``true'')
categories.  This can be done for classification problems with any
number of categories.  

\subsection{Example: Blind Wine Classification by Grape}

Confusion matrices are perhaps most easily understood with an example.
Consider blind wine tasting, which may be viewed as an attempt by a
taster to classify a wine, whose label is hidden, as to whether it is
made primarily from the syrah, pinot (noir), or cabernet (sauvignon)
grape.  We have to assume that each wine is made primarily from one of
these three grapes so the categories are exclusive and exhaustive.  An
alternative, binary classification problem would be to determine if
a wine had syrah in it or not.

Suppose our taster works their way through 27 wines, assigning a
single grape as a guess for each wine.  For each of the 27 wines,
we have assumed there is a true answer among the three grapes
syrah, pinot and cabernet.  The resulting confusion matrix might look
as follows.
%
\begin{center}
\begin{tabular}{r|r|c|c|c|c}
\multicolumn{2}{c}{ } & \multicolumn{3}{c}{\tblhead{\bfseries Response}}
\\ \cline{3-5}
\multicolumn{2}{c|}{ } & \tblhead{cabernet} & \tblhead{syrah} & \tblhead{pinot}
\\ \cline{2-5}
\multirow{3}{0.15\textwidth}{\hfill\tblhead{\bfseries Reference}}
& \tblhead{cabernet} & 9 & 3 & 0 & {\it 12}
\\ \cline{2-5}
& \tblhead{syrah} & 3 & 5 & 1 & {\it 9}
\\ \cline{2-5}
& \tblhead{pinot} & 1 & 1 & 4 & {\it 6}
\\ \cline{2-6}
\multicolumn{2}{c}{ } & \multicolumn{1}{c}{\it 13} & \multicolumn{1}{c}{\it 9} & \multicolumn{1}{c}{\it 5} & \multicolumn{1}{|c}{\it\bfseries 27}
\end{tabular}%
\end{center}
%
Each row represents results for a specific reference category.  In the
example, the first row represents the results for all wines that were
truly cabernets.  Of the 12 cabernets presented, the taster guessed
that 9 were cabernets, 3 were syrah, and none were guessed to be a
pinot.  The total number of items in a row is represented at the end
in italices, here {\it 12}.  The second row represents the taster's
resuls for the 9 syrahs in the evaluation.  Of these, 5 were correctly
classified as syrahs, whereas 3 were misclassified as cabernets and
one as a pinot.  The last row is the pinots, with 4 correctly
identified and 1 misclassified as cabernet and 1 as syrah.  Note that
the table is not symmetric.  One pinot was guessed to be a cabernet,
but no cabernets were guessed to be pinots.  

The totals along the right are total number of reference items, of
which there were 12 cabernets, 9 syrahs, and 6 pinots.  The totals
along the bottom are for responses.  The taster guessed 13 wines were
cabernets, 9 were syrahs, and 5 were pinots.  The overall total in the
lower right is 27, which is sum of the values in all the cells, and
equal to the total number of wines evaluated.

\subsection{Accuracy}

A number of useful statistics may derived from confusion matrices.  We
have already seen the basic counts of reference/response pairs, as
well as reference and response counts by category, as well as the overall counts.

\subsubsection{Overall Accuracy}

The overall accuracy is just the number of correct responses divided
by the total numbe of responses.  A correct response for an item
occurs when the response category matches the reference category.  The
count of correct responses are thus on the diagonal of the confusion
matrix.  There were 9 correct responses for reference cabernets, 5 for
syrahs, and 4 for pinots, for a total of 18 correct responses.  The
total accuracy of the classifier is thus $18/27 \approx 0.67$.  

Total accuracy estimates the probability that a classifier will make
the correct categorization of the next item it faces, assuming the
items to be evaluated are sampled the same way as the test data.

\subsubsection{Accuracy Confidence Intervals} 

Because overall accuracy is a simple binomial statistic, we can
compute a normal approximation to a 95\% confidence interval directly
(see \refsec{stats-binomial-variance} for an explanation of the
formula).  In this case, the 95\% interval is approximately plus or
minus 0.18 and the 99\% interval approximately plus or minus 0.23.  As
usual, with so few evaluation cases, we don't get a tight estimate of
our system's accuracy.

In almost all cases, including this one, confidence in our accuracy
numbers will be inversely proportional to the square root of the
number of test cases.  For instance, with 16 test cases, our
confidence interval is sized proportionally to $1/\sqrt{16} = 1/4$,
and with 64 test cases, it's proportional to $1/\sqrt{64} = 1/8$.  So
even though we multiply the number of cases by 4, the confidence only
shrinks by $\sqrt{4}$, or a half.


\subsection{$\kappa$ Statistics for Chance-Adjusted Agreement}\label{section:classifier-eval-kappa}

\subsubsection{Random Accuracy}

Suppose that the response is chosen randomly according to the
distribution of response answers.  In our running example, the taster
responded cabernet 13 times out of 27, syrah 9 of 27, and pinot 5 of
27.  We could choose a response randomly by choosing cabernet 13/27
times, syrah 9/27, and pinot 5/27.  

If we assume test cases appear in the proportions they do in the test
set, we have cabernet 12/27 times, syrah 9/27, and pinot 6/27 times.

Now if the responses are chosen randomly, our probabilty of getting
the right answer is 
%
\begin{equation}
\left(\frac{13}{27} \times \frac{12}{27}\right)
+ \left(\frac{9}{27} \times \frac{9}{27}\right)
+ \left(\frac{5}{27} \times \frac{6}{27}\right)
\approx 0.3663.
\end{equation}
%
(We use so many decimal places here to compare two different versions
of $\kappa$ below.)  The best possible random accuracy is achieved
when the distribution of responses matches that of the references.

\subsubsection{Cohen's $\kappa$ Statistic}

Cohen's $\kappa$ statistic uses the random accuracy for a
classification problem to adjust accuracy for chance.%
%
\footnote{Cohen, Jacob. 1960. A coefficient of agreement for nominal
  scales. {\it Educational And Psychological Measurement} {\bf
    20}:37-46.}
%
The basic idea is that performance evaluations should be discounted
based on what could be achieved by randomly guessing (in proportions
based on the marginal response categories of a responder).  Cohen
defined $\kappa$ for a specific classification problem, reference
answers and responses as
%
\begin{equation}\label{eq:kappa-statistic}
\kappa = \frac{a - e}{1 - e}
\end{equation}
%
where $a$ is a response's total accuracy, and $e$ is the random
accuracy, computed as above.  The notation involves an $e$ because its
value is also the expected accuracy from guessing randomly (according
to the response's own proportions).  Thus we expect a system that only
does as well as random, so that $a = e$, to have a $\kappa$ score of
0.  The maximum possible value of $\kappa$ is 1, which occurs only for
perfectly accurate systems where $a = 1$.  The minimum possible value,
for a perfectly innacurate system with $a = 0$ is $-e/(1-e)$, which is
coincidentally the (negative) odds for a randomly chosen item to be
guessed correctly with a random guess.

Cohen's $\kappa$ is often used to evaluate the agreement between a
pair of annotators for a data labeling task.  Because accuracy only
depends on the diagonal items, the $\kappa$ statistic provides the
same result no matter which annotator is treated as the reference and
which as the response.  LingPipe's confusion matrix class can compute
$\kappa$, as we see in the demo in the next section.

\subsubsection{Siegel and Castellan's $\kappa$ Statistic}

Depending on how expected accuracy is calculated, the result of the
$\kappa$ calculation, as given in \refeq{kappa-statistic}, varies.

A popular alternative to Cohen's version is Siegel and Castellan's
$\kappa$, 
%
\footnote{Siegel, Sidney and N.~John Castellan, Jr. 1988.  {\it
    Nonparametric Statistics for the Behavioral Sciences}. McGraw
  Hill.}
%
which uses the average of the reference and response
proportions to compute the chance accuracy.  

Continuing our wine example, recall that the reference had 12/27
cabernet and the response 13/27 cabernet.  In the Siegel and Castellan
version of $\kappa$, we average these two to get 12.5/27; for syrah,
averaging 9/27 and 9/27 yields 9/27, and for pinot, 5/27 and 6/27 make
5.5/27.  The average of values summing to 1 also sums to 1, so we use
them as probabilities to compute expected accuracy as
%
\begin{equation}
\left( \frac{12.5}{27} \right)^2
+ \left( \frac{9}{27} \right)^2
+ \left( \frac{5.5}{27} \right)^2
\approx 0.3669.
\end{equation}
%
In general, Siegel and Castellan's expected agreement will be higher
than with Cohen's calculation.

Given this definition of expected accuracy, we just plug it into the
definition of the $\kappa$ statistic, $\kappa = (a - e)/(1 - e)$.


\subsubsection{Byrt et al.'s $\kappa$ Statistic}

Redefining $\kappa$ is a popular sport, the most recent widely cited
version being provided by Byrt, Bishop and Carlin.%
%
\footnote{Byrt, Ted, Janet Bishop and John B. Carlin. 1993. Bias,
  prevalence, and kappa. {\it Journal of Clinical Epidemiology}
  {\bf 46}(5):423--429.}
%
In this version, $\kappa$ is computed directly as
%
\begin{equation}
\kappa = 2 a - 1
\end{equation}
%
where $a$ is the accuracy.  The minimum value, for 0\% accuracy,
is -1, the maximum value, for 100\% accuracy is 1, and the
breakeven poin is 50\% accuracy, yielding a $\kappa$ value of 0.


\subsection{Entropy Measures}



A good classifier will produce decisions whose decision matrices show
very low entropy.  Low entropy arises when the uncertainty in the
category of an item given the classifier's decision is low.  The
minimal entropy is 0, which occurs when the classifier is perfect in
the sense that given the classifiers decision, there was no
uncertainty at all about the actual category.

Like variance and standard deviation, entropy is a measure of the
randomness of a variable.  If the outcomes are predictable, the entropy 
of a variable is low.

We can measure the 


\subsection{The \code{ConfusionMatrix} Class}

A confusion matrix is represented in LingPipe as an instance of the
class \code{ConfusionMatrix}, which is in package
\code{com.aliasi.classify}.  A confusion matrix is immutable and
constructed with its categories


The confusion matrix class has dozens of methods that return either
statistics derived from the confusion matrix or views of the confusion
matrix such as one-versus-all comparisons of a single category versus
all other categories.




\subsection{Demo: Confusion Matrices}

There is a demo of confusion matrices in the \code{main()} method of
the class \code{ConfusionMatrixDemo}, and there are no command-line
arguments.
%
\codeblock{ConfusionMatrixDemo.1}
%



\subsubsection{One Versus All Comparisons}



