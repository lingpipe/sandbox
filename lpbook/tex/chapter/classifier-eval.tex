\chapter{Classifiers and Evaluation}\label{chap:classifier-evaluation}

We are going to introduce the classifier interface, discuss what
classifiers do, and then show how to evaluate them.  In subsequent
chapters, we consider a selection of the classifier implementations
offered in LingPipe.

\section{What is a Classifier?}

A classifier takes inputs, which could be just about anything, and
return a classification of the input over a finite number of discrete
categories.  For example, a classifier might take a biomedical
research paper's abstract and determine if it is about genomics or
not.  The outputs are ``about genomics'' and ``not about genomics.''

Classifiers can have more than two outputs.  For instance, a
classifier might take a newswire article and classify whether it is
politics, sports, entertainment, science and technology, or world or
local news; this is what Google and Bing's news services do.%
%
\footnote{At \url{http://news.google.com} and
  \url{http://news.bing.com}.}
%
Other applications of text classifiers, either alone or in concert
with other components, range from classifying documents by topic,
identifying the language of a snippet of text, analyzing whether a
movie review is postive or negative, linking a mention of a gene name
in a text to a database entry for that gene, or resolving the sense of
an ambiguous word like \stringmention{bank} as meaning a savings
institution or a flowing body of water.  In each of these cases, we
have a known finite set of outcomes and some evidence in the form of
text.

\subsection{Exclusive and Exhaustive Categories}

In the standard formulation of classification, which LingPipe follows,
the categories are taken to be both exhaustive and mutually exclusive.
Thus every item being classified has exactly one category.

One way to get around the exhaustiveness problem is to include an
``other'' category that matches any input that doesn't match one of
the other categories.  The other category is sometimes called a
``sink'' or ``trash'' category, and may be handled differently than
the other categories during training.

Exclusivity is more difficult to engineer.  While it is possible to
allow categories that represent more than one outcome, if we have $n$
base categories, we'll have ${n \choose 2}$ unique pairs of
categories.  The combinatorics quickly gets out of hand.

If an item needs to be classified for two cross-cutting
categorizations, we can use two classifiers, one for each
categorization.  For instance, we might classify MEDLINE citations as
being about genomics or not, and about being about clinical trials or
not.  The two binary classifiers produce four possible outcomes.
Another example of this would be to use two binary classifiers for
sentiment, one indicating if a text had positive sentiment or not, and
another indicating if it had negative sentiment or not.  The result is
a four-way classification, with a neutral article having neither
positive nor negative sentiment, a mixed review having both positive
and negative sentiment, and a positive or negative review having one
or the other.

The latent Dirichlet allocation (LDA) model we consider in
\refchap{lda} assigns more than one category to each document, under
the assumption that every document is a mixture of topics blended
according to some document-specific ratio that the model infers.

\subsection{First-Best, Ranked and Probabilistic Results}

A simple first-best classifier need only return its best guess for the
category of each input item.  Some applications only allow a single
best guess, and thus some evaluations are geared toward evaluating
only a classifiers' first-best guess for an input.

A ranking classifier returns the possible categories in rank order of
their match to the input.  The top ranked answer is the first-best
result.  We still assume exhaustive and exclusive categories, but the
classfier supplies its second-best guess, third-best guess and so on.
In cases where there are large numbers of categories, an application
might return more than one possible answer to a user.

A scoring classifier goes one step further and assigns a (floating
point) score to each categories.  These may then be sorted to provide
a ranking and a first-best result, with higher scores taken to be
better matches.  For example, LingPipe's implementations of averaged
perceptrons returns scored results.

Scores for categories given an input are often normalized so that they
represent an estimate of the conditional probability of a category
given the input.  This is the case for LingPipe's logistic regression
classifiers and k-nearest neighbors classifiers.  For instance, a
classifier might see a MEDLINE citation about testing for a disease
with a known genetic component and estimate a 40\% probability it is
about genomics and 60\% probability that it is not about genomics.
Because the outcomes are exhaustive and exclusive, the probabilities
assigned to the categories must sum to 1.

In the case of generative statistical models, such as naive Bayes
classifiers or hidden Markov models, the score represents the joint
probabilty of the output category and the input being classified.
Given the joint probabilities, we can use the rule of total
probability to compute conditional probabilities of categories given
inputs.

\subsection{Reductions to Classification Problems}

Many problems that don't at first blush appear to be classification
problems may be reduced to classification problems.  For instance, the
standard document search problem (see \refchap{lucene}) may be recast
as a classification problem.  Given a user query, such as
\searchquery{be-bim-bop recipe}, documents may be classified as
relevant or not-relevant to the search.  

Just about any problem that may be cast in a hypothesize-and-test
algorithm may use a classifier to do the testing, with a simple binary
outcome of accept or reject.  For instance, we can generate possible
named-entity mention chunkings of a text and then use a binary
classifier to evaluate if they are correct or not.  

Such reductions are often not interpetable probabilistically in the
sense of assigning probabilities to possible outcomes.

\subsection{Ordinals, Counts, and Scalars}

A classifier is an instance of what statisticians call categorical
models.  The outcome of a classification is a category, not a number.

Classifiers deal with categorical outcomes.  Ordinal outcomes are like
categorical outcomes, only they come with an order.  Examples of
ordinal outcomes include rating movies on a $\{ 1, 2, 3, 4, 5 \}$
scale, answering a survey question with strongly-disagree, disagree,
neutral, agree, or strongly agree, and rating a political attitude as
left, center, or right.

In many cases, we will be dealing with counts, which are non-negative
natural numbers.  Examples of count variables include the number of
times a word appears in a document, the length of a document's title
in characters, and the number of home runs a baseball player gets in a
season.  Classifiers like naive Bayes convert word count data into
categorical outcomes based on a probability model for documents (see
\refchap{naive-bayes}).

Scalar outcomes are typically continuous.  Examples of continuous
scalar variables include a person's height, the length of the vowel
sequence in milliseconds in the pronunciation of the word
\charmention{lion}, or the number of miles between two cities.  Wes

Rating movies on an ordinal 1-5 scale doesn't allow ratings like
1.793823.  Half-star ratings could be accomodated by including
additional discrete outcomes like 1.5, 2.5, 3.5 and 4.5, leaving a
total of 9 possible ratings.  When there are 100 possible ratings, it
makes less sense to treat the outcome as ordinal.  Often, it is
approximated by a continuous scalar variable.

Models that predict or estimate the values of ordinal, count and
scalar values all have different evaluation metrics than categorical
outcomes.  In this section, we focus exclusively on categorical
outcomes.


\section{Labeled Data, Gold Standards, and Reference Data}

For classification and other natural language tasks, the categories
assigned to texts are designed and applied by humans, who are
notoriously noisy in the semantic judgments for natural language.  

For instance, we made up the classification of genomics/non-genomics
for MEDLINE citations.  In order to gather evaluation data for a
classifier, we would typically select a bunch of MEDLINE citations at
random (or maybe look at a subset of interest), and label them
as to whether they are about genomics or not.

At this point, we have a problem.  If one annotator goes about his or
her merry way and annotates all the data, everything may seem fine.
But as soon as you have a second annotator try to annotate the same
data, you will see a remarkable number of disagreements over what seem
like reasonably simple notions, like being ``about'' genomics.  For
instance, what's the status of a citation in which DNA is only
mentioned in passing?  What about articles that only mention proteins
and their interactions?  What about an article on the sociology of the
human genome project?  These boundary cases may seem outlandish, but
try to label some data and see what happens.

In the Message Understanding Conference evaluations, there were a
large number of test instances involving the word \stringmention{Mars}
used to refer to the planet.  The contestants' systems varied in
whether they treated this as a location or not.  The reference data
did, but there weren't any planets mentioned in the training data.

\subsection{Majority Rules, Adjudication and Censorship}

Data sets are sometimes created by taking a majority vote on the
category for items in a corpus.  If there is no majority (for
instance, if there are two annotators and they disagree), there are
two options.  First, the data can be censored, meaning that it's
removed from consideration.  This has the adverse side effect of
making the actual test cases in the corpus easier than a random
selection might be because the borderline cases are removed.  Perhaps
one could argue this is an advantage, because we're not even sure what
the categories are for those borderline cases, so who cares what the
system does with them.  

The second approach to disagreements during annotation is
adjudication.  This can be as simple as having a third judge look at
the data.  This will break a tie for a binary classification problem,
but may not make the task any clearer.  It may also introduce noise
as borderline cases are resolved inconsistently.

A more labor intensive approach is to consider the cases of
uncertainty and attempt to update the notion of what is being
annotated.  It helps to do this with batches of examples rather than
one example at a time.  

The ideal is to have a standalone written coding standard explaining
how the data was annotated that is so clear that a third party could
read it and label the data consistently with the reference data.  


\section{Confusion Matrices}

One of the fundamental tools for evaluating first-best classifiers is
the confusion matrix.  A confusion matrix reports on the number of
agreements between a classifier and the reference (or ``true'')
categories.  This can be done for classification problems with any
number of categories.  

\subsection{Example: Blind Wine Classification by Grape}

Confusion matrices are perhaps most easily understood with an example.
Consider blind wine tasting, which may be viewed as an attempt by a
taster to classify a wine, whose label is hidden, as to whether it is
made primarily from the syrah, pinot (noir), or cabernet (sauvignon)
grape.  We have to assume that each wine is made primarily from one of
these three grapes so the categories are exclusive and exhaustive.  An
alternative, binary classification problem would be to determine if
a wine had syrah in it or not.

Suppose our taster works their way through 27 wines, assigning a
single grape as a guess for each wine.  For each of the 27 wines,
we have assumed there is a true answer among the three grapes
syrah, pinot and cabernet.  The resulting confusion matrix might look
as follows.
%
\begin{center}
\begin{tabular}{r|r|c|c|c|c}
\multicolumn{2}{c}{ } & \multicolumn{3}{c}{\tblhead{\bfseries Response}}
\\ \cline{3-5}
\multicolumn{2}{c|}{ } & \tblhead{cabernet} & \tblhead{syrah} & \tblhead{pinot}
\\ \cline{2-5}
\multirow{3}{0.15\textwidth}{\hfill\tblhead{\bfseries Reference}}
& \tblhead{cabernet} & 9 & 3 & 0 & {\it 12}
\\ \cline{2-5}
& \tblhead{syrah} & 3 & 5 & 1 & {\it 9}
\\ \cline{2-5}
& \tblhead{pinot} & 1 & 1 & 4 & {\it 6}
\\ \cline{2-6}
\multicolumn{2}{c}{ } & \multicolumn{1}{c}{\it 13} & \multicolumn{1}{c}{\it 9} & \multicolumn{1}{c}{\it 5} & \multicolumn{1}{|c}{\it\bfseries 27}
\end{tabular}%
\end{center}
%
Each row represents results for a specific reference category.  In the
example, the first row represents the results for all wines that were
truly cabernets.  Of the 12 cabernets presented, the taster guessed
that 9 were cabernets, 3 were syrah, and none were guessed to be a
pinot.  The total number of items in a row is represented at the end
in italices, here {\it 12}.  The second row represents the taster's
resuls for the 9 syrahs in the evaluation.  Of these, 5 were correctly
classified as syrahs, whereas 3 were misclassified as cabernets and
one as a pinot.  The last row is the pinots, with 4 correctly
identified and 1 misclassified as cabernet and 1 as syrah.  Note that
the table is not symmetric.  One pinot was guessed to be a cabernet,
but no cabernets were guessed to be pinots.  

The totals along the right are total number of reference items, of
which there were 12 cabernets, 9 syrahs, and 6 pinots.  The totals
along the bottom are for responses.  The taster guessed 13 wines were
cabernets, 9 were syrahs, and 5 were pinots.  The overall total in the
lower right is 27, which is sum of the values in all the cells, and
equal to the total number of wines evaluated.

\subsection{Confusion Matrix Statistics}

A number of useful statistics may derived from confusion matrices.  We
have already seen the basic counts of reference/response pairs, as
well as reference and response counts by category, as well as the overall counts.

\subsubsection{Total Accuracy}

The overall accuracy is just the number of correct responses divided
by the total numbe of responses.  A correct response for an item
occurs when the response category matches the reference category.  The
count of correct responses are thus on the diagonal of the confusion
matrix.  There were 9 correct responses for reference cabernets, 5 for
syrahs, and 4 for pinots, for a total of 18 correct responses.  The
total accuracy of the classifier is thus $18/27 \approx 0.67$.  

Total accuracy estimates the probability that a classifier will make
the correct categorization of the next item it faces, assuming the
items to be evaluated are sampled the same way as the test data.

Because overall accuracy is a simple binomial statistic, we can
compute a normal approximation to a 95\% confidence interval directly
(see \refsec{stats-binomial-variance} for an explanation of the
formula).  In this case, the 95\% interval is approximately plus or
minus 0.18.  As usual, with so few evaluation cases, we don't get a
tight estimate of our system's accuracy.  In almost all cases,
including this one, confidence in our accuracy numbers will be
inversely proportional to the square root of the number of test cases.
For instance, with 16 test cases, our confidence interval is sized
proportionally to $1/\sqrt{16} = 1/4$, and with 64 test cases, it's
proportional to $1/\sqrt{64} = 1/8$.  So even though we multiply the
number of cases by 4, the confidence only shrinks by $\sqrt{4}$, or a
half.


\subsection{The \code{ConfusionMatrix} Class}

A confusion matrix is represented in LingPipe as an instance of the
class \code{ConfusionMatrix}, which is in package
\code{com.aliasi.classify}.  A confusion matrix is immutable and
constructed with its categories


The confusion matrix class has dozens of methods that return either
statistics derived from the confusion matrix or views of the confusion
matrix such as one-versus-all comparisons of a single category versus
all other categories.




\subsection{Demo: Confusion Matrices}

There is a demo of confusion matrices in the \code{main()} method of
the class \code{ConfusionMatrixDemo}, and there are no command-line
arguments.
%
\codeblock{ConfusionMatrixDemo.1}
%



\subsubsection{One Versus All Comparisons}



