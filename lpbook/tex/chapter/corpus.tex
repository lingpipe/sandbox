\chapter{Handlers, Parsers, and Corpora}\label{chapter:corpus}

LingPipe uses a parser-handler pattern for parsing objects out of
files and processing them.  The pattern follows the design of XML's
SAX parser and content handlers, which are built into Java in package
\code{org.xml.sax}.

A parser is created for the format in which the data objects are
represented.  Then a handler for the data objects is created and
attached to the parser.  At this point, the parser may be used to
parse objects from a file or other input specification, and all
objects found in the file will be passed off to the handler for
processing.

Many of LingPipe's modules provide online training through handlers.
For instance, the dynamic language models implement character sequence
handlers.  

LingPipe's batch training modules require an entire corpus for
training.  A corpus represents an entire data set with a built-in
training and test division.  A corpus has methods that take handlers
and deliver to them the training data, the test data, or both.


\section{Handlers and Object Handlers}

\subsection{Handlers}

The marker interface \code{Handler} is included in LingPipe 4 only for
backward compatibility.  The original parser/handler design allowed
for handlers that implemented arbitrary methods, following the design
of the SAX content handler interface.

\subsection{Object Handlers}

As of Lingpipe 4, all built-in parser/handler usage is through the
\code{ObjectHandler<E>} interface, in package
\code{com.aliasi.corpus}, which extends \code{Handler}.  The
\code{ObjectHandler<E>} interface specifies a single method,
\code{handle(E)}, which processes a single data object of type
\code{E}.  It is like a simplified form of the built-in interface for
processing a streaming XML document, \code{ContentHandler}, in package
\code{org.xml.sax}.

\subsection{Demo: Character Counting Handler}

We define a simple text handler in the demo class
\code{CountingTextHandler}.  It's so simple that it doesn't
even need a constructor.
%
\codeblock{CountingTextHandler.1}
%
We declare the class to implement \code{ObjectHandler<CharSequence>}.
This contract is satisfied by our implementation of the
\code{handle(CharSequence)} method.  The method just increments the
counters for number of characters and sequences it's seen.

The \code{main()} method in the same class implements a simple
command-line demo.  It creates a conting text handler, then calls is
handle method with two strings (recall that \code{String} implements
\code{CharSequence}).
%
\codeblock{CountingTextHandler.2}

The Ant target \code{counting-text-handler} calls the main method,
which assumes no command-line arguments.  
%
\commandlinefollow{ant counting-text-handler}
\begin{verbatim}
# seqs=2 # chars=18
\end{verbatim}
%
The output comes from calling the handler's \code{toString()} method,
which is defined to print the number of sequences and characters.  In
most cases, handlers will either hold references to collections they
update or will make summary statistics or other data accumulated from
the calls to the \code{handle()} method.  We'll see more examples when
we discuss parsers and corpora in the following sections.




\section{Parsers}

A parser reads character data from an input source or character
sequence to produce objects of a specified type.  By attaching a
handler to a parser, the handler will receive a stream of
objects produced by the parser.

\subsection{The \code{Parser} Abstract Base Class}

The type hierarchy for a parser is more general than usage in LingPipe
4 demands.  The abstract base class is specified as \code{Parser<H
  extends Handler>}, in package \code{com.aliasi.corpus}.  The generic
specification requires the instantiation of the type parameter
\code{H} to extend the \code{Handler} interface.  

For use in LingPipe 4, all parsers will use object handler insances,
so all parsers will actually extend \code{Parser<ObjectHandler<E>>}
for some arbitrary object type \code{E}.

\subsubsection{Getting and Setting Handlers}

The base parser class implements getters and setters for 
handlers of the appropriate type; \code{getHandler()}
returns the handler, and \code{setHandler(H)} sets the
value of the handler.

\subsubsection{Parsing Methods}

There are a range of parsing methods specified for \code{Parser}.  Two
of them are abstract, \code{parseString(char[],int,int)} and
\code{parse(InputSource)}, which read data from a character array
slice or an input source.  The input source abstraction, borrowed from
XML's SAX parsers, generalizes the specification of a character
sequence through a reader or an input stream or URL plus a character
encoding specification; input sources also conveniently allow a
relative directory to be specified for finding resources such as DTDs
for XML parsing.

There are several helper methods specified on top of these.  The
method \code{parse(CharSequence)} reads directly from a character
sequence.  The \code{parse(File,String)} method reads the characters
to parse from a file using the specified character encoding.%
%
\footnote{The encoding is set in an \code{InputSource}, which a parser
  implementation may choose to ignore.  For instance, an XML parser
  may choose to read the encoding from the header rather than trusting
  the specification.  The helper implementations in LingPipe all
  respect the character encoding declaration.}
%
The method \code{parse(String,String)} reads from a resource specified
as a system identifier, such as a URL, using the specified character
encoding.  There are also two parallel methods, \code{parse(File)} and
\code{parse(String)}, which do not specify their character encodings.


\subsection{Abstract Base Helper Parsers}

There are two helper subclasses of \code{Parser}, both abstract and
both in package \code{com.aliasi.corpus}.  The class
\code{StringParser} requires a concrete subclass to implement the
abstract method \code{parse(char[],int,int)}.  The \code{StringParser}
class implements the \code{parse(InputSource)} method by reading a
character array from the input source using LingPipe's static utility
method \code{Streams.toCharArray(InputSource)}

The second helper subclass is \code{InputSourceParser}, which requires
subclasses to implement \code{parse(InputSource)}.  It implements
\code{parse(char[],int,int)} by converting the character array to an
input source specified through a reader.

\subsection{Line-Based Tagging Parser}

There are two more specialized classes for parsing.  The
\code{LineTaggingParser}, also in \code{com.aliasi.corpus}, parses
output for a tagger from a line-oriented format.  This format is
common due to the simplicity of its parsing and its use in the
Conference on Natural Language Learning (CoNLL).  

\subsection{XML Parser} 

The \code{XMLParser}, in package \code{com.aliasi.corpus}, extends
\code{InputSourceParser} to support parsing of XML-formatted data.
The XML parser requires the method \code{getXMLHandler()} to return an
instance of \code{DefaultHandler} (built into Java in package
\code{org.xml.sax.helpers}).  

The parser works by creating an XML reader using the static method
\code{createXMLReader()} in \code{XMLReaderFactory}, which is in
\code{org.xml.sax.helpers}.  The XML handler returned by the parser
method \code{getXMLHandler()} is then set on the reader as the content
handler, DTD handler, entity resolver, and error handler.  Thus the
XML handler must actually hold a reference to the LingPipe handler or
retain a reference to the parser (for instance, by being an inner
class) and use the \code{getHandler()} method in \code{Parser} to pass
it data.


\subsection{Demo: MedPost Part-of-Speech Tagging Parser}

As an example, we provide a demo class \code{MedPostPosParser}, which
provides a parser for taggings represented in the MedPost format.  The
MedPost corpus is a collection of sentences drawn from biomedical
research titles and abstracts in MEDLINE (see \refsec{corpora-medtag}
for more information and downloading instructions).  This parser will
extract instances of type \code{Tagging<CharSequence>} and send them
to an arbitrary parser specified at run time.

\subsubsection{Corpus Structure}

The MedPost corpus is arranged by line.  For each sentence fragment,
%
\footnote{The sentence fragments were extracted automatically by a
  system that was not 100\% accurate and were left as is for
  annotation.}
%
we have a line indicating its source in MEDLINE followed by a line
with the tokens and part of speech tags.   Here are the first
three entries from file \code{tag\_cl.ioc}.
%
\begin{verbatim}
P01282450A01
Pain_NN management_NN ,_, nutritional_JJ support_NN ,_, ...
P01350716A05
Seven_MC trials_NNS of_II beta-blockers_NNS and_CC 15_MC ...
P01421653A15
CONCLUSIONS_NNS :_: Although_CS more_RR well-designed_VVNJ ...
...
\end{verbatim}
%
We have truncated the part-of-speech tagged lines as indicated by the
ellipses (\code{...}).  The source of the first entry is a MEDLINE
citation with PubMed ID 01282450, with the rest of the annotation,
\code{A01}, indicating it was the first fragment from the abstract.
The first token in the first fragment is \stringmention{Pain}, which
is assigned a singular noun tag, \code{NN}.  The second token is
\stringmention{management}, assigned the same tag.  The third token is
a comma, (\stringmention{,}), assigned a comma (\code{,}) as a
tag. Note that the string \stringmention{beta-blockers} is analyzed as
a single token, and assigned the plural noun tag \code{NNS}, as is
\stringmention{well-designed}, which is assigned a deverbal adjective
category \code{VVNJ}.


\subsubsection{Code Walkthrough}

We define the MedPost parser to extend \code{StringParser}.
%
\codeblock{MedPostPosParser.1}
%
The generic parameter for the parser's handler is specified to be of
type \code{ObjectHandler<Tagging<String>>}.  LingPipe's \code{Tagging}
class, which represents tags applied to a sequence of items, is in
package \code{com.aliasi.tag}; we gloss over its details here as we
are only using it as an example for parsing.

Because we extended \code{StringParser}, we must implement the
\code{parseString()} method, which is done as follows.
%
\codeblock{MedPostPosParser.2}
%
We construct a string out of the character array slice passed in,
switching from the start/end notation of LingPipe and later Java to
the start/length notation of early Java.  We then split it on Unix
newlines, looking at each line in the for-each loop.  If the line
doesn't have an underscore character, there are no part-of-speech
tags, so we ignore it (we also know there are no underscores in the
identifier lines and that there are no comments).  

We send each line in the training data with an underscore in it to the
\code{process()} method.
%
\codeblock{MedPostPosParser.3}
%
In order to construct a tagging, we will collect a list of tokens and
a list of tags.  We split the line on whitespace, and consider the
strings representing token/tag pairs in turn.   For instance, the
first value for \code{pair} we see in processing the fragment above
is \code{Pain\_NN}.  

We break the token/tag pair down into its token and tag components by
splitting on the underscore character (again, keeping in mind that
there are no underscores in the tokens).  We then just add the first
and second elements derived from the split, which are our token and
tag pair.

After we add the tokens and tags from all the sentences, we use them
to construct a tagging.  Then, we use the \code{getHandler()} method,
which is implemented by the superclass \code{Parser}, to get the
handler itself.  The handler then gets called with the tagging
that's just been constructed by calling its \code{handle(Tagging)}
method.  In this way, the handler gets all the taggings produced
by parsing the input data.

Even though we've implemented our parser over character array slices,
through its superclass, we can use it to parse files or input sources.
The \code{main()} method takes a single command-line argument for
the directory containing the tagged data, comprising a set of
files ending in the suffix \code{ioc}.  

The first real work the method does is assign a new tree set of
strings to the final variable \code{tagSet}. 
%
\codeblock{MedPostPosParser.4}
%
It is final because it's used in the anonymous inner class defiend to
create an object to assign to variable \code{handler}.  The anonymous
inner class implements \code{ObjectHandler<Tagging<String>>}, so it
must define a method \code{handle(Tagging<String>)}.  All the handle
method does is add the list of tags found in the tagging to the tag
set.

Next, we create a parser and set its handler to be the
handler we just defined.
%
\codeblock{MedPostPosParser.5}
%
The last thing the main method does before printing out the tag set is
walk over all the files ending in \code{ioc} and parse each of them,
declaring the character encoding to be ASCII (see
\refsec{io-file-extension-filter} for more information on LingPipe's
file extension filter class).

\subsubsection{Running the Demo}

There is an Ant target \code{medpost-parse} that passes
the value of property \code{medpost.dir} to the command
in \code{MedPostPosParser}.  We have downloaded the data
and unpacked it into the directory given in the command,
%
\commandlinefollow{ant -Dmedpost.dir=c:/lpb/data/medtag/medpost medpost-parse}
\begin{verbatim}
#Tags=63    Tags=    ''    (    )    ,    .    :    CC    CC+
... 
VVGJ    VVGN    VVI    VVN    VVNJ    VVZ    ``
\end{verbatim}
%
We've skipped the output for most of the tags, where denoted by
ellipses (\code{...}).


\section{Corpora}

A corpus, in the linguistic sense, is a body of data.  In the context
of LingPipe, a corpus of data may be labeled with some kind of
conceptual or linguistic annotation, or it may consist solely of text
data.

\subsection{The \code{Corpus} Class}

To represent a corpus, LingPipe uses the abstract base class
\code{Corpus}, in package \code{com.aliasi.corpus}.  It is
parameterized generically the same way as a parser, with
\code{Corpus<H extends Handler>}.  As of LingPipe 4, we only really
have need of instances of \code{Corpus<ObjectHandler<E>>}, where
\code{E} is the type of object being represented by the corpus.

The basic work of a corpus class is done by its \code{visitTrain(H)}
and \code{visitTest(H)} methods, where \code{H} is the handler type.
Calling these methods sends all the training or testing events to the
specified handler.  

The method \code{visitCorpus(H)} sends both training and testing
events to the specified handler and \code{visitCorpus(H,H)} sends the
training events to the first handler and the test events to the
second.

In all the LingPipe built ins, these methods will be
\code{visitTrain(ObjectHandler<E>)} and
\code{visitTest(ObjectHandler<E>)} for some object \code{E}.

\subsection{Demo: 20 Newsgroups Corpus}

We will use the 20 newsgroups corpus as an example (see
\refsec{corpora-20-newsgroups}).  The corpus contains around 20,000
newsgroup posts to 20 different newsgroups.

The demo class \code{TwentyNewsgroupsCorpus} implements a corpus based
on this data.  The corpus is implemented so that it doesn't need to
keep all the data in memory; it reads it in from a compressed tar file
on each pass.  This approach is highly scalable because only one item
at a time need reside in memory.

\subsubsection{Code Walkthrough}

The 20 newsgroups corpus class is defined to extend \code{Corpus}.
%
\codeblock{TwentyNewsgroupsCorpus.1}
%
The handler is an object handler handling objects of type
\code{Classified<CharSequence>}, which represent character sequences
with first-best category assignments from a classification.
In the 20 newsgrops corpus, the categories are the newsgroups from
which the posts originated and the objects being classified,
of type \code{CharSequence}, are the texts of the posts themselves.

The constructor takes the corpus file, which is presumed to be
directory structures that have been tarred and then compressed with
GZIP (see \refsec{io-tar} and \refsec{io-gzip} for more information on
tar and GZIP).  The corpus file is saved for later use.  

The \code{visitTrain()} method sends all the classified character
sequences in the training data to the specified hander.
%
\codeblock{TwentyNewsgroupsCorpus.2}
%
The \code{visitTest()} method is defined similarly.  

The \code{visitFile()} method which is doing the work, is
defined as follows.
%
\codeblock{TwentyNewsgroupsCorpus.3}
%
It takes the string to match against the directories in the tarred
directory data, and the handler.  It starts by creating an input
stream for gzipped tar data.  

Next, it walks through the directories and files in he tarred
data.
%
\codeblock{TwentyNewsgroupsCorpus.4}
%
We loop, getting the next tar entry, breaking out of the loop if the
entry is null, indicating we have processed every entry.  If the entry
is a directory, we skip it.  Otherwise, we get its name, then parse
out the directory structure using the indexes of the directory
structure slash (\code{/}) separators, pulling out the name of the
directory indicating whether we have training or test data.  For
example, a file tar entry with name
\path{20news-bydate-test/alt.atheism/53265} uses the top-level
directory to indicate the training-testing split, here \code{test},
and the subdirectory to indicate the category, here
\code{alt.atheism}.  We are not keeping track of the article number,
though it would make sense to set things up to keep track of it for a
real application.


If the top-level train-test directory doesn't match, we continue.  If
it does match, we pull out the newsgroup name, then read the actual
bytes of the entry using LingPipe's \code{Streams.toByteArray()}
utility method.  We create a string using the ASCII-encoding because
the 20 newsgroups corpus is in ASCII.  We then create a classification
with category corresponding to the name of the newsgroup, and use it
to create the classified cahracter sequence object.  Finally, we give
the classified character sequence to the object handler, which is
defined to handle objects of type \code{Classified<CharSequence>}.

When we finish the loop, we close the tar input stream (which closes
the other streams) and return.

There is a \code{main()} method which is called by the demo,
converting a single command-line argument into a file
\code{tngFileTgz}.  The code starts by defining a final set
of categories, then an anonymous inner class to define a handler.
%
\codeblock{TwentyNewsgroupsCorpus.5}
%
The handle method extracts the best category from the classification
and adds it to the category set.  The category set needed to be final
so that it could be referenced from within the anonymous inner class's
handle method.

Once the handler is defined, we create a corpus from the gzipped
tar file, then send the train and testing events to the handler.
%
\codeblock{TwentyNewsgroupsCorpus.6}
%
The rest of the code just prints out the categories.

\subsubsection{Running the Demo}

The Ant target \code{twenty-newsgroups} invokes the
\code{TwentyNewsgroupsCorpus} command, sending the value of property
\code{tng.tgz} as the command-line argument.
%
\commandlinefollow{ant -Dtng.tgz=c:/lpb/data/20news-bydate.tar.gz twenty-newsgroups}
\begin{verbatim}
Cats= alt.atheism    comp.graphics    comp.os.ms-windows.misc
...
talk.politics.mideast    talk.politics.misc    talk.religion.misc
\end{verbatim}
%
We have left out some in the middle, but the result is indeed the
twenty expected categories, which correspond to the newsgroups.


\subsection{The \code{ListCorpus} Class}

LingPipe provides a dynamic, in-memory corpus implementation in the
class \code{ListCorpus}, in package \code{com.aliasi.corpus}.  The
class has a generic parameter, \code{ListCorpus<E>}, where \code{E} is
the type of objects handled.  It is defined to extend
\code{Corpus<ObjectHandler<E>>}, so the handler will be an object
handler for type \code{E} objects.

There is a zero-argument constructor \code{ListCorpus()} that
constructs an initially empty corpus.  The two methods
\code{addTrain(E)} and \code{addTest(E)} allow training or test items
to be added to the corpus.

The methods \code{trainCases()} and \code{testCases()} return lists of
the training and test items in the corpus.  These lists are
unmodifiable views of the underlying lists.  Although they cannot be
changed, they will track changes to the lists underlying the corpus.

There is an additional method \code{permuteCorpus(Random)}, that uses
the specified random number generator to permute the order of the test
and training items.

\subsubsection{Serialization}

A list-based corpus may be serialized if the underlying objects
it stores may be serialized.  Upon being reconstituted, the result
will be a \code{ListCorpus} that may be cast to the appropriate
generic type.

\subsubsection{Thread Safety}

A list corpus must be read-write synchronized, where the add
and permute methods are writers, and the visit methods are readers.

\subsection{The \code{DiskCorpus} Class}

The \code{DiskCorpus} class provides an implementation of the
\code{Corpus} interface based on one directory of files containing
training cases and one containing test cases.  A parser is used to
extract data from the files in the training and test directories.

Like our example in the previous section, a disk corpus reads its data
in dynamically rather than loading it into memory.  Thus, if the data
changes on disk, the next run through the corpus may produce different
data.  

The directories are expored recursively, and files with GZIP
or Zip compression indicated by suffix \code{.gz} or \code{.zip}
are automatically unpacked.  For zip files, each entry is visited.

At the lowest level, every ordinary file, eithe in the directory
structure or in a Zip archive is visited and processed. 

\subsubsection{Constructing a Disk Corpus}

An instance of \code{DiskCorpus<H>}, where \code{H} extends or
implements the marker interface \code{Handler}, is constructed using
\code{DiskCoropus(Parser<H>,File,File)}, with the two files
specifying the training and test directories respectively.

Once a disk corpus is constructed, the methods
\code{setSystemId(String)} and \code{setCharEncoding(String)} may be
used to configure it.  These settings will be passed to the input
sources constructed and sent to the parser.

\subsubsection{Thread Safety}

As long as there are no sets and the file system doesn't change,
it'll be safe to use a single insance of a disk corpus from
multiple threads.


\section{Cross Validation}

Cross validation is a process for evaluating a learning system in such
a way as to extract as many evaluation points as possible from a fixed
collection of data.

Cross validation works by dividing a corpus up into a set of
non-overlapping slices, called folds.  Then, each fold is used for
evaluation in turn, using the other folds as training data.  For
instance, if we have 1000 data items, we may divide them into 3 folds
of 333, 333, and 334 items respectively.  We then evaluate fold 1
using fold 2 and 3 for training, evaluate on fold 2 using folds 1 and
3 for training, and evaluate on fold 3 using folds 1 and 2 for
training.  This way, every item in the corpus is used as a test case.

\subsubsection{Leave-One-Out Evaluation}

At the limit where the number of folds equals the number of data
items, we have what is called leave-one-out (LOO) evaluation.  For
instance, with 1000 data items, we have 1000 folds.  This uses as much
data as possible in the training set.  This is usually too slow to
implement directly by retraining learning systems for each fold of
leave-one-out evaluation.


\subsubsection{Bias in Cross Validation}

In some sitautions, cross-validation may be very biased.  For
instance, suppose we have 100 items, 50 of which are ``positive'' and
50 of which are ``negative'' and we consider leave-one-out evaluation.
Even though our full training data is balanced, with equal numbers of
positive and negative items, each fold of leave-one-out evaluation has
50 examples of the other category, and 49 examples of the category
being evaluated.  For learners that use this ratio in the training
data to help with predictions, such as naive Bayes classifiers, there
will be a noticeable bias against the category being evaluated in a
leave-one-out evaluation.

\subsection{Permuting the Corpus}

Cross-validation only makes sense theoretically when the items are all
drawn at random from some large population (or potential population)
of items.  Very rarely does natural language data satisfy this
requirement.  Instead, we gather data by document and by date.  If you
have the choice, by all means take a truly random sample of data.

It is sometimes reasonable to permute the items in a cross-validating
corpus.  Often, this is because examples of the same item may be
stored together and should be broken up for evaluation.  

Permuting may make some problems too easy.  For instance, if the items
are chunkings of sentences for named entities, and more than one
sentence is drawn from a given document, it makes sense to keep these
together (or at least not intentionally split them apart).  Otherwise,
we are much more likely to have been trained no the rare words
appearing in sentences.

\subsection{The \code{CrossValidatingObjectCorpus} Class}

LingPipe provides a corpus implementation tailored to implementing
cross validation.  The class \code{CrossValidatingObjectCorpus<E>} has
a generic parameter \code{E} defining the objects that are handled in
the corpus.  The class extends \code{Corpus<ObjectHandler<E>>}, so the
handlers for a cross-validating object corpus will have a
\code{handle(E)} method.

\subsubsection{Construction}

The cross-validating corpus has a constructor
\code{CrossValidatingObjectCorpus(int)}, where the number of folds
must be specified.  These may be changed later using the
\code{setNumFolds(int)} method.

\subsubsection{Populating the Corpus}

Items are added to the corpus using the method \code{handle(E)}.
These are all stored in lists in memory.  

For convenience, the cross-validating corpus class is defined to
implement \code{ObjectHandler<E>} itself, which only requires the
\code{handle(E)} method.  Thus an entire cross-validating corpus may
be sent to a parser for population.

The \code{size()} method indicates the total number of objects in the
corpus.

\subsubsection{Using the Corpus}

The particular fold to use to divide up the data is set using the
method \code{setFold(int)}.  The current fold is available through the
method \code{fold()}.  The folds are numbered starting at 0 and ending
at the number of folds minus one.  The number of folds may also be
reset at any time using the method \code{setFold(int)}.  The corpus
may be permuted using a specified pseudorandom number generator with
the method \code{permuteCorpus(Random)}.



\subsubsection{Serialization}

A cross-validating corpus will be serializable if the objects
it contains are serializable.  


\subsubsection{Thread Safety and Concurrent Cross-Validation}

Cross-validating corpus instances must be read-write synchronized (see
\refsec{java-read-write-synchronization}).  The read operations are in
the handle, permute and set methods.

The obstacle to using a cross-validating corpus across threads is that
the fold is set on the class itself.  To get around this limitation,
the method \code{itemView()} returns a ``view'' of the corpus that
allows the fold number or number of folds to be changed, but not the
items.  Thus to cross-validate across folds, just create and populate
and permute a corpus.  Then use \code{itemView()} to produce as
many virtual copies as you need, setting the folds on each one.  

Note that this does not help with the problem of evaluators being used
to aggregate scores across folds not being thread safe.  Given that it
is usually training that is the bottleneck in cross validation,
explicit synchronization of the evaluators should not cause too much
of a synchronization bottleneck.











